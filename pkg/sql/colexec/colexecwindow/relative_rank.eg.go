// Code generated by execgen; DO NOT EDIT.
// Copyright 2020 The Cockroach Authors.
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.

package colexecwindow

import (
	"context"

	"github.com/cockroachdb/cockroach/pkg/col/coldata"
	"github.com/cockroachdb/cockroach/pkg/sql/colcontainer"
	"github.com/cockroachdb/cockroach/pkg/sql/colexec/colexecbase"
	"github.com/cockroachdb/cockroach/pkg/sql/colexec/colexecutils"
	"github.com/cockroachdb/cockroach/pkg/sql/colexecerror"
	"github.com/cockroachdb/cockroach/pkg/sql/colexecop"
	"github.com/cockroachdb/cockroach/pkg/sql/colmem"
	"github.com/cockroachdb/cockroach/pkg/sql/execinfrapb"
	"github.com/cockroachdb/cockroach/pkg/sql/sem/tree"
	"github.com/cockroachdb/cockroach/pkg/sql/types"
	"github.com/cockroachdb/cockroach/pkg/util/mon"
	"github.com/cockroachdb/errors"
	"github.com/marusama/semaphore"
)

// TODO(yuzefovich): add benchmarks.

// NewRelativeRankOperator creates a new Operator that computes window
// functions PERCENT_RANK or CUME_DIST (depending on the passed in windowFn).
// outputColIdx specifies in which coldata.Vec the operator should put its
// output (if there is no such column, a new column is appended).
func NewRelativeRankOperator(
	unlimitedAllocator *colmem.Allocator,
	memoryLimit int64,
	diskQueueCfg colcontainer.DiskQueueCfg,
	fdSemaphore semaphore.Semaphore,
	input colexecop.Operator,
	inputTypes []*types.T,
	windowFn execinfrapb.WindowerSpec_WindowFunc,
	orderingCols []execinfrapb.Ordering_Column,
	outputColIdx int,
	partitionColIdx int,
	peersColIdx int,
	diskAcc *mon.BoundAccount,
) (colexecop.Operator, error) {
	if len(orderingCols) == 0 {
		constValue := float64(0)
		if windowFn == execinfrapb.WindowerSpec_CUME_DIST {
			constValue = 1
		}
		return colexecbase.NewConstOp(unlimitedAllocator, input, types.Float, constValue, outputColIdx)
	}
	rrInitFields := relativeRankInitFields{
		rankInitFields: rankInitFields{
			OneInputNode:    colexecop.NewOneInputNode(input),
			allocator:       unlimitedAllocator,
			outputColIdx:    outputColIdx,
			partitionColIdx: partitionColIdx,
			peersColIdx:     peersColIdx,
		},
		memoryLimit:  memoryLimit,
		diskQueueCfg: diskQueueCfg,
		fdSemaphore:  fdSemaphore,
		inputTypes:   inputTypes,
		diskAcc:      diskAcc,
	}
	switch windowFn {
	case execinfrapb.WindowerSpec_PERCENT_RANK:
		if partitionColIdx != tree.NoColumnIdx {
			return &percentRankWithPartitionOp{
				relativeRankInitFields: rrInitFields,
			}, nil
		}
		return &percentRankNoPartitionOp{
			relativeRankInitFields: rrInitFields,
		}, nil
	case execinfrapb.WindowerSpec_CUME_DIST:
		if partitionColIdx != tree.NoColumnIdx {
			return &cumeDistWithPartitionOp{
				relativeRankInitFields: rrInitFields,
			}, nil
		}
		return &cumeDistNoPartitionOp{
			relativeRankInitFields: rrInitFields,
		}, nil
	default:
		return nil, errors.Errorf("unsupported relative rank type %s", windowFn)
	}
}

// relativeRankNumRequiredFDs is the minimum number of file descriptors that
// might be needed for the machinery of the relative rank operators: the maximum
// number is needed when CUME_DIST function with either PARTITION BY or ORDER BY
// clause (or both) is used - we need 3 FDs for each of the spilling queues used
// by the operator directly plus we use an external sort to handle PARTITION BY
// and/or ORDER BY clauses.
const relativeRankNumRequiredFDs = 3 + colexecop.ExternalSorterMinPartitions

// NOTE: in the context of window functions "partitions" mean a different thing
// from "partition" in the context of external algorithms and some disk
// infrastructure: here, "partitions" are sets of tuples that are not distinct
// on the columns specified in PARTITION BY clause of the window function. If
// such clause is omitted, then all tuples from the input belong to the same
// partition.

type relativeRankState int

const (
	// relativeRankBuffering is the state in which relativeRank operators fully
	// buffer their input using SpillingQueue. Additionally, the operators will
	// be computing the sizes of the partitions and peer groups (if needed)
	// using separate spillingQueues for each. Once a zero-length batch is
	// received, the operator transitions to relativeRankEmitting state.
	relativeRankBuffering relativeRankState = iota
	// relativeRankEmitting is the state in which relativeRank operators emit
	// the output. The output batch is populated by copying the next batch from
	// the "buffered tuples" spilling queue and manually computing the output
	// column for the window function using the already computed sizes of
	// partitions and peer groups. Once a zero-length batch is dequeued from
	// the "buffered tuples" queue, the operator transitions to
	// relativeRankFinished state.
	relativeRankEmitting
	// relativeRankFinished is the state in which relativeRank operators close
	// any non-closed disk resources and emit the zero-length batch.
	relativeRankFinished
)

type relativeRankInitFields struct {
	rankInitFields
	colexecop.CloserHelper

	state        relativeRankState
	memoryLimit  int64
	diskQueueCfg colcontainer.DiskQueueCfg
	fdSemaphore  semaphore.Semaphore
	inputTypes   []*types.T

	diskAcc *mon.BoundAccount
}

type relativeRankSizesState struct {
	*colexecutils.SpillingQueue

	// runningSizes is a batch consisting of a single int64 vector that stores
	// sizes while we're computing them. Once all coldata.BatchSize() slots are
	// filled, it will be flushed to the SpillingQueue.
	runningSizes coldata.Batch
	// dequeuedSizes is a batch of already computed sizes that is dequeued
	// from the SpillingQueue.
	dequeuedSizes coldata.Batch
	// idx stores the index of the current slot in one of the batches above
	// that we're currently working with.
	idx int
}

// relativeRankUtilityQueueMemLimitFraction defines the fraction of the memory
// limit that will be given to the "utility" spillingQueues of relativeRank
// operators (i.e. non "buffered tuples" queues).
const relativeRankUtilityQueueMemLimitFraction = 0.1

type percentRankNoPartitionOp struct {
	relativeRankInitFields

	// rank indicates which rank should be assigned to the next tuple.
	rank int64
	// rankIncrement indicates by how much rank should be incremented when a
	// tuple distinct from the previous one on the ordering columns is seen.
	rankIncrement int64

	// numTuplesInPartition contains the number of tuples in the current
	// partition.
	numTuplesInPartition int64

	bufferedTuples *colexecutils.SpillingQueue
	scratch        coldata.Batch
	output         coldata.Batch
}

var _ colexecop.ClosableOperator = &percentRankNoPartitionOp{}

func (r *percentRankNoPartitionOp) Init(ctx context.Context) {
	if !r.InitHelper.Init(ctx) {
		return
	}
	r.Input.Init(r.Ctx)
	r.state = relativeRankBuffering
	usedMemoryLimitFraction := 0.0
	r.bufferedTuples = colexecutils.NewSpillingQueue(
		&colexecutils.NewSpillingQueueArgs{
			UnlimitedAllocator: r.allocator,
			Types:              r.inputTypes,
			MemoryLimit:        int64(float64(r.memoryLimit) * (1.0 - usedMemoryLimitFraction)),
			DiskQueueCfg:       r.diskQueueCfg,
			FDSemaphore:        r.fdSemaphore,
			DiskAcc:            r.diskAcc,
		},
	)
	r.scratch = r.allocator.NewMemBatchWithFixedCapacity(r.inputTypes, coldata.BatchSize())
	r.output = r.allocator.NewMemBatchWithFixedCapacity(append(r.inputTypes, types.Float), coldata.BatchSize())
	// All rank functions start counting from 1. Before we assign the rank to a
	// tuple in the batch, we first increment r.rank, so setting this
	// rankIncrement to 1 will update r.rank to 1 on the very first tuple (as
	// desired).
	r.rankIncrement = 1
}

func (r *percentRankNoPartitionOp) Next() coldata.Batch {
	var err error
	for {
		switch r.state {
		case relativeRankBuffering:
			// The outline of what we need to do in "buffering" state:
			// 1. we need to buffer the tuples that we read from the input.
			// These are simply copied into r.bufferedTuples SpillingQueue.
			// 2. (if we have PARTITION BY clause) we need to compute the sizes of
			// partitions. These sizes are stored in r.partitionsState.runningSizes
			// batch (that consists of a single vector) and r.partitionsState.idx
			// points at the next slot in that vector to write to. Once it
			// reaches coldata.BatchSize(), the batch is "flushed" to the
			// corresponding SpillingQueue. The "running" value of the current
			// partition size is stored in r.numTuplesInPartition.
			// 3. (if we have CUME_DIST function) we need to compute the sizes
			// of peer groups. These sizes are stored in r.peerGroupsState.runningSizes
			// batch (that consists of a single vector) and r.peerGroupsState.idx
			// points at the next slot in that vector to write to. Once it
			// reaches coldata.BatchSize(), the batch is "flushed" to the
			// corresponding SpillingQueue. The "running" value of the current
			// peer group size is stored in r.numPeers.
			// For example, if we have the following setup:
			//   partitionCol = {true, false, false, true, false, false, false, false}
			//   peersCol     = {true, false, true, true, false, false, true, false}
			// we want this as the result:
			//   partitionsSizes = {3, 5}
			//   peerGroupsSizes = {2, 1, 3, 2}.
			// This example also shows why we need to use two different queues
			// (since every partition can have multiple peer groups, the
			// schedule of "flushing" is different).
			batch := r.Input.Next()
			n := batch.Length()
			if n == 0 {
				r.bufferedTuples.Enqueue(r.Ctx, coldata.ZeroBatch)
				// We have fully consumed the input, so now we can populate the output.
				r.state = relativeRankEmitting
				continue
			}

			// All tuples belong to the same partition, so we need to fully consume
			// the input before we can proceed.

			sel := batch.Selection()
			// First, we buffer up all of the tuples.
			r.scratch.ResetInternalBatch()
			r.allocator.PerformOperation(r.scratch.ColVecs(), func() {
				for colIdx, vec := range r.scratch.ColVecs() {
					vec.Copy(
						coldata.CopySliceArgs{
							SliceArgs: coldata.SliceArgs{
								Src:       batch.ColVec(colIdx),
								Sel:       sel,
								SrcEndIdx: n,
							},
						},
					)
				}
				r.scratch.SetLength(n)
			})
			r.bufferedTuples.Enqueue(r.Ctx, r.scratch)

			// Then, we need to update the sizes of the partitions.
			// There is a single partition in the whole input.
			r.numTuplesInPartition += int64(n)

			continue

		case relativeRankEmitting:
			if r.scratch, err = r.bufferedTuples.Dequeue(r.Ctx); err != nil {
				colexecerror.InternalError(err)
			}
			n := r.scratch.Length()
			if n == 0 {
				r.state = relativeRankFinished
				continue
			}

			r.output.ResetInternalBatch()
			// First, we copy over the buffered up columns.
			r.allocator.PerformOperation(r.output.ColVecs()[:len(r.inputTypes)], func() {
				for colIdx, vec := range r.output.ColVecs()[:len(r.inputTypes)] {
					vec.Copy(
						coldata.CopySliceArgs{
							SliceArgs: coldata.SliceArgs{
								Src:       r.scratch.ColVec(colIdx),
								SrcEndIdx: n,
							},
						},
					)
				}
			})

			// Now we will populate the output column.
			relativeRankOutputCol := r.output.ColVec(r.outputColIdx).Float64()
			_ = relativeRankOutputCol[n-1]
			peersCol := r.scratch.ColVec(r.peersColIdx).Bool()
			_ = peersCol[n-1]
			// We don't need to think about the selection vector since all the
			// buffered up tuples have been "deselected" during the buffering
			// stage.
			for i := 0; i < n; i++ {
				// We need to set r.numTuplesInPartition to the size of the
				// partition that i'th tuple belongs to (which we have already
				// computed).
				// There is a single partition in the whole input, and
				// r.numTuplesInPartition already contains the correct number.

				//gcassert:bce
				if peersCol[i] {
					r.rank += r.rankIncrement
					r.rankIncrement = 0
				}

				// Now we can compute the value of the window function for i'th
				// tuple.
				if r.numTuplesInPartition == 1 {
					// There is a single tuple in the partition, so we return 0, per spec.
					//gcassert:bce
					relativeRankOutputCol[i] = 0
				} else {
					//gcassert:bce
					relativeRankOutputCol[i] = float64(r.rank-1) / float64(r.numTuplesInPartition-1)
				}
				r.rankIncrement++
			}
			r.output.SetLength(n)
			return r.output

		case relativeRankFinished:
			if err := r.Close(); err != nil {
				colexecerror.InternalError(err)
			}
			return coldata.ZeroBatch

		default:
			colexecerror.InternalError(errors.AssertionFailedf("percent rank operator in unhandled state"))
			// This code is unreachable, but the compiler cannot infer that.
			return nil
		}
	}
}

func (r *percentRankNoPartitionOp) Close() error {
	if !r.CloserHelper.Close() || r.Ctx == nil {
		// Either Close() has already been called or Init() was never called. In
		// both cases there is nothing to do.
		return nil
	}
	var lastErr error
	if err := r.bufferedTuples.Close(r.Ctx); err != nil {
		lastErr = err
	}
	return lastErr
}

type percentRankWithPartitionOp struct {
	relativeRankInitFields

	// rank indicates which rank should be assigned to the next tuple.
	rank int64
	// rankIncrement indicates by how much rank should be incremented when a
	// tuple distinct from the previous one on the ordering columns is seen.
	rankIncrement int64

	partitionsState relativeRankSizesState
	// numTuplesInPartition contains the number of tuples in the current
	// partition.
	numTuplesInPartition int64

	bufferedTuples *colexecutils.SpillingQueue
	scratch        coldata.Batch
	output         coldata.Batch
}

var _ colexecop.ClosableOperator = &percentRankWithPartitionOp{}

func (r *percentRankWithPartitionOp) Init(ctx context.Context) {
	if !r.InitHelper.Init(ctx) {
		return
	}
	r.Input.Init(r.Ctx)
	r.state = relativeRankBuffering
	usedMemoryLimitFraction := 0.0
	r.partitionsState.SpillingQueue = colexecutils.NewSpillingQueue(
		&colexecutils.NewSpillingQueueArgs{
			UnlimitedAllocator: r.allocator,
			Types:              []*types.T{types.Int},
			MemoryLimit:        int64(float64(r.memoryLimit) * relativeRankUtilityQueueMemLimitFraction),
			DiskQueueCfg:       r.diskQueueCfg,
			FDSemaphore:        r.fdSemaphore,
			DiskAcc:            r.diskAcc,
		},
	)
	r.partitionsState.runningSizes = r.allocator.NewMemBatchWithFixedCapacity([]*types.T{types.Int}, coldata.BatchSize())
	usedMemoryLimitFraction += relativeRankUtilityQueueMemLimitFraction
	r.bufferedTuples = colexecutils.NewSpillingQueue(
		&colexecutils.NewSpillingQueueArgs{
			UnlimitedAllocator: r.allocator,
			Types:              r.inputTypes,
			MemoryLimit:        int64(float64(r.memoryLimit) * (1.0 - usedMemoryLimitFraction)),
			DiskQueueCfg:       r.diskQueueCfg,
			FDSemaphore:        r.fdSemaphore,
			DiskAcc:            r.diskAcc,
		},
	)
	r.scratch = r.allocator.NewMemBatchWithFixedCapacity(r.inputTypes, coldata.BatchSize())
	r.output = r.allocator.NewMemBatchWithFixedCapacity(append(r.inputTypes, types.Float), coldata.BatchSize())
	// All rank functions start counting from 1. Before we assign the rank to a
	// tuple in the batch, we first increment r.rank, so setting this
	// rankIncrement to 1 will update r.rank to 1 on the very first tuple (as
	// desired).
	r.rankIncrement = 1
}

func (r *percentRankWithPartitionOp) Next() coldata.Batch {
	var err error
	for {
		switch r.state {
		case relativeRankBuffering:
			// The outline of what we need to do in "buffering" state:
			// 1. we need to buffer the tuples that we read from the input.
			// These are simply copied into r.bufferedTuples SpillingQueue.
			// 2. (if we have PARTITION BY clause) we need to compute the sizes of
			// partitions. These sizes are stored in r.partitionsState.runningSizes
			// batch (that consists of a single vector) and r.partitionsState.idx
			// points at the next slot in that vector to write to. Once it
			// reaches coldata.BatchSize(), the batch is "flushed" to the
			// corresponding SpillingQueue. The "running" value of the current
			// partition size is stored in r.numTuplesInPartition.
			// 3. (if we have CUME_DIST function) we need to compute the sizes
			// of peer groups. These sizes are stored in r.peerGroupsState.runningSizes
			// batch (that consists of a single vector) and r.peerGroupsState.idx
			// points at the next slot in that vector to write to. Once it
			// reaches coldata.BatchSize(), the batch is "flushed" to the
			// corresponding SpillingQueue. The "running" value of the current
			// peer group size is stored in r.numPeers.
			// For example, if we have the following setup:
			//   partitionCol = {true, false, false, true, false, false, false, false}
			//   peersCol     = {true, false, true, true, false, false, true, false}
			// we want this as the result:
			//   partitionsSizes = {3, 5}
			//   peerGroupsSizes = {2, 1, 3, 2}.
			// This example also shows why we need to use two different queues
			// (since every partition can have multiple peer groups, the
			// schedule of "flushing" is different).
			batch := r.Input.Next()
			n := batch.Length()
			if n == 0 {
				r.bufferedTuples.Enqueue(r.Ctx, coldata.ZeroBatch)
				// We need to flush the last vector of the running partitions
				// sizes, including the very last partition.
				runningPartitionsSizesCol := r.partitionsState.runningSizes.ColVec(0).Int64()
				runningPartitionsSizesCol[r.partitionsState.idx] = r.numTuplesInPartition
				r.partitionsState.idx++
				r.partitionsState.runningSizes.SetLength(r.partitionsState.idx)
				r.partitionsState.Enqueue(r.Ctx, r.partitionsState.runningSizes)
				r.partitionsState.Enqueue(r.Ctx, coldata.ZeroBatch)
				// We have fully consumed the input, so now we can populate the output.
				r.state = relativeRankEmitting
				continue
			}

			// For simplicity, we will fully consume the input before we start
			// producing the output.
			// TODO(yuzefovich): we could be emitting output once we see that a new
			// partition has begun.

			sel := batch.Selection()
			// First, we buffer up all of the tuples.
			r.scratch.ResetInternalBatch()
			r.allocator.PerformOperation(r.scratch.ColVecs(), func() {
				for colIdx, vec := range r.scratch.ColVecs() {
					vec.Copy(
						coldata.CopySliceArgs{
							SliceArgs: coldata.SliceArgs{
								Src:       batch.ColVec(colIdx),
								Sel:       sel,
								SrcEndIdx: n,
							},
						},
					)
				}
				r.scratch.SetLength(n)
			})
			r.bufferedTuples.Enqueue(r.Ctx, r.scratch)

			// Then, we need to update the sizes of the partitions.
			partitionCol := batch.ColVec(r.partitionColIdx).Bool()
			var runningPartitionsSizesCol []int64
			if r.partitionsState.runningSizes != nil {
				runningPartitionsSizesCol = r.partitionsState.runningSizes.ColVec(0).Int64()
			}
			if sel != nil {
				for _, i := range sel[:n] {
					if partitionCol[i] {
						// We have encountered a start of a new partition, so we
						// need to save the computed size of the previous one
						// (if there was one).
						if r.numTuplesInPartition > 0 {
							runningPartitionsSizesCol[r.partitionsState.idx] = r.numTuplesInPartition
							r.numTuplesInPartition = 0
							r.partitionsState.idx++
							if r.partitionsState.idx == coldata.BatchSize() {
								// We need to flush the vector of partitions sizes.
								r.partitionsState.runningSizes.SetLength(coldata.BatchSize())
								r.partitionsState.Enqueue(r.Ctx, r.partitionsState.runningSizes)
								r.partitionsState.idx = 0
								r.partitionsState.runningSizes.ResetInternalBatch()
							}
						}
					}
					r.numTuplesInPartition++
				}
			} else {
				_ = partitionCol[n-1]
				for i := 0; i < n; i++ {
					//gcassert:bce
					if partitionCol[i] {
						// We have encountered a start of a new partition, so we
						// need to save the computed size of the previous one
						// (if there was one).
						if r.numTuplesInPartition > 0 {
							runningPartitionsSizesCol[r.partitionsState.idx] = r.numTuplesInPartition
							r.numTuplesInPartition = 0
							r.partitionsState.idx++
							if r.partitionsState.idx == coldata.BatchSize() {
								// We need to flush the vector of partitions sizes.
								r.partitionsState.runningSizes.SetLength(coldata.BatchSize())
								r.partitionsState.Enqueue(r.Ctx, r.partitionsState.runningSizes)
								r.partitionsState.idx = 0
								r.partitionsState.runningSizes.ResetInternalBatch()
							}
						}
					}
					r.numTuplesInPartition++
				}
			}

			continue

		case relativeRankEmitting:
			if r.scratch, err = r.bufferedTuples.Dequeue(r.Ctx); err != nil {
				colexecerror.InternalError(err)
			}
			n := r.scratch.Length()
			if n == 0 {
				r.state = relativeRankFinished
				continue
			}
			// Get the next batch of partition sizes if we haven't already.
			if r.partitionsState.dequeuedSizes == nil {
				if r.partitionsState.dequeuedSizes, err = r.partitionsState.Dequeue(r.Ctx); err != nil {
					colexecerror.InternalError(err)
				}
				r.partitionsState.idx = 0
				r.numTuplesInPartition = 0
			}

			r.output.ResetInternalBatch()
			// First, we copy over the buffered up columns.
			r.allocator.PerformOperation(r.output.ColVecs()[:len(r.inputTypes)], func() {
				for colIdx, vec := range r.output.ColVecs()[:len(r.inputTypes)] {
					vec.Copy(
						coldata.CopySliceArgs{
							SliceArgs: coldata.SliceArgs{
								Src:       r.scratch.ColVec(colIdx),
								SrcEndIdx: n,
							},
						},
					)
				}
			})

			// Now we will populate the output column.
			relativeRankOutputCol := r.output.ColVec(r.outputColIdx).Float64()
			_ = relativeRankOutputCol[n-1]
			partitionCol := r.scratch.ColVec(r.partitionColIdx).Bool()
			_ = partitionCol[n-1]
			peersCol := r.scratch.ColVec(r.peersColIdx).Bool()
			_ = peersCol[n-1]
			// We don't need to think about the selection vector since all the
			// buffered up tuples have been "deselected" during the buffering
			// stage.
			for i := 0; i < n; i++ {
				// We need to set r.numTuplesInPartition to the size of the
				// partition that i'th tuple belongs to (which we have already
				// computed).
				//gcassert:bce
				if partitionCol[i] {
					if r.partitionsState.idx == r.partitionsState.dequeuedSizes.Length() {
						if r.partitionsState.dequeuedSizes, err = r.partitionsState.Dequeue(r.Ctx); err != nil {
							colexecerror.InternalError(err)
						}
						r.partitionsState.idx = 0
					}
					r.numTuplesInPartition = r.partitionsState.dequeuedSizes.ColVec(0).Int64()[r.partitionsState.idx]
					r.partitionsState.idx++
					// We need to reset the internal state because of the new
					// partition.
					r.rank = 0
					r.rankIncrement = 1
				}

				//gcassert:bce
				if peersCol[i] {
					r.rank += r.rankIncrement
					r.rankIncrement = 0
				}

				// Now we can compute the value of the window function for i'th
				// tuple.
				if r.numTuplesInPartition == 1 {
					// There is a single tuple in the partition, so we return 0, per spec.
					//gcassert:bce
					relativeRankOutputCol[i] = 0
				} else {
					//gcassert:bce
					relativeRankOutputCol[i] = float64(r.rank-1) / float64(r.numTuplesInPartition-1)
				}
				r.rankIncrement++
			}
			r.output.SetLength(n)
			return r.output

		case relativeRankFinished:
			if err := r.Close(); err != nil {
				colexecerror.InternalError(err)
			}
			return coldata.ZeroBatch

		default:
			colexecerror.InternalError(errors.AssertionFailedf("percent rank operator in unhandled state"))
			// This code is unreachable, but the compiler cannot infer that.
			return nil
		}
	}
}

func (r *percentRankWithPartitionOp) Close() error {
	if !r.CloserHelper.Close() || r.Ctx == nil {
		// Either Close() has already been called or Init() was never called. In
		// both cases there is nothing to do.
		return nil
	}
	var lastErr error
	if err := r.bufferedTuples.Close(r.Ctx); err != nil {
		lastErr = err
	}
	if err := r.partitionsState.Close(r.Ctx); err != nil {
		lastErr = err
	}
	return lastErr
}

type cumeDistNoPartitionOp struct {
	relativeRankInitFields

	peerGroupsState relativeRankSizesState
	// numPrecedingTuples stores the number of tuples preceding to the first
	// peer of the current tuple in the current partition.
	numPrecedingTuples int64
	// numPeers stores the number of tuples that are peers with the current
	// tuple.
	numPeers int64

	// numTuplesInPartition contains the number of tuples in the current
	// partition.
	numTuplesInPartition int64

	bufferedTuples *colexecutils.SpillingQueue
	scratch        coldata.Batch
	output         coldata.Batch
}

var _ colexecop.ClosableOperator = &cumeDistNoPartitionOp{}

func (r *cumeDistNoPartitionOp) Init(ctx context.Context) {
	if !r.InitHelper.Init(ctx) {
		return
	}
	r.Input.Init(r.Ctx)
	r.state = relativeRankBuffering
	usedMemoryLimitFraction := 0.0
	r.peerGroupsState.SpillingQueue = colexecutils.NewSpillingQueue(
		&colexecutils.NewSpillingQueueArgs{
			UnlimitedAllocator: r.allocator,
			Types:              []*types.T{types.Int},
			MemoryLimit:        int64(float64(r.memoryLimit) * relativeRankUtilityQueueMemLimitFraction),
			DiskQueueCfg:       r.diskQueueCfg,
			FDSemaphore:        r.fdSemaphore,
			DiskAcc:            r.diskAcc,
		},
	)
	r.peerGroupsState.runningSizes = r.allocator.NewMemBatchWithFixedCapacity([]*types.T{types.Int}, coldata.BatchSize())
	usedMemoryLimitFraction += relativeRankUtilityQueueMemLimitFraction
	r.bufferedTuples = colexecutils.NewSpillingQueue(
		&colexecutils.NewSpillingQueueArgs{
			UnlimitedAllocator: r.allocator,
			Types:              r.inputTypes,
			MemoryLimit:        int64(float64(r.memoryLimit) * (1.0 - usedMemoryLimitFraction)),
			DiskQueueCfg:       r.diskQueueCfg,
			FDSemaphore:        r.fdSemaphore,
			DiskAcc:            r.diskAcc,
		},
	)
	r.scratch = r.allocator.NewMemBatchWithFixedCapacity(r.inputTypes, coldata.BatchSize())
	r.output = r.allocator.NewMemBatchWithFixedCapacity(append(r.inputTypes, types.Float), coldata.BatchSize())
}

func (r *cumeDistNoPartitionOp) Next() coldata.Batch {
	var err error
	for {
		switch r.state {
		case relativeRankBuffering:
			// The outline of what we need to do in "buffering" state:
			// 1. we need to buffer the tuples that we read from the input.
			// These are simply copied into r.bufferedTuples SpillingQueue.
			// 2. (if we have PARTITION BY clause) we need to compute the sizes of
			// partitions. These sizes are stored in r.partitionsState.runningSizes
			// batch (that consists of a single vector) and r.partitionsState.idx
			// points at the next slot in that vector to write to. Once it
			// reaches coldata.BatchSize(), the batch is "flushed" to the
			// corresponding SpillingQueue. The "running" value of the current
			// partition size is stored in r.numTuplesInPartition.
			// 3. (if we have CUME_DIST function) we need to compute the sizes
			// of peer groups. These sizes are stored in r.peerGroupsState.runningSizes
			// batch (that consists of a single vector) and r.peerGroupsState.idx
			// points at the next slot in that vector to write to. Once it
			// reaches coldata.BatchSize(), the batch is "flushed" to the
			// corresponding SpillingQueue. The "running" value of the current
			// peer group size is stored in r.numPeers.
			// For example, if we have the following setup:
			//   partitionCol = {true, false, false, true, false, false, false, false}
			//   peersCol     = {true, false, true, true, false, false, true, false}
			// we want this as the result:
			//   partitionsSizes = {3, 5}
			//   peerGroupsSizes = {2, 1, 3, 2}.
			// This example also shows why we need to use two different queues
			// (since every partition can have multiple peer groups, the
			// schedule of "flushing" is different).
			batch := r.Input.Next()
			n := batch.Length()
			if n == 0 {
				r.bufferedTuples.Enqueue(r.Ctx, coldata.ZeroBatch)
				// We need to flush the last vector of the running peer groups
				// sizes, including the very last peer group.
				runningPeerGroupsSizesCol := r.peerGroupsState.runningSizes.ColVec(0).Int64()
				runningPeerGroupsSizesCol[r.peerGroupsState.idx] = r.numPeers
				r.peerGroupsState.idx++
				r.peerGroupsState.runningSizes.SetLength(r.peerGroupsState.idx)
				r.peerGroupsState.Enqueue(r.Ctx, r.peerGroupsState.runningSizes)
				r.peerGroupsState.Enqueue(r.Ctx, coldata.ZeroBatch)
				// We have fully consumed the input, so now we can populate the output.
				r.state = relativeRankEmitting
				continue
			}

			// All tuples belong to the same partition, so we need to fully consume
			// the input before we can proceed.

			sel := batch.Selection()
			// First, we buffer up all of the tuples.
			r.scratch.ResetInternalBatch()
			r.allocator.PerformOperation(r.scratch.ColVecs(), func() {
				for colIdx, vec := range r.scratch.ColVecs() {
					vec.Copy(
						coldata.CopySliceArgs{
							SliceArgs: coldata.SliceArgs{
								Src:       batch.ColVec(colIdx),
								Sel:       sel,
								SrcEndIdx: n,
							},
						},
					)
				}
				r.scratch.SetLength(n)
			})
			r.bufferedTuples.Enqueue(r.Ctx, r.scratch)

			// Then, we need to update the sizes of the partitions.
			// There is a single partition in the whole input.
			r.numTuplesInPartition += int64(n)

			// Next, we need to update the sizes of the peer groups.
			peersCol := batch.ColVec(r.peersColIdx).Bool()
			var runningPeerGroupsSizesCol []int64
			if r.peerGroupsState.runningSizes != nil {
				runningPeerGroupsSizesCol = r.peerGroupsState.runningSizes.ColVec(0).Int64()
			}
			if sel != nil {
				for _, i := range sel[:n] {
					if peersCol[i] {
						// We have encountered a start of a new peer group, so we
						// need to save the computed size of the previous one
						// (if there was one).
						if r.numPeers > 0 {
							runningPeerGroupsSizesCol[r.peerGroupsState.idx] = r.numPeers
							r.numPeers = 0
							r.peerGroupsState.idx++
							if r.peerGroupsState.idx == coldata.BatchSize() {
								// We need to flush the vector of peer group sizes.
								r.peerGroupsState.runningSizes.SetLength(coldata.BatchSize())
								r.peerGroupsState.Enqueue(r.Ctx, r.peerGroupsState.runningSizes)
								r.peerGroupsState.idx = 0
								r.peerGroupsState.runningSizes.ResetInternalBatch()
							}
						}
					}
					r.numPeers++
				}
			} else {
				_ = peersCol[n-1]
				for i := 0; i < n; i++ {
					//gcassert:bce
					if peersCol[i] {
						// We have encountered a start of a new peer group, so we
						// need to save the computed size of the previous one
						// (if there was one).
						if r.numPeers > 0 {
							runningPeerGroupsSizesCol[r.peerGroupsState.idx] = r.numPeers
							r.numPeers = 0
							r.peerGroupsState.idx++
							if r.peerGroupsState.idx == coldata.BatchSize() {
								// We need to flush the vector of peer group sizes.
								r.peerGroupsState.runningSizes.SetLength(coldata.BatchSize())
								r.peerGroupsState.Enqueue(r.Ctx, r.peerGroupsState.runningSizes)
								r.peerGroupsState.idx = 0
								r.peerGroupsState.runningSizes.ResetInternalBatch()
							}
						}
					}
					r.numPeers++
				}
			}
			continue

		case relativeRankEmitting:
			if r.scratch, err = r.bufferedTuples.Dequeue(r.Ctx); err != nil {
				colexecerror.InternalError(err)
			}
			n := r.scratch.Length()
			if n == 0 {
				r.state = relativeRankFinished
				continue
			}
			// Get the next batch of peer group sizes if we haven't already.
			if r.peerGroupsState.dequeuedSizes == nil {
				if r.peerGroupsState.dequeuedSizes, err = r.peerGroupsState.Dequeue(r.Ctx); err != nil {
					colexecerror.InternalError(err)
				}
				r.peerGroupsState.idx = 0
				r.numPeers = 0
			}

			r.output.ResetInternalBatch()
			// First, we copy over the buffered up columns.
			r.allocator.PerformOperation(r.output.ColVecs()[:len(r.inputTypes)], func() {
				for colIdx, vec := range r.output.ColVecs()[:len(r.inputTypes)] {
					vec.Copy(
						coldata.CopySliceArgs{
							SliceArgs: coldata.SliceArgs{
								Src:       r.scratch.ColVec(colIdx),
								SrcEndIdx: n,
							},
						},
					)
				}
			})

			// Now we will populate the output column.
			relativeRankOutputCol := r.output.ColVec(r.outputColIdx).Float64()
			_ = relativeRankOutputCol[n-1]
			peersCol := r.scratch.ColVec(r.peersColIdx).Bool()
			_ = peersCol[n-1]
			// We don't need to think about the selection vector since all the
			// buffered up tuples have been "deselected" during the buffering
			// stage.
			for i := 0; i < n; i++ {
				// We need to set r.numTuplesInPartition to the size of the
				// partition that i'th tuple belongs to (which we have already
				// computed).
				// There is a single partition in the whole input, and
				// r.numTuplesInPartition already contains the correct number.

				//gcassert:bce
				if peersCol[i] {
					// We have encountered a new peer group, and we need to update the
					// number of preceding tuples and get the number of tuples in
					// this peer group.
					r.numPrecedingTuples += r.numPeers
					if r.peerGroupsState.idx == r.peerGroupsState.dequeuedSizes.Length() {
						if r.peerGroupsState.dequeuedSizes, err = r.peerGroupsState.Dequeue(r.Ctx); err != nil {
							colexecerror.InternalError(err)
						}
						r.peerGroupsState.idx = 0
					}
					r.numPeers = r.peerGroupsState.dequeuedSizes.ColVec(0).Int64()[r.peerGroupsState.idx]
					r.peerGroupsState.idx++
				}

				// Now we can compute the value of the window function for i'th
				// tuple.
				//gcassert:bce
				relativeRankOutputCol[i] = float64(r.numPrecedingTuples+r.numPeers) / float64(r.numTuplesInPartition)
			}
			r.output.SetLength(n)
			return r.output

		case relativeRankFinished:
			if err := r.Close(); err != nil {
				colexecerror.InternalError(err)
			}
			return coldata.ZeroBatch

		default:
			colexecerror.InternalError(errors.AssertionFailedf("percent rank operator in unhandled state"))
			// This code is unreachable, but the compiler cannot infer that.
			return nil
		}
	}
}

func (r *cumeDistNoPartitionOp) Close() error {
	if !r.CloserHelper.Close() || r.Ctx == nil {
		// Either Close() has already been called or Init() was never called. In
		// both cases there is nothing to do.
		return nil
	}
	var lastErr error
	if err := r.bufferedTuples.Close(r.Ctx); err != nil {
		lastErr = err
	}
	if err := r.peerGroupsState.Close(r.Ctx); err != nil {
		lastErr = err
	}
	return lastErr
}

type cumeDistWithPartitionOp struct {
	relativeRankInitFields

	peerGroupsState relativeRankSizesState
	// numPrecedingTuples stores the number of tuples preceding to the first
	// peer of the current tuple in the current partition.
	numPrecedingTuples int64
	// numPeers stores the number of tuples that are peers with the current
	// tuple.
	numPeers int64

	partitionsState relativeRankSizesState
	// numTuplesInPartition contains the number of tuples in the current
	// partition.
	numTuplesInPartition int64

	bufferedTuples *colexecutils.SpillingQueue
	scratch        coldata.Batch
	output         coldata.Batch
}

var _ colexecop.ClosableOperator = &cumeDistWithPartitionOp{}

func (r *cumeDistWithPartitionOp) Init(ctx context.Context) {
	if !r.InitHelper.Init(ctx) {
		return
	}
	r.Input.Init(r.Ctx)
	r.state = relativeRankBuffering
	usedMemoryLimitFraction := 0.0
	r.partitionsState.SpillingQueue = colexecutils.NewSpillingQueue(
		&colexecutils.NewSpillingQueueArgs{
			UnlimitedAllocator: r.allocator,
			Types:              []*types.T{types.Int},
			MemoryLimit:        int64(float64(r.memoryLimit) * relativeRankUtilityQueueMemLimitFraction),
			DiskQueueCfg:       r.diskQueueCfg,
			FDSemaphore:        r.fdSemaphore,
			DiskAcc:            r.diskAcc,
		},
	)
	r.partitionsState.runningSizes = r.allocator.NewMemBatchWithFixedCapacity([]*types.T{types.Int}, coldata.BatchSize())
	usedMemoryLimitFraction += relativeRankUtilityQueueMemLimitFraction
	r.peerGroupsState.SpillingQueue = colexecutils.NewSpillingQueue(
		&colexecutils.NewSpillingQueueArgs{
			UnlimitedAllocator: r.allocator,
			Types:              []*types.T{types.Int},
			MemoryLimit:        int64(float64(r.memoryLimit) * relativeRankUtilityQueueMemLimitFraction),
			DiskQueueCfg:       r.diskQueueCfg,
			FDSemaphore:        r.fdSemaphore,
			DiskAcc:            r.diskAcc,
		},
	)
	r.peerGroupsState.runningSizes = r.allocator.NewMemBatchWithFixedCapacity([]*types.T{types.Int}, coldata.BatchSize())
	usedMemoryLimitFraction += relativeRankUtilityQueueMemLimitFraction
	r.bufferedTuples = colexecutils.NewSpillingQueue(
		&colexecutils.NewSpillingQueueArgs{
			UnlimitedAllocator: r.allocator,
			Types:              r.inputTypes,
			MemoryLimit:        int64(float64(r.memoryLimit) * (1.0 - usedMemoryLimitFraction)),
			DiskQueueCfg:       r.diskQueueCfg,
			FDSemaphore:        r.fdSemaphore,
			DiskAcc:            r.diskAcc,
		},
	)
	r.scratch = r.allocator.NewMemBatchWithFixedCapacity(r.inputTypes, coldata.BatchSize())
	r.output = r.allocator.NewMemBatchWithFixedCapacity(append(r.inputTypes, types.Float), coldata.BatchSize())
}

func (r *cumeDistWithPartitionOp) Next() coldata.Batch {
	var err error
	for {
		switch r.state {
		case relativeRankBuffering:
			// The outline of what we need to do in "buffering" state:
			// 1. we need to buffer the tuples that we read from the input.
			// These are simply copied into r.bufferedTuples SpillingQueue.
			// 2. (if we have PARTITION BY clause) we need to compute the sizes of
			// partitions. These sizes are stored in r.partitionsState.runningSizes
			// batch (that consists of a single vector) and r.partitionsState.idx
			// points at the next slot in that vector to write to. Once it
			// reaches coldata.BatchSize(), the batch is "flushed" to the
			// corresponding SpillingQueue. The "running" value of the current
			// partition size is stored in r.numTuplesInPartition.
			// 3. (if we have CUME_DIST function) we need to compute the sizes
			// of peer groups. These sizes are stored in r.peerGroupsState.runningSizes
			// batch (that consists of a single vector) and r.peerGroupsState.idx
			// points at the next slot in that vector to write to. Once it
			// reaches coldata.BatchSize(), the batch is "flushed" to the
			// corresponding SpillingQueue. The "running" value of the current
			// peer group size is stored in r.numPeers.
			// For example, if we have the following setup:
			//   partitionCol = {true, false, false, true, false, false, false, false}
			//   peersCol     = {true, false, true, true, false, false, true, false}
			// we want this as the result:
			//   partitionsSizes = {3, 5}
			//   peerGroupsSizes = {2, 1, 3, 2}.
			// This example also shows why we need to use two different queues
			// (since every partition can have multiple peer groups, the
			// schedule of "flushing" is different).
			batch := r.Input.Next()
			n := batch.Length()
			if n == 0 {
				r.bufferedTuples.Enqueue(r.Ctx, coldata.ZeroBatch)
				// We need to flush the last vector of the running partitions
				// sizes, including the very last partition.
				runningPartitionsSizesCol := r.partitionsState.runningSizes.ColVec(0).Int64()
				runningPartitionsSizesCol[r.partitionsState.idx] = r.numTuplesInPartition
				r.partitionsState.idx++
				r.partitionsState.runningSizes.SetLength(r.partitionsState.idx)
				r.partitionsState.Enqueue(r.Ctx, r.partitionsState.runningSizes)
				r.partitionsState.Enqueue(r.Ctx, coldata.ZeroBatch)
				// We need to flush the last vector of the running peer groups
				// sizes, including the very last peer group.
				runningPeerGroupsSizesCol := r.peerGroupsState.runningSizes.ColVec(0).Int64()
				runningPeerGroupsSizesCol[r.peerGroupsState.idx] = r.numPeers
				r.peerGroupsState.idx++
				r.peerGroupsState.runningSizes.SetLength(r.peerGroupsState.idx)
				r.peerGroupsState.Enqueue(r.Ctx, r.peerGroupsState.runningSizes)
				r.peerGroupsState.Enqueue(r.Ctx, coldata.ZeroBatch)
				// We have fully consumed the input, so now we can populate the output.
				r.state = relativeRankEmitting
				continue
			}

			// For simplicity, we will fully consume the input before we start
			// producing the output.
			// TODO(yuzefovich): we could be emitting output once we see that a new
			// partition has begun.

			sel := batch.Selection()
			// First, we buffer up all of the tuples.
			r.scratch.ResetInternalBatch()
			r.allocator.PerformOperation(r.scratch.ColVecs(), func() {
				for colIdx, vec := range r.scratch.ColVecs() {
					vec.Copy(
						coldata.CopySliceArgs{
							SliceArgs: coldata.SliceArgs{
								Src:       batch.ColVec(colIdx),
								Sel:       sel,
								SrcEndIdx: n,
							},
						},
					)
				}
				r.scratch.SetLength(n)
			})
			r.bufferedTuples.Enqueue(r.Ctx, r.scratch)

			// Then, we need to update the sizes of the partitions.
			partitionCol := batch.ColVec(r.partitionColIdx).Bool()
			var runningPartitionsSizesCol []int64
			if r.partitionsState.runningSizes != nil {
				runningPartitionsSizesCol = r.partitionsState.runningSizes.ColVec(0).Int64()
			}
			if sel != nil {
				for _, i := range sel[:n] {
					if partitionCol[i] {
						// We have encountered a start of a new partition, so we
						// need to save the computed size of the previous one
						// (if there was one).
						if r.numTuplesInPartition > 0 {
							runningPartitionsSizesCol[r.partitionsState.idx] = r.numTuplesInPartition
							r.numTuplesInPartition = 0
							r.partitionsState.idx++
							if r.partitionsState.idx == coldata.BatchSize() {
								// We need to flush the vector of partitions sizes.
								r.partitionsState.runningSizes.SetLength(coldata.BatchSize())
								r.partitionsState.Enqueue(r.Ctx, r.partitionsState.runningSizes)
								r.partitionsState.idx = 0
								r.partitionsState.runningSizes.ResetInternalBatch()
							}
						}
					}
					r.numTuplesInPartition++
				}
			} else {
				_ = partitionCol[n-1]
				for i := 0; i < n; i++ {
					//gcassert:bce
					if partitionCol[i] {
						// We have encountered a start of a new partition, so we
						// need to save the computed size of the previous one
						// (if there was one).
						if r.numTuplesInPartition > 0 {
							runningPartitionsSizesCol[r.partitionsState.idx] = r.numTuplesInPartition
							r.numTuplesInPartition = 0
							r.partitionsState.idx++
							if r.partitionsState.idx == coldata.BatchSize() {
								// We need to flush the vector of partitions sizes.
								r.partitionsState.runningSizes.SetLength(coldata.BatchSize())
								r.partitionsState.Enqueue(r.Ctx, r.partitionsState.runningSizes)
								r.partitionsState.idx = 0
								r.partitionsState.runningSizes.ResetInternalBatch()
							}
						}
					}
					r.numTuplesInPartition++
				}
			}

			// Next, we need to update the sizes of the peer groups.
			peersCol := batch.ColVec(r.peersColIdx).Bool()
			var runningPeerGroupsSizesCol []int64
			if r.peerGroupsState.runningSizes != nil {
				runningPeerGroupsSizesCol = r.peerGroupsState.runningSizes.ColVec(0).Int64()
			}
			if sel != nil {
				for _, i := range sel[:n] {
					if peersCol[i] {
						// We have encountered a start of a new peer group, so we
						// need to save the computed size of the previous one
						// (if there was one).
						if r.numPeers > 0 {
							runningPeerGroupsSizesCol[r.peerGroupsState.idx] = r.numPeers
							r.numPeers = 0
							r.peerGroupsState.idx++
							if r.peerGroupsState.idx == coldata.BatchSize() {
								// We need to flush the vector of peer group sizes.
								r.peerGroupsState.runningSizes.SetLength(coldata.BatchSize())
								r.peerGroupsState.Enqueue(r.Ctx, r.peerGroupsState.runningSizes)
								r.peerGroupsState.idx = 0
								r.peerGroupsState.runningSizes.ResetInternalBatch()
							}
						}
					}
					r.numPeers++
				}
			} else {
				_ = peersCol[n-1]
				for i := 0; i < n; i++ {
					//gcassert:bce
					if peersCol[i] {
						// We have encountered a start of a new peer group, so we
						// need to save the computed size of the previous one
						// (if there was one).
						if r.numPeers > 0 {
							runningPeerGroupsSizesCol[r.peerGroupsState.idx] = r.numPeers
							r.numPeers = 0
							r.peerGroupsState.idx++
							if r.peerGroupsState.idx == coldata.BatchSize() {
								// We need to flush the vector of peer group sizes.
								r.peerGroupsState.runningSizes.SetLength(coldata.BatchSize())
								r.peerGroupsState.Enqueue(r.Ctx, r.peerGroupsState.runningSizes)
								r.peerGroupsState.idx = 0
								r.peerGroupsState.runningSizes.ResetInternalBatch()
							}
						}
					}
					r.numPeers++
				}
			}
			continue

		case relativeRankEmitting:
			if r.scratch, err = r.bufferedTuples.Dequeue(r.Ctx); err != nil {
				colexecerror.InternalError(err)
			}
			n := r.scratch.Length()
			if n == 0 {
				r.state = relativeRankFinished
				continue
			}
			// Get the next batch of partition sizes if we haven't already.
			if r.partitionsState.dequeuedSizes == nil {
				if r.partitionsState.dequeuedSizes, err = r.partitionsState.Dequeue(r.Ctx); err != nil {
					colexecerror.InternalError(err)
				}
				r.partitionsState.idx = 0
				r.numTuplesInPartition = 0
			}
			// Get the next batch of peer group sizes if we haven't already.
			if r.peerGroupsState.dequeuedSizes == nil {
				if r.peerGroupsState.dequeuedSizes, err = r.peerGroupsState.Dequeue(r.Ctx); err != nil {
					colexecerror.InternalError(err)
				}
				r.peerGroupsState.idx = 0
				r.numPeers = 0
			}

			r.output.ResetInternalBatch()
			// First, we copy over the buffered up columns.
			r.allocator.PerformOperation(r.output.ColVecs()[:len(r.inputTypes)], func() {
				for colIdx, vec := range r.output.ColVecs()[:len(r.inputTypes)] {
					vec.Copy(
						coldata.CopySliceArgs{
							SliceArgs: coldata.SliceArgs{
								Src:       r.scratch.ColVec(colIdx),
								SrcEndIdx: n,
							},
						},
					)
				}
			})

			// Now we will populate the output column.
			relativeRankOutputCol := r.output.ColVec(r.outputColIdx).Float64()
			_ = relativeRankOutputCol[n-1]
			partitionCol := r.scratch.ColVec(r.partitionColIdx).Bool()
			_ = partitionCol[n-1]
			peersCol := r.scratch.ColVec(r.peersColIdx).Bool()
			_ = peersCol[n-1]
			// We don't need to think about the selection vector since all the
			// buffered up tuples have been "deselected" during the buffering
			// stage.
			for i := 0; i < n; i++ {
				// We need to set r.numTuplesInPartition to the size of the
				// partition that i'th tuple belongs to (which we have already
				// computed).
				//gcassert:bce
				if partitionCol[i] {
					if r.partitionsState.idx == r.partitionsState.dequeuedSizes.Length() {
						if r.partitionsState.dequeuedSizes, err = r.partitionsState.Dequeue(r.Ctx); err != nil {
							colexecerror.InternalError(err)
						}
						r.partitionsState.idx = 0
					}
					r.numTuplesInPartition = r.partitionsState.dequeuedSizes.ColVec(0).Int64()[r.partitionsState.idx]
					r.partitionsState.idx++
					// We need to reset the number of preceding tuples because of the
					// new partition.
					r.numPrecedingTuples = 0
					r.numPeers = 0
				}

				//gcassert:bce
				if peersCol[i] {
					// We have encountered a new peer group, and we need to update the
					// number of preceding tuples and get the number of tuples in
					// this peer group.
					r.numPrecedingTuples += r.numPeers
					if r.peerGroupsState.idx == r.peerGroupsState.dequeuedSizes.Length() {
						if r.peerGroupsState.dequeuedSizes, err = r.peerGroupsState.Dequeue(r.Ctx); err != nil {
							colexecerror.InternalError(err)
						}
						r.peerGroupsState.idx = 0
					}
					r.numPeers = r.peerGroupsState.dequeuedSizes.ColVec(0).Int64()[r.peerGroupsState.idx]
					r.peerGroupsState.idx++
				}

				// Now we can compute the value of the window function for i'th
				// tuple.
				//gcassert:bce
				relativeRankOutputCol[i] = float64(r.numPrecedingTuples+r.numPeers) / float64(r.numTuplesInPartition)
			}
			r.output.SetLength(n)
			return r.output

		case relativeRankFinished:
			if err := r.Close(); err != nil {
				colexecerror.InternalError(err)
			}
			return coldata.ZeroBatch

		default:
			colexecerror.InternalError(errors.AssertionFailedf("percent rank operator in unhandled state"))
			// This code is unreachable, but the compiler cannot infer that.
			return nil
		}
	}
}

func (r *cumeDistWithPartitionOp) Close() error {
	if !r.CloserHelper.Close() || r.Ctx == nil {
		// Either Close() has already been called or Init() was never called. In
		// both cases there is nothing to do.
		return nil
	}
	var lastErr error
	if err := r.bufferedTuples.Close(r.Ctx); err != nil {
		lastErr = err
	}
	if err := r.partitionsState.Close(r.Ctx); err != nil {
		lastErr = err
	}
	if err := r.peerGroupsState.Close(r.Ctx); err != nil {
		lastErr = err
	}
	return lastErr
}
