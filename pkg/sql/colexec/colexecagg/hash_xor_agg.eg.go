// Code generated by execgen; DO NOT EDIT.
// Copyright 2026 The Cockroach Authors.
//
// Use of this software is governed by the CockroachDB Software License
// included in the /LICENSE file.

package colexecagg

import (
	"unsafe"

	"github.com/cockroachdb/cockroach/pkg/col/coldata"
	"github.com/cockroachdb/cockroach/pkg/col/typeconv"
	"github.com/cockroachdb/cockroach/pkg/sql/colexecerror"
	"github.com/cockroachdb/cockroach/pkg/sql/colmem"
	"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgcode"
	"github.com/cockroachdb/cockroach/pkg/sql/pgwire/pgerror"
	"github.com/cockroachdb/cockroach/pkg/sql/types"
	"github.com/cockroachdb/errors"
)

// Workaround for bazel auto-generated code. goimports does not automatically
// pick up the right packages when run within the bazel sandbox.
var (
	_ = typeconv.TypeFamilyToCanonicalTypeFamily
	_ pgcode.Code
	_ pgerror.Error
)

func newXorHashAggAlloc(
	allocator *colmem.Allocator, t *types.T, allocSize int64,
) (aggregateFuncAlloc, error) {
	allocBase := aggAllocBase{allocator: allocator, allocSize: allocSize}
	switch t.Family() {
	case types.IntFamily:
		switch t.Width() {
		case 16:
			return &xorInt16HashAggAlloc{aggAllocBase: allocBase}, nil
		case 32:
			return &xorInt32HashAggAlloc{aggAllocBase: allocBase}, nil
		case -1:
		default:
			return &xorInt64HashAggAlloc{aggAllocBase: allocBase}, nil
		}
	case types.BytesFamily:
		switch t.Width() {
		case -1:
		default:
			return &xorBytesHashAggAlloc{aggAllocBase: allocBase}, nil
		}
	}
	return nil, errors.AssertionFailedf("unsupported xor agg type %s", t.Name())
}

type xorInt16HashAgg struct {
	unorderedAggregateFuncBase
	// curAgg holds the running XOR result, so we can index into the slice once per
	// group, instead of on each iteration.
	curAgg int64
	// numNonNull tracks the number of non-null values we have seen for the group
	// that is currently being aggregated.
	numNonNull uint64
}

var _ AggregateFunc = &xorInt16HashAgg{}

func (a *xorInt16HashAgg) Compute(
	vecs []*coldata.Vec, inputIdxs []uint32, startIdx, endIdx int, sel []int,
) {
	var oldCurAggSize uintptr
	vec := vecs[inputIdxs[0]]
	col, nulls := vec.Int16(), vec.Nulls()
	a.allocator.PerformOperation([]*coldata.Vec{a.vec}, func() {
		{
			sel = sel[startIdx:endIdx]
			if nulls.MaybeHasNulls() {
				for _, i := range sel {

					var isNull bool
					isNull = nulls.NullAt(i)
					if !isNull {
						v := col.Get(i)

						a.curAgg = int64(a.curAgg) ^ int64(v)

						a.numNonNull++
					}
				}
			} else {
				for _, i := range sel {

					var isNull bool
					isNull = false
					if !isNull {
						v := col.Get(i)

						a.curAgg = int64(a.curAgg) ^ int64(v)

						a.numNonNull++
					}
				}
			}
		}
	},
	)
	var newCurAggSize uintptr
	if newCurAggSize != oldCurAggSize {
		a.allocator.AdjustMemoryUsageAfterAllocation(int64(newCurAggSize - oldCurAggSize))
	}
}

func (a *xorInt16HashAgg) Flush(outputIdx int) {
	// The aggregation is finished. Flush the last value. If we haven't found
	// any non-nulls for this group so far, the output for this group should be
	// null.
	col := a.vec.Int64()
	if a.numNonNull == 0 {
		a.nulls.SetNull(outputIdx)
	} else {
		col.Set(outputIdx, a.curAgg)
	}
}

func (a *xorInt16HashAgg) Reset() {
	a.curAgg = zeroInt64Value
	a.numNonNull = 0
}

type xorInt16HashAggAlloc struct {
	aggAllocBase
	aggFuncs []xorInt16HashAgg
}

var _ aggregateFuncAlloc = &xorInt16HashAggAlloc{}

const sizeOfXorInt16HashAgg = int64(unsafe.Sizeof(xorInt16HashAgg{}))
const xorInt16HashAggSliceOverhead = int64(unsafe.Sizeof([]xorInt16HashAgg{}))

func (a *xorInt16HashAggAlloc) newAggFunc() AggregateFunc {
	if len(a.aggFuncs) == 0 {
		a.allocator.AdjustMemoryUsage(xorInt16HashAggSliceOverhead + sizeOfXorInt16HashAgg*a.allocSize)
		a.aggFuncs = make([]xorInt16HashAgg, a.allocSize)
	}
	f := &a.aggFuncs[0]
	f.allocator = a.allocator
	a.aggFuncs = a.aggFuncs[1:]
	return f
}

type xorInt32HashAgg struct {
	unorderedAggregateFuncBase
	// curAgg holds the running XOR result, so we can index into the slice once per
	// group, instead of on each iteration.
	curAgg int64
	// numNonNull tracks the number of non-null values we have seen for the group
	// that is currently being aggregated.
	numNonNull uint64
}

var _ AggregateFunc = &xorInt32HashAgg{}

func (a *xorInt32HashAgg) Compute(
	vecs []*coldata.Vec, inputIdxs []uint32, startIdx, endIdx int, sel []int,
) {
	var oldCurAggSize uintptr
	vec := vecs[inputIdxs[0]]
	col, nulls := vec.Int32(), vec.Nulls()
	a.allocator.PerformOperation([]*coldata.Vec{a.vec}, func() {
		{
			sel = sel[startIdx:endIdx]
			if nulls.MaybeHasNulls() {
				for _, i := range sel {

					var isNull bool
					isNull = nulls.NullAt(i)
					if !isNull {
						v := col.Get(i)

						a.curAgg = int64(a.curAgg) ^ int64(v)

						a.numNonNull++
					}
				}
			} else {
				for _, i := range sel {

					var isNull bool
					isNull = false
					if !isNull {
						v := col.Get(i)

						a.curAgg = int64(a.curAgg) ^ int64(v)

						a.numNonNull++
					}
				}
			}
		}
	},
	)
	var newCurAggSize uintptr
	if newCurAggSize != oldCurAggSize {
		a.allocator.AdjustMemoryUsageAfterAllocation(int64(newCurAggSize - oldCurAggSize))
	}
}

func (a *xorInt32HashAgg) Flush(outputIdx int) {
	// The aggregation is finished. Flush the last value. If we haven't found
	// any non-nulls for this group so far, the output for this group should be
	// null.
	col := a.vec.Int64()
	if a.numNonNull == 0 {
		a.nulls.SetNull(outputIdx)
	} else {
		col.Set(outputIdx, a.curAgg)
	}
}

func (a *xorInt32HashAgg) Reset() {
	a.curAgg = zeroInt64Value
	a.numNonNull = 0
}

type xorInt32HashAggAlloc struct {
	aggAllocBase
	aggFuncs []xorInt32HashAgg
}

var _ aggregateFuncAlloc = &xorInt32HashAggAlloc{}

const sizeOfXorInt32HashAgg = int64(unsafe.Sizeof(xorInt32HashAgg{}))
const xorInt32HashAggSliceOverhead = int64(unsafe.Sizeof([]xorInt32HashAgg{}))

func (a *xorInt32HashAggAlloc) newAggFunc() AggregateFunc {
	if len(a.aggFuncs) == 0 {
		a.allocator.AdjustMemoryUsage(xorInt32HashAggSliceOverhead + sizeOfXorInt32HashAgg*a.allocSize)
		a.aggFuncs = make([]xorInt32HashAgg, a.allocSize)
	}
	f := &a.aggFuncs[0]
	f.allocator = a.allocator
	a.aggFuncs = a.aggFuncs[1:]
	return f
}

type xorInt64HashAgg struct {
	unorderedAggregateFuncBase
	// curAgg holds the running XOR result, so we can index into the slice once per
	// group, instead of on each iteration.
	curAgg int64
	// numNonNull tracks the number of non-null values we have seen for the group
	// that is currently being aggregated.
	numNonNull uint64
}

var _ AggregateFunc = &xorInt64HashAgg{}

func (a *xorInt64HashAgg) Compute(
	vecs []*coldata.Vec, inputIdxs []uint32, startIdx, endIdx int, sel []int,
) {
	var oldCurAggSize uintptr
	vec := vecs[inputIdxs[0]]
	col, nulls := vec.Int64(), vec.Nulls()
	a.allocator.PerformOperation([]*coldata.Vec{a.vec}, func() {
		{
			sel = sel[startIdx:endIdx]
			if nulls.MaybeHasNulls() {
				for _, i := range sel {

					var isNull bool
					isNull = nulls.NullAt(i)
					if !isNull {
						v := col.Get(i)

						a.curAgg = int64(a.curAgg) ^ int64(v)

						a.numNonNull++
					}
				}
			} else {
				for _, i := range sel {

					var isNull bool
					isNull = false
					if !isNull {
						v := col.Get(i)

						a.curAgg = int64(a.curAgg) ^ int64(v)

						a.numNonNull++
					}
				}
			}
		}
	},
	)
	var newCurAggSize uintptr
	if newCurAggSize != oldCurAggSize {
		a.allocator.AdjustMemoryUsageAfterAllocation(int64(newCurAggSize - oldCurAggSize))
	}
}

func (a *xorInt64HashAgg) Flush(outputIdx int) {
	// The aggregation is finished. Flush the last value. If we haven't found
	// any non-nulls for this group so far, the output for this group should be
	// null.
	col := a.vec.Int64()
	if a.numNonNull == 0 {
		a.nulls.SetNull(outputIdx)
	} else {
		col.Set(outputIdx, a.curAgg)
	}
}

func (a *xorInt64HashAgg) Reset() {
	a.curAgg = zeroInt64Value
	a.numNonNull = 0
}

type xorInt64HashAggAlloc struct {
	aggAllocBase
	aggFuncs []xorInt64HashAgg
}

var _ aggregateFuncAlloc = &xorInt64HashAggAlloc{}

const sizeOfXorInt64HashAgg = int64(unsafe.Sizeof(xorInt64HashAgg{}))
const xorInt64HashAggSliceOverhead = int64(unsafe.Sizeof([]xorInt64HashAgg{}))

func (a *xorInt64HashAggAlloc) newAggFunc() AggregateFunc {
	if len(a.aggFuncs) == 0 {
		a.allocator.AdjustMemoryUsage(xorInt64HashAggSliceOverhead + sizeOfXorInt64HashAgg*a.allocSize)
		a.aggFuncs = make([]xorInt64HashAgg, a.allocSize)
	}
	f := &a.aggFuncs[0]
	f.allocator = a.allocator
	a.aggFuncs = a.aggFuncs[1:]
	return f
}

type xorBytesHashAgg struct {
	unorderedAggregateFuncBase
	// curAgg holds the running XOR result, so we can index into the slice once per
	// group, instead of on each iteration.
	curAgg []byte
	// numNonNull tracks the number of non-null values we have seen for the group
	// that is currently being aggregated.
	numNonNull uint64
}

var _ AggregateFunc = &xorBytesHashAgg{}

func (a *xorBytesHashAgg) Compute(
	vecs []*coldata.Vec, inputIdxs []uint32, startIdx, endIdx int, sel []int,
) {
	oldCurAggSize := len(a.curAgg)
	vec := vecs[inputIdxs[0]]
	col, nulls := vec.Bytes(), vec.Nulls()
	a.allocator.PerformOperation([]*coldata.Vec{a.vec}, func() {
		{
			sel = sel[startIdx:endIdx]
			if nulls.MaybeHasNulls() {
				for _, i := range sel {

					var isNull bool
					isNull = nulls.NullAt(i)
					if !isNull {
						v := col.Get(i)

						{
							if a.numNonNull == 0 {
								a.curAgg = make([]byte, len(v))
								copy(a.curAgg, v)
							} else {
								if len(a.curAgg) != len(v) {
									colexecerror.ExpectedError(pgerror.Newf(pgcode.InvalidParameterValue,
										"arguments to xor must all be the same length %d vs %d", len(a.curAgg), len(v),
									))
								}
								for j := range a.curAgg {
									a.curAgg[j] = a.curAgg[j] ^ v[j]
								}
							}
						}
						a.numNonNull++
					}
				}
			} else {
				for _, i := range sel {

					var isNull bool
					isNull = false
					if !isNull {
						v := col.Get(i)

						{
							if a.numNonNull == 0 {
								a.curAgg = make([]byte, len(v))
								copy(a.curAgg, v)
							} else {
								if len(a.curAgg) != len(v) {
									colexecerror.ExpectedError(pgerror.Newf(pgcode.InvalidParameterValue,
										"arguments to xor must all be the same length %d vs %d", len(a.curAgg), len(v),
									))
								}
								for j := range a.curAgg {
									a.curAgg[j] = a.curAgg[j] ^ v[j]
								}
							}
						}
						a.numNonNull++
					}
				}
			}
		}
	},
	)
	newCurAggSize := len(a.curAgg)
	if newCurAggSize != oldCurAggSize {
		a.allocator.AdjustMemoryUsageAfterAllocation(int64(newCurAggSize - oldCurAggSize))
	}
}

func (a *xorBytesHashAgg) Flush(outputIdx int) {
	// The aggregation is finished. Flush the last value. If we haven't found
	// any non-nulls for this group so far, the output for this group should be
	// null.
	col := a.vec.Bytes()
	if a.numNonNull == 0 {
		a.nulls.SetNull(outputIdx)
	} else {
		col.Set(outputIdx, a.curAgg)
	}
}

func (a *xorBytesHashAgg) Reset() {
	a.curAgg = zeroBytesValue
	a.numNonNull = 0
}

type xorBytesHashAggAlloc struct {
	aggAllocBase
	aggFuncs []xorBytesHashAgg
}

var _ aggregateFuncAlloc = &xorBytesHashAggAlloc{}

const sizeOfXorBytesHashAgg = int64(unsafe.Sizeof(xorBytesHashAgg{}))
const xorBytesHashAggSliceOverhead = int64(unsafe.Sizeof([]xorBytesHashAgg{}))

func (a *xorBytesHashAggAlloc) newAggFunc() AggregateFunc {
	if len(a.aggFuncs) == 0 {
		a.allocator.AdjustMemoryUsage(xorBytesHashAggSliceOverhead + sizeOfXorBytesHashAgg*a.allocSize)
		a.aggFuncs = make([]xorBytesHashAgg, a.allocSize)
	}
	f := &a.aggFuncs[0]
	f.allocator = a.allocator
	a.aggFuncs = a.aggFuncs[1:]
	return f
}
