// Code generated by execgen; DO NOT EDIT.
// Copyright 2019 The Cockroach Authors.
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.

package colexec

import (
	"bytes"
	"context"
	"math"
	"time"

	"github.com/cockroachdb/apd/v2"
	"github.com/cockroachdb/cockroach/pkg/col/coldata"
	"github.com/cockroachdb/cockroach/pkg/col/coldataext"
	"github.com/cockroachdb/cockroach/pkg/col/typeconv"
	"github.com/cockroachdb/cockroach/pkg/server/telemetry"
	"github.com/cockroachdb/cockroach/pkg/sql/colexecbase"
	"github.com/cockroachdb/cockroach/pkg/sql/colexecbase/colexecerror"
	"github.com/cockroachdb/cockroach/pkg/sql/colmem"
	"github.com/cockroachdb/cockroach/pkg/sql/sem/tree"
	"github.com/cockroachdb/cockroach/pkg/sql/sqltelemetry"
	"github.com/cockroachdb/cockroach/pkg/sql/types"
	"github.com/cockroachdb/cockroach/pkg/util/duration"
	"github.com/cockroachdb/errors"
)

type projBitandInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projBitandInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitandInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projBitandInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projBitandInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitandInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projBitandInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projBitandInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitandInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projBitandInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projBitandInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitandInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projBitandInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projBitandInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitandInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projBitandInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projBitandInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitandInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projBitandInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projBitandInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitandInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projBitandInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projBitandInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitandInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projBitandInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projBitandInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) & int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) & int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitandInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projBitandDatumDatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projBitandDatumDatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitandDatumDatumConstOp) Init() {
	p.input.Init()
}

type projBitorInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projBitorInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitorInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projBitorInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projBitorInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitorInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projBitorInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projBitorInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitorInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projBitorInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projBitorInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitorInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projBitorInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projBitorInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitorInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projBitorInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projBitorInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitorInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projBitorInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projBitorInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitorInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projBitorInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projBitorInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitorInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projBitorInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projBitorInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) | int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) | int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitorInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projBitorDatumDatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projBitorDatumDatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitorDatumDatumConstOp) Init() {
	p.input.Init()
}

type projBitxorInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projBitxorInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitxorInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projBitxorInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projBitxorInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitxorInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projBitxorInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projBitxorInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitxorInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projBitxorInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projBitxorInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitxorInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projBitxorInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projBitxorInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitxorInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projBitxorInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projBitxorInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitxorInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projBitxorInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projBitxorInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitxorInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projBitxorInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projBitxorInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitxorInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projBitxorInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projBitxorInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					projCol[i] = int64(arg) ^ int64(p.constArg)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				projCol[i] = int64(arg) ^ int64(p.constArg)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitxorInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projBitxorDatumDatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projBitxorDatumDatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projBitxorDatumDatumConstOp) Init() {
	p.input.Init()
}

type projPlusDecimalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projPlusDecimalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Add(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Add(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Add(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Add(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusDecimalInt16ConstOp) Init() {
	p.input.Init()
}

type projPlusDecimalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projPlusDecimalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Add(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Add(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Add(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Add(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusDecimalInt32ConstOp) Init() {
	p.input.Init()
}

type projPlusDecimalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projPlusDecimalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Add(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Add(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Add(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Add(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusDecimalInt64ConstOp) Init() {
	p.input.Init()
}

type projPlusDecimalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projPlusDecimalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						_, err := tree.ExactCtx.Add(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						_, err := tree.ExactCtx.Add(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					_, err := tree.ExactCtx.Add(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					_, err := tree.ExactCtx.Add(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusDecimalDecimalConstOp) Init() {
	p.input.Init()
}

type projPlusInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projPlusInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projPlusInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projPlusInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projPlusInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projPlusInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projPlusInt16DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projPlusInt16DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Add(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Add(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Add(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Add(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt16DecimalConstOp) Init() {
	p.input.Init()
}

type projPlusInt16DatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projPlusInt16DatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInt(arg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInt(arg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInt(arg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInt(arg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt16DatumConstOp) Init() {
	p.input.Init()
}

type projPlusInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projPlusInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projPlusInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projPlusInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projPlusInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projPlusInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projPlusInt32DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projPlusInt32DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Add(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Add(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Add(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Add(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt32DecimalConstOp) Init() {
	p.input.Init()
}

type projPlusInt32DatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projPlusInt32DatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInt(arg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInt(arg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInt(arg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInt(arg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt32DatumConstOp) Init() {
	p.input.Init()
}

type projPlusInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projPlusInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projPlusInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projPlusInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projPlusInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projPlusInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) + int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) < 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) + int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) < 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projPlusInt64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projPlusInt64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Add(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Add(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Add(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Add(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt64DecimalConstOp) Init() {
	p.input.Init()
}

type projPlusInt64DatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projPlusInt64DatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInt(arg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInt(arg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInt(arg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInt(arg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusInt64DatumConstOp) Init() {
	p.input.Init()
}

type projPlusFloat64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projPlusFloat64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Float64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						projCol[i] = float64(arg) + float64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						projCol[i] = float64(arg) + float64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					projCol[i] = float64(arg) + float64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					projCol[i] = float64(arg) + float64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusFloat64Float64ConstOp) Init() {
	p.input.Init()
}

type projPlusTimestampIntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projPlusTimestampIntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Times
	col = vec.Timestamp()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Timestamp()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = duration.Add(arg, p.constArg)
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = duration.Add(arg, p.constArg)
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = duration.Add(arg, p.constArg)
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = duration.Add(arg, p.constArg)
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusTimestampIntervalConstOp) Init() {
	p.input.Init()
}

type projPlusIntervalTimestampConstOp struct {
	projConstOpBase
	constArg time.Time
}

func (p projPlusIntervalTimestampConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Timestamp()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = duration.Add(p.constArg, arg)
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = duration.Add(p.constArg, arg)
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = duration.Add(p.constArg, arg)
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = duration.Add(p.constArg, arg)
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusIntervalTimestampConstOp) Init() {
	p.input.Init()
}

type projPlusIntervalIntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projPlusIntervalIntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = arg.Add(p.constArg)
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = arg.Add(p.constArg)
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = arg.Add(p.constArg)
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = arg.Add(p.constArg)
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusIntervalIntervalConstOp) Init() {
	p.input.Init()
}

type projPlusIntervalDatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projPlusIntervalDatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInterval{Duration: arg}
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInterval{Duration: arg}
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInterval{Duration: arg}
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInterval{Duration: arg}
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusIntervalDatumConstOp) Init() {
	p.input.Init()
}

type projPlusDatumIntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projPlusDatumIntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInterval{Duration: p.constArg}
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInterval{Duration: p.constArg}
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInterval{Duration: p.constArg}
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInterval{Duration: p.constArg}
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusDatumIntervalConstOp) Init() {
	p.input.Init()
}

type projPlusDatumInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projPlusDatumInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusDatumInt16ConstOp) Init() {
	p.input.Init()
}

type projPlusDatumInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projPlusDatumInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusDatumInt32ConstOp) Init() {
	p.input.Init()
}

type projPlusDatumInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projPlusDatumInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPlusDatumInt64ConstOp) Init() {
	p.input.Init()
}

type projMinusDecimalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projMinusDecimalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Sub(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Sub(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Sub(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Sub(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusDecimalInt16ConstOp) Init() {
	p.input.Init()
}

type projMinusDecimalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projMinusDecimalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Sub(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Sub(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Sub(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Sub(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusDecimalInt32ConstOp) Init() {
	p.input.Init()
}

type projMinusDecimalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projMinusDecimalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Sub(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Sub(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Sub(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Sub(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusDecimalInt64ConstOp) Init() {
	p.input.Init()
}

type projMinusDecimalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projMinusDecimalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						_, err := tree.ExactCtx.Sub(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						_, err := tree.ExactCtx.Sub(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					_, err := tree.ExactCtx.Sub(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					_, err := tree.ExactCtx.Sub(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusDecimalDecimalConstOp) Init() {
	p.input.Init()
}

type projMinusInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projMinusInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projMinusInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projMinusInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projMinusInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projMinusInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projMinusInt16DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projMinusInt16DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Sub(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Sub(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Sub(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Sub(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt16DecimalConstOp) Init() {
	p.input.Init()
}

type projMinusInt16DatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projMinusInt16DatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInt(arg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInt(arg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInt(arg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInt(arg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt16DatumConstOp) Init() {
	p.input.Init()
}

type projMinusInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projMinusInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projMinusInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projMinusInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projMinusInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projMinusInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projMinusInt32DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projMinusInt32DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Sub(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Sub(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Sub(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Sub(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt32DecimalConstOp) Init() {
	p.input.Init()
}

type projMinusInt32DatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projMinusInt32DatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInt(arg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInt(arg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInt(arg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInt(arg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt32DatumConstOp) Init() {
	p.input.Init()
}

type projMinusInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projMinusInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projMinusInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projMinusInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projMinusInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projMinusInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						result := int64(arg) - int64(p.constArg)
						if (result < int64(arg)) != (int64(p.constArg) > 0) {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					result := int64(arg) - int64(p.constArg)
					if (result < int64(arg)) != (int64(p.constArg) > 0) {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projMinusInt64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projMinusInt64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Sub(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Sub(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Sub(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Sub(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt64DecimalConstOp) Init() {
	p.input.Init()
}

type projMinusInt64DatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projMinusInt64DatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInt(arg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInt(arg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInt(arg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInt(arg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusInt64DatumConstOp) Init() {
	p.input.Init()
}

type projMinusFloat64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projMinusFloat64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Float64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						projCol[i] = float64(arg) - float64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						projCol[i] = float64(arg) - float64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					projCol[i] = float64(arg) - float64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					projCol[i] = float64(arg) - float64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusFloat64Float64ConstOp) Init() {
	p.input.Init()
}

type projMinusTimestampTimestampConstOp struct {
	projConstOpBase
	constArg time.Time
}

func (p projMinusTimestampTimestampConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Times
	col = vec.Timestamp()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					nanos := arg.Sub(p.constArg).Nanoseconds()
					projCol[i] = duration.MakeDuration(nanos, 0, 0)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					nanos := arg.Sub(p.constArg).Nanoseconds()
					projCol[i] = duration.MakeDuration(nanos, 0, 0)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				nanos := arg.Sub(p.constArg).Nanoseconds()
				projCol[i] = duration.MakeDuration(nanos, 0, 0)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				nanos := arg.Sub(p.constArg).Nanoseconds()
				projCol[i] = duration.MakeDuration(nanos, 0, 0)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusTimestampTimestampConstOp) Init() {
	p.input.Init()
}

type projMinusTimestampIntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projMinusTimestampIntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Times
	col = vec.Timestamp()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Timestamp()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = duration.Add(arg, p.constArg.Mul(-1))
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = duration.Add(arg, p.constArg.Mul(-1))
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = duration.Add(arg, p.constArg.Mul(-1))
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = duration.Add(arg, p.constArg.Mul(-1))
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusTimestampIntervalConstOp) Init() {
	p.input.Init()
}

type projMinusIntervalIntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projMinusIntervalIntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = arg.Sub(p.constArg)
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = arg.Sub(p.constArg)
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = arg.Sub(p.constArg)
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = arg.Sub(p.constArg)
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusIntervalIntervalConstOp) Init() {
	p.input.Init()
}

type projMinusIntervalDatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projMinusIntervalDatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInterval{Duration: arg}
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					_convertedNativeElem := tree.DInterval{Duration: arg}
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

					_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInterval{Duration: arg}
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				_convertedNativeElem := tree.DInterval{Duration: arg}
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_nonDatumArgAsColdataExtDatum := &coldataext.Datum{Datum: _nonDatumArgAsDatum}

				_res, err := _nonDatumArgAsColdataExtDatum.BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusIntervalDatumConstOp) Init() {
	p.input.Init()
}

type projMinusDatumDatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projMinusDatumDatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusDatumDatumConstOp) Init() {
	p.input.Init()
}

type projMinusDatumIntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projMinusDatumIntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInterval{Duration: p.constArg}
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInterval{Duration: p.constArg}
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInterval{Duration: p.constArg}
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInterval{Duration: p.constArg}
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusDatumIntervalConstOp) Init() {
	p.input.Init()
}

type projMinusDatumBytesConstOp struct {
	projConstOpBase
	constArg []byte
}

func (p projMinusDatumBytesConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DBytes(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DBytes(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DBytes(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DBytes(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusDatumBytesConstOp) Init() {
	p.input.Init()
}

type projMinusDatumInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projMinusDatumInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusDatumInt16ConstOp) Init() {
	p.input.Init()
}

type projMinusDatumInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projMinusDatumInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusDatumInt32ConstOp) Init() {
	p.input.Init()
}

type projMinusDatumInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projMinusDatumInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMinusDatumInt64ConstOp) Init() {
	p.input.Init()
}

type projMultDecimalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projMultDecimalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Mul(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Mul(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Mul(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Mul(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultDecimalInt16ConstOp) Init() {
	p.input.Init()
}

type projMultDecimalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projMultDecimalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Mul(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Mul(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Mul(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Mul(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultDecimalInt32ConstOp) Init() {
	p.input.Init()
}

type projMultDecimalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projMultDecimalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Mul(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.ExactCtx.Mul(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Mul(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.ExactCtx.Mul(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultDecimalInt64ConstOp) Init() {
	p.input.Init()
}

type projMultDecimalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projMultDecimalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						_, err := tree.ExactCtx.Mul(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						_, err := tree.ExactCtx.Mul(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					_, err := tree.ExactCtx.Mul(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					_, err := tree.ExactCtx.Mul(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultDecimalDecimalConstOp) Init() {
	p.input.Init()
}

type projMultDecimalIntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projMultDecimalIntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					f, err := arg.Float64()
					if err != nil {
						colexecerror.InternalError(err)
					}
					projCol[i] = p.constArg.MulFloat(f)
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					f, err := arg.Float64()
					if err != nil {
						colexecerror.InternalError(err)
					}
					projCol[i] = p.constArg.MulFloat(f)
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				f, err := arg.Float64()
				if err != nil {
					colexecerror.InternalError(err)
				}
				projCol[i] = p.constArg.MulFloat(f)
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				f, err := arg.Float64()
				if err != nil {
					colexecerror.InternalError(err)
				}
				projCol[i] = p.constArg.MulFloat(f)
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultDecimalIntervalConstOp) Init() {
	p.input.Init()
}

type projMultInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projMultInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt8 || _left < math.MinInt8 || _right > math.MaxInt8 || _right < math.MinInt8 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt8 || _left < math.MinInt8 || _right > math.MaxInt8 || _right < math.MinInt8 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt8 || _left < math.MinInt8 || _right > math.MaxInt8 || _right < math.MinInt8 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt8 || _left < math.MinInt8 || _right > math.MaxInt8 || _right < math.MinInt8 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projMultInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projMultInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projMultInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projMultInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projMultInt16DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projMultInt16DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Mul(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Mul(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Mul(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Mul(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt16DecimalConstOp) Init() {
	p.input.Init()
}

type projMultInt16IntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projMultInt16IntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = p.constArg.Mul(int64(arg))
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = p.constArg.Mul(int64(arg))
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = p.constArg.Mul(int64(arg))
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = p.constArg.Mul(int64(arg))
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt16IntervalConstOp) Init() {
	p.input.Init()
}

type projMultInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projMultInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projMultInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projMultInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt16 || _left < math.MinInt16 || _right > math.MaxInt16 || _right < math.MinInt16 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt16 || _left < math.MinInt16 || _right > math.MaxInt16 || _right < math.MinInt16 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt16 || _left < math.MinInt16 || _right > math.MaxInt16 || _right < math.MinInt16 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt16 || _left < math.MinInt16 || _right > math.MaxInt16 || _right < math.MinInt16 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projMultInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projMultInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projMultInt32DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projMultInt32DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Mul(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Mul(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Mul(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Mul(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt32DecimalConstOp) Init() {
	p.input.Init()
}

type projMultInt32IntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projMultInt32IntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = p.constArg.Mul(int64(arg))
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = p.constArg.Mul(int64(arg))
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = p.constArg.Mul(int64(arg))
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = p.constArg.Mul(int64(arg))
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt32IntervalConstOp) Init() {
	p.input.Init()
}

type projMultInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projMultInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projMultInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projMultInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projMultInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projMultInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						_left, _right := int64(arg), int64(p.constArg)
						result := _left * _right
						if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
							if _left != 0 && _right != 0 {
								sameSign := (_left < 0) == (_right < 0)
								if (result < 0) == sameSign {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								} else if result/_right != _left {
									colexecerror.ExpectedError(tree.ErrIntOutOfRange)
								}
							}
						}
						projCol[i] = result
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					_left, _right := int64(arg), int64(p.constArg)
					result := _left * _right
					if _left > math.MaxInt32 || _left < math.MinInt32 || _right > math.MaxInt32 || _right < math.MinInt32 {
						if _left != 0 && _right != 0 {
							sameSign := (_left < 0) == (_right < 0)
							if (result < 0) == sameSign {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							} else if result/_right != _left {
								colexecerror.ExpectedError(tree.ErrIntOutOfRange)
							}
						}
					}
					projCol[i] = result
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projMultInt64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projMultInt64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Mul(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.ExactCtx.Mul(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Mul(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.ExactCtx.Mul(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt64DecimalConstOp) Init() {
	p.input.Init()
}

type projMultInt64IntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projMultInt64IntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = p.constArg.Mul(int64(arg))
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = p.constArg.Mul(int64(arg))
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = p.constArg.Mul(int64(arg))
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = p.constArg.Mul(int64(arg))
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultInt64IntervalConstOp) Init() {
	p.input.Init()
}

type projMultFloat64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projMultFloat64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Float64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						projCol[i] = float64(arg) * float64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						projCol[i] = float64(arg) * float64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					projCol[i] = float64(arg) * float64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					projCol[i] = float64(arg) * float64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultFloat64Float64ConstOp) Init() {
	p.input.Init()
}

type projMultFloat64IntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projMultFloat64IntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = p.constArg.MulFloat(float64(arg))
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = p.constArg.MulFloat(float64(arg))
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = p.constArg.MulFloat(float64(arg))
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = p.constArg.MulFloat(float64(arg))
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultFloat64IntervalConstOp) Init() {
	p.input.Init()
}

type projMultIntervalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projMultIntervalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = arg.Mul(int64(p.constArg))
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = arg.Mul(int64(p.constArg))
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = arg.Mul(int64(p.constArg))
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = arg.Mul(int64(p.constArg))
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultIntervalInt16ConstOp) Init() {
	p.input.Init()
}

type projMultIntervalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projMultIntervalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = arg.Mul(int64(p.constArg))
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = arg.Mul(int64(p.constArg))
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = arg.Mul(int64(p.constArg))
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = arg.Mul(int64(p.constArg))
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultIntervalInt32ConstOp) Init() {
	p.input.Init()
}

type projMultIntervalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projMultIntervalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = arg.Mul(int64(p.constArg))
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = arg.Mul(int64(p.constArg))
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = arg.Mul(int64(p.constArg))
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = arg.Mul(int64(p.constArg))
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultIntervalInt64ConstOp) Init() {
	p.input.Init()
}

type projMultIntervalFloat64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projMultIntervalFloat64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = arg.MulFloat(float64(p.constArg))
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline
					projCol[i] = arg.MulFloat(float64(p.constArg))
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = arg.MulFloat(float64(p.constArg))
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline
				projCol[i] = arg.MulFloat(float64(p.constArg))
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultIntervalFloat64ConstOp) Init() {
	p.input.Init()
}

type projMultIntervalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projMultIntervalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					f, err := p.constArg.Float64()
					if err != nil {
						colexecerror.InternalError(err)
					}
					projCol[i] = arg.MulFloat(f)
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					f, err := p.constArg.Float64()
					if err != nil {
						colexecerror.InternalError(err)
					}
					projCol[i] = arg.MulFloat(f)
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				f, err := p.constArg.Float64()
				if err != nil {
					colexecerror.InternalError(err)
				}
				projCol[i] = arg.MulFloat(f)
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				f, err := p.constArg.Float64()
				if err != nil {
					colexecerror.InternalError(err)
				}
				projCol[i] = arg.MulFloat(f)
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projMultIntervalDecimalConstOp) Init() {
	p.input.Init()
}

type projDivDecimalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projDivDecimalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivDecimalInt16ConstOp) Init() {
	p.input.Init()
}

type projDivDecimalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projDivDecimalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivDecimalInt32ConstOp) Init() {
	p.input.Init()
}

type projDivDecimalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projDivDecimalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivDecimalInt64ConstOp) Init() {
	p.input.Init()
}

type projDivDecimalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projDivDecimalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						_, err := tree.DecimalCtx.Quo(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						_, err := tree.DecimalCtx.Quo(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					_, err := tree.DecimalCtx.Quo(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					_, err := tree.DecimalCtx.Quo(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivDecimalDecimalConstOp) Init() {
	p.input.Init()
}

type projDivInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projDivInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projDivInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projDivInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projDivInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projDivInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projDivInt16DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projDivInt16DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.DecimalCtx.Quo(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.DecimalCtx.Quo(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.DecimalCtx.Quo(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.DecimalCtx.Quo(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivInt16DecimalConstOp) Init() {
	p.input.Init()
}

type projDivInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projDivInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projDivInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projDivInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projDivInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projDivInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projDivInt32DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projDivInt32DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.DecimalCtx.Quo(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.DecimalCtx.Quo(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.DecimalCtx.Quo(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.DecimalCtx.Quo(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivInt32DecimalConstOp) Init() {
	p.input.Init()
}

type projDivInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projDivInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projDivInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projDivInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projDivInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projDivInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Quo(&projCol[i], leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projDivInt64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projDivInt64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.DecimalCtx.Quo(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.DecimalCtx.Quo(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.DecimalCtx.Quo(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.DecimalCtx.Quo(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivInt64DecimalConstOp) Init() {
	p.input.Init()
}

type projDivFloat64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projDivFloat64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Float64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0.0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						projCol[i] = float64(arg) / float64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0.0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						projCol[i] = float64(arg) / float64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0.0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					projCol[i] = float64(arg) / float64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0.0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					projCol[i] = float64(arg) / float64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivFloat64Float64ConstOp) Init() {
	p.input.Init()
}

type projDivIntervalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projDivIntervalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = arg.Div(int64(p.constArg))
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = arg.Div(int64(p.constArg))
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				if p.constArg == 0 {
					colexecerror.ExpectedError(tree.ErrDivByZero)
				}
				projCol[i] = arg.Div(int64(p.constArg))
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				if p.constArg == 0 {
					colexecerror.ExpectedError(tree.ErrDivByZero)
				}
				projCol[i] = arg.Div(int64(p.constArg))
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivIntervalInt64ConstOp) Init() {
	p.input.Init()
}

type projDivIntervalFloat64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projDivIntervalFloat64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Interval()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					if p.constArg == 0.0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = arg.DivFloat(float64(p.constArg))
				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					if p.constArg == 0.0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = arg.DivFloat(float64(p.constArg))
				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				if p.constArg == 0.0 {
					colexecerror.ExpectedError(tree.ErrDivByZero)
				}
				projCol[i] = arg.DivFloat(float64(p.constArg))
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				if p.constArg == 0.0 {
					colexecerror.ExpectedError(tree.ErrDivByZero)
				}
				projCol[i] = arg.DivFloat(float64(p.constArg))
			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projDivIntervalFloat64ConstOp) Init() {
	p.input.Init()
}

type projFloorDivDecimalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projFloorDivDecimalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivDecimalInt16ConstOp) Init() {
	p.input.Init()
}

type projFloorDivDecimalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projFloorDivDecimalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivDecimalInt32ConstOp) Init() {
	p.input.Init()
}

type projFloorDivDecimalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projFloorDivDecimalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivDecimalInt64ConstOp) Init() {
	p.input.Init()
}

type projFloorDivDecimalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projFloorDivDecimalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivDecimalDecimalConstOp) Init() {
	p.input.Init()
}

type projFloorDivInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projFloorDivInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projFloorDivInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projFloorDivInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projFloorDivInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projFloorDivInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projFloorDivInt16DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projFloorDivInt16DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivInt16DecimalConstOp) Init() {
	p.input.Init()
}

type projFloorDivInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projFloorDivInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projFloorDivInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projFloorDivInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projFloorDivInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projFloorDivInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projFloorDivInt32DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projFloorDivInt32DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivInt32DecimalConstOp) Init() {
	p.input.Init()
}

type projFloorDivInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projFloorDivInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projFloorDivInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projFloorDivInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projFloorDivInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projFloorDivInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) // int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) // int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projFloorDivInt64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projFloorDivInt64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.HighPrecisionCtx.QuoInteger(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivInt64DecimalConstOp) Init() {
	p.input.Init()
}

type projFloorDivFloat64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projFloorDivFloat64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Float64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0.0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						projCol[i] = math.Trunc(float64(arg) / float64(p.constArg))
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0.0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						projCol[i] = math.Trunc(float64(arg) / float64(p.constArg))
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0.0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					projCol[i] = math.Trunc(float64(arg) / float64(p.constArg))
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0.0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					projCol[i] = math.Trunc(float64(arg) / float64(p.constArg))
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projFloorDivFloat64Float64ConstOp) Init() {
	p.input.Init()
}

type projModDecimalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projModDecimalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModDecimalInt16ConstOp) Init() {
	p.input.Init()
}

type projModDecimalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projModDecimalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModDecimalInt32ConstOp) Init() {
	p.input.Init()
}

type projModDecimalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projModDecimalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModDecimalInt64ConstOp) Init() {
	p.input.Init()
}

type projModDecimalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projModDecimalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						_, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						_, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					_, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					_, err := tree.HighPrecisionCtx.Rem(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModDecimalDecimalConstOp) Init() {
	p.input.Init()
}

type projModInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projModInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projModInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projModInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projModInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projModInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projModInt16DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projModInt16DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.HighPrecisionCtx.Rem(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.HighPrecisionCtx.Rem(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.HighPrecisionCtx.Rem(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.HighPrecisionCtx.Rem(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModInt16DecimalConstOp) Init() {
	p.input.Init()
}

type projModInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projModInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projModInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projModInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projModInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projModInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projModInt32DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projModInt32DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.HighPrecisionCtx.Rem(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.HighPrecisionCtx.Rem(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.HighPrecisionCtx.Rem(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.HighPrecisionCtx.Rem(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModInt32DecimalConstOp) Init() {
	p.input.Init()
}

type projModInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projModInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projModInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projModInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projModInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projModInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) == 0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}
						projCol[i] = int64(arg) % int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) == 0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}
					projCol[i] = int64(arg) % int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projModInt64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projModInt64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.HighPrecisionCtx.Rem(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg.IsZero() {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.HighPrecisionCtx.Rem(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.HighPrecisionCtx.Rem(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg.IsZero() {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.HighPrecisionCtx.Rem(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModInt64DecimalConstOp) Init() {
	p.input.Init()
}

type projModFloat64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projModFloat64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Float64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0.0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						projCol[i] = math.Mod(float64(arg), float64(p.constArg))
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						if p.constArg == 0.0 {
							colexecerror.ExpectedError(tree.ErrDivByZero)
						}

						projCol[i] = math.Mod(float64(arg), float64(p.constArg))
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0.0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					projCol[i] = math.Mod(float64(arg), float64(p.constArg))
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					if p.constArg == 0.0 {
						colexecerror.ExpectedError(tree.ErrDivByZero)
					}

					projCol[i] = math.Mod(float64(arg), float64(p.constArg))
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projModFloat64Float64ConstOp) Init() {
	p.input.Init()
}

type projPowDecimalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projPowDecimalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.DecimalCtx.Pow(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.DecimalCtx.Pow(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.DecimalCtx.Pow(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.DecimalCtx.Pow(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowDecimalInt16ConstOp) Init() {
	p.input.Init()
}

type projPowDecimalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projPowDecimalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.DecimalCtx.Pow(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.DecimalCtx.Pow(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.DecimalCtx.Pow(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.DecimalCtx.Pow(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowDecimalInt32ConstOp) Init() {
	p.input.Init()
}

type projPowDecimalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projPowDecimalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.DecimalCtx.Pow(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						if _, err := tree.DecimalCtx.Pow(&projCol[i], &arg, tmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.DecimalCtx.Pow(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(p.constArg))
					if _, err := tree.DecimalCtx.Pow(&projCol[i], &arg, tmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowDecimalInt64ConstOp) Init() {
	p.input.Init()
}

type projPowDecimalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projPowDecimalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						_, err := tree.DecimalCtx.Pow(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						_, err := tree.DecimalCtx.Pow(&projCol[i], &arg, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					_, err := tree.DecimalCtx.Pow(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					_, err := tree.DecimalCtx.Pow(&projCol[i], &arg, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowDecimalDecimalConstOp) Init() {
	p.input.Init()
}

type projPowInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projPowInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projPowInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projPowInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projPowInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projPowInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projPowInt16DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projPowInt16DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.DecimalCtx.Pow(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.DecimalCtx.Pow(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.DecimalCtx.Pow(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.DecimalCtx.Pow(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowInt16DecimalConstOp) Init() {
	p.input.Init()
}

type projPowInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projPowInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projPowInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projPowInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projPowInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projPowInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projPowInt32DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projPowInt32DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.DecimalCtx.Pow(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.DecimalCtx.Pow(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.DecimalCtx.Pow(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.DecimalCtx.Pow(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowInt32DecimalConstOp) Init() {
	p.input.Init()
}

type projPowInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projPowInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projPowInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projPowInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projPowInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projPowInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
						leftTmpDec.SetInt64(int64(int64(arg)))
						rightTmpDec.SetInt64(int64(int64(p.constArg)))
						if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
							colexecerror.ExpectedError(err)
						}
						resultInt, err := leftTmpDec.Int64()
						if err != nil {
							colexecerror.ExpectedError(tree.ErrIntOutOfRange)
						}
						projCol[i] = resultInt
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					leftTmpDec, rightTmpDec := &_overloadHelper.tmpDec1, &_overloadHelper.tmpDec2
					leftTmpDec.SetInt64(int64(int64(arg)))
					rightTmpDec.SetInt64(int64(int64(p.constArg)))
					if _, err := tree.DecimalCtx.Pow(leftTmpDec, leftTmpDec, rightTmpDec); err != nil {
						colexecerror.ExpectedError(err)
					}
					resultInt, err := leftTmpDec.Int64()
					if err != nil {
						colexecerror.ExpectedError(tree.ErrIntOutOfRange)
					}
					projCol[i] = resultInt
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projPowInt64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projPowInt64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Decimal()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.DecimalCtx.Pow(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						_, err := tree.DecimalCtx.Pow(&projCol[i], tmpDec, &p.constArg)
						if err != nil {
							colexecerror.ExpectedError(err)
						}
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.DecimalCtx.Pow(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					tmpDec := &_overloadHelper.tmpDec1
					tmpDec.SetInt64(int64(arg))
					_, err := tree.DecimalCtx.Pow(&projCol[i], tmpDec, &p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowInt64DecimalConstOp) Init() {
	p.input.Init()
}

type projPowFloat64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projPowFloat64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Float64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						projCol[i] = math.Pow(float64(arg), float64(p.constArg))
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{

						projCol[i] = math.Pow(float64(arg), float64(p.constArg))
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{

					projCol[i] = math.Pow(float64(arg), float64(p.constArg))
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{

					projCol[i] = math.Pow(float64(arg), float64(p.constArg))
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projPowFloat64Float64ConstOp) Init() {
	p.input.Init()
}

type projConcatBytesBytesConstOp struct {
	projConstOpBase
	constArg []byte
}

func (p projConcatBytesBytesConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col *coldata.Bytes
	col = vec.Bytes()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bytes()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var r = []byte{}
						r = append(r, arg...)
						r = append(r, p.constArg...)
						projCol.Set(i, r)
					}

				}
			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var r = []byte{}
						r = append(r, arg...)
						r = append(r, p.constArg...)
						projCol.Set(i, r)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var r = []byte{}
					r = append(r, arg...)
					r = append(r, p.constArg...)
					projCol.Set(i, r)
				}

			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var r = []byte{}
					r = append(r, arg...)
					r = append(r, p.constArg...)
					projCol.Set(i, r)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projConcatBytesBytesConstOp) Init() {
	p.input.Init()
}

type projConcatDatumDatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projConcatDatumDatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, p.constArg)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projConcatDatumDatumConstOp) Init() {
	p.input.Init()
}

type projLShiftInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLShiftInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLShiftInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projLShiftInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLShiftInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLShiftInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projLShiftInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLShiftInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLShiftInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projLShiftInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLShiftInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLShiftInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projLShiftInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLShiftInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLShiftInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projLShiftInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLShiftInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLShiftInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projLShiftInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLShiftInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLShiftInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projLShiftInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLShiftInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLShiftInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projLShiftInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLShiftInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) << int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeLShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) << int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLShiftInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projLShiftDatumInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLShiftDatumInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLShiftDatumInt16ConstOp) Init() {
	p.input.Init()
}

type projLShiftDatumInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLShiftDatumInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLShiftDatumInt32ConstOp) Init() {
	p.input.Init()
}

type projLShiftDatumInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLShiftDatumInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLShiftDatumInt64ConstOp) Init() {
	p.input.Init()
}

type projRShiftInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projRShiftInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projRShiftInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projRShiftInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projRShiftInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projRShiftInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projRShiftInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projRShiftInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projRShiftInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projRShiftInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projRShiftInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projRShiftInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projRShiftInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projRShiftInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projRShiftInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projRShiftInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projRShiftInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projRShiftInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projRShiftInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projRShiftInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projRShiftInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projRShiftInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projRShiftInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projRShiftInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projRShiftInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projRShiftInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Int64()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
							telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
							colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
						}
						projCol[i] = int64(arg) >> int64(p.constArg)
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					if int64(p.constArg) < 0 || int64(p.constArg) >= 64 {
						telemetry.Inc(sqltelemetry.LargeRShiftArgumentCounter)
						colexecerror.ExpectedError(tree.ErrShiftArgOutOfRange)
					}
					projCol[i] = int64(arg) >> int64(p.constArg)
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projRShiftInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projRShiftDatumInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projRShiftDatumInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projRShiftDatumInt16ConstOp) Init() {
	p.input.Init()
}

type projRShiftDatumInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projRShiftDatumInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projRShiftDatumInt32ConstOp) Init() {
	p.input.Init()
}

type projRShiftDatumInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projRShiftDatumInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projRShiftDatumInt64ConstOp) Init() {
	p.input.Init()
}

type projJSONFetchValDatumBytesConstOp struct {
	projConstOpBase
	constArg []byte
}

func (p projJSONFetchValDatumBytesConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DString(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DString(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DString(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DString(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projJSONFetchValDatumBytesConstOp) Init() {
	p.input.Init()
}

type projJSONFetchValDatumInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projJSONFetchValDatumInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projJSONFetchValDatumInt16ConstOp) Init() {
	p.input.Init()
}

type projJSONFetchValDatumInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projJSONFetchValDatumInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projJSONFetchValDatumInt32ConstOp) Init() {
	p.input.Init()
}

type projJSONFetchValDatumInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projJSONFetchValDatumInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Datum()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					_convertedNativeElem := tree.DInt(p.constArg)
					var _nonDatumArgAsDatum tree.Datum
					_nonDatumArgAsDatum = &_convertedNativeElem

					_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
					if err != nil {
						colexecerror.ExpectedError(err)
					}
					projCol.Set(i, _res)

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1)
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				_convertedNativeElem := tree.DInt(p.constArg)
				var _nonDatumArgAsDatum tree.Datum
				_nonDatumArgAsDatum = &_convertedNativeElem

				_res, err := arg.(*coldataext.Datum).BinFn(_overloadHelper.binFn, _overloadHelper.evalCtx, _nonDatumArgAsDatum)
				if err != nil {
					colexecerror.ExpectedError(err)
				}
				projCol.Set(i, _res)

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projJSONFetchValDatumInt64ConstOp) Init() {
	p.input.Init()
}

type projEQBoolBoolConstOp struct {
	projConstOpBase
	constArg bool
}

func (p projEQBoolBoolConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Bools
	col = vec.Bool()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if !arg && p.constArg {
							cmpResult = -1
						} else if arg && !p.constArg {
							cmpResult = 1
						} else {
							cmpResult = 0
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if !arg && p.constArg {
							cmpResult = -1
						} else if arg && !p.constArg {
							cmpResult = 1
						} else {
							cmpResult = 0
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if !arg && p.constArg {
						cmpResult = -1
					} else if arg && !p.constArg {
						cmpResult = 1
					} else {
						cmpResult = 0
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if !arg && p.constArg {
						cmpResult = -1
					} else if arg && !p.constArg {
						cmpResult = 1
					} else {
						cmpResult = 0
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQBoolBoolConstOp) Init() {
	p.input.Init()
}

type projEQBytesBytesConstOp struct {
	projConstOpBase
	constArg []byte
}

func (p projEQBytesBytesConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col *coldata.Bytes
	col = vec.Bytes()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = bytes.Compare(arg, p.constArg)
						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = bytes.Compare(arg, p.constArg)
						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = bytes.Compare(arg, p.constArg)
					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = bytes.Compare(arg, p.constArg)
					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQBytesBytesConstOp) Init() {
	p.input.Init()
}

type projEQDecimalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projEQDecimalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQDecimalInt16ConstOp) Init() {
	p.input.Init()
}

type projEQDecimalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projEQDecimalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQDecimalInt32ConstOp) Init() {
	p.input.Init()
}

type projEQDecimalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projEQDecimalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQDecimalInt64ConstOp) Init() {
	p.input.Init()
}

type projEQDecimalFloat64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projEQDecimalFloat64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQDecimalFloat64ConstOp) Init() {
	p.input.Init()
}

type projEQDecimalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projEQDecimalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = tree.CompareDecimals(&arg, &p.constArg)
						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = tree.CompareDecimals(&arg, &p.constArg)
						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = tree.CompareDecimals(&arg, &p.constArg)
					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = tree.CompareDecimals(&arg, &p.constArg)
					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQDecimalDecimalConstOp) Init() {
	p.input.Init()
}

type projEQInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projEQInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projEQInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projEQInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projEQInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projEQInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projEQInt16Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projEQInt16Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt16Float64ConstOp) Init() {
	p.input.Init()
}

type projEQInt16DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projEQInt16DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt16DecimalConstOp) Init() {
	p.input.Init()
}

type projEQInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projEQInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projEQInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projEQInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projEQInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projEQInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projEQInt32Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projEQInt32Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt32Float64ConstOp) Init() {
	p.input.Init()
}

type projEQInt32DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projEQInt32DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt32DecimalConstOp) Init() {
	p.input.Init()
}

type projEQInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projEQInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projEQInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projEQInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projEQInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projEQInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projEQInt64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projEQInt64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt64Float64ConstOp) Init() {
	p.input.Init()
}

type projEQInt64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projEQInt64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQInt64DecimalConstOp) Init() {
	p.input.Init()
}

type projEQFloat64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projEQFloat64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQFloat64Int16ConstOp) Init() {
	p.input.Init()
}

type projEQFloat64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projEQFloat64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQFloat64Int32ConstOp) Init() {
	p.input.Init()
}

type projEQFloat64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projEQFloat64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQFloat64Int64ConstOp) Init() {
	p.input.Init()
}

type projEQFloat64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projEQFloat64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQFloat64Float64ConstOp) Init() {
	p.input.Init()
}

type projEQFloat64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projEQFloat64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQFloat64DecimalConstOp) Init() {
	p.input.Init()
}

type projEQTimestampTimestampConstOp struct {
	projConstOpBase
	constArg time.Time
}

func (p projEQTimestampTimestampConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Times
	col = vec.Timestamp()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if arg.Before(p.constArg) {
							cmpResult = -1
						} else if p.constArg.Before(arg) {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if arg.Before(p.constArg) {
							cmpResult = -1
						} else if p.constArg.Before(arg) {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if arg.Before(p.constArg) {
						cmpResult = -1
					} else if p.constArg.Before(arg) {
						cmpResult = 1
					} else {
						cmpResult = 0
					}
					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if arg.Before(p.constArg) {
						cmpResult = -1
					} else if p.constArg.Before(arg) {
						cmpResult = 1
					} else {
						cmpResult = 0
					}
					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQTimestampTimestampConstOp) Init() {
	p.input.Init()
}

type projEQIntervalIntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projEQIntervalIntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = arg.Compare(p.constArg)
						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = arg.Compare(p.constArg)
						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = arg.Compare(p.constArg)
					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = arg.Compare(p.constArg)
					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQIntervalIntervalConstOp) Init() {
	p.input.Init()
}

type projEQDatumDatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projEQDatumDatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					{
						var cmpResult int

						cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

						projCol[i] = cmpResult == 0
					}

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					{
						var cmpResult int

						cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

						projCol[i] = cmpResult == 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				{
					var cmpResult int

					cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

					projCol[i] = cmpResult == 0
				}

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				{
					var cmpResult int

					cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

					projCol[i] = cmpResult == 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projEQDatumDatumConstOp) Init() {
	p.input.Init()
}

type projNEBoolBoolConstOp struct {
	projConstOpBase
	constArg bool
}

func (p projNEBoolBoolConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Bools
	col = vec.Bool()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if !arg && p.constArg {
							cmpResult = -1
						} else if arg && !p.constArg {
							cmpResult = 1
						} else {
							cmpResult = 0
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if !arg && p.constArg {
							cmpResult = -1
						} else if arg && !p.constArg {
							cmpResult = 1
						} else {
							cmpResult = 0
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if !arg && p.constArg {
						cmpResult = -1
					} else if arg && !p.constArg {
						cmpResult = 1
					} else {
						cmpResult = 0
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if !arg && p.constArg {
						cmpResult = -1
					} else if arg && !p.constArg {
						cmpResult = 1
					} else {
						cmpResult = 0
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEBoolBoolConstOp) Init() {
	p.input.Init()
}

type projNEBytesBytesConstOp struct {
	projConstOpBase
	constArg []byte
}

func (p projNEBytesBytesConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col *coldata.Bytes
	col = vec.Bytes()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = bytes.Compare(arg, p.constArg)
						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = bytes.Compare(arg, p.constArg)
						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = bytes.Compare(arg, p.constArg)
					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = bytes.Compare(arg, p.constArg)
					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEBytesBytesConstOp) Init() {
	p.input.Init()
}

type projNEDecimalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projNEDecimalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEDecimalInt16ConstOp) Init() {
	p.input.Init()
}

type projNEDecimalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projNEDecimalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEDecimalInt32ConstOp) Init() {
	p.input.Init()
}

type projNEDecimalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projNEDecimalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEDecimalInt64ConstOp) Init() {
	p.input.Init()
}

type projNEDecimalFloat64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projNEDecimalFloat64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEDecimalFloat64ConstOp) Init() {
	p.input.Init()
}

type projNEDecimalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projNEDecimalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = tree.CompareDecimals(&arg, &p.constArg)
						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = tree.CompareDecimals(&arg, &p.constArg)
						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = tree.CompareDecimals(&arg, &p.constArg)
					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = tree.CompareDecimals(&arg, &p.constArg)
					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEDecimalDecimalConstOp) Init() {
	p.input.Init()
}

type projNEInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projNEInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projNEInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projNEInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projNEInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projNEInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projNEInt16Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projNEInt16Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt16Float64ConstOp) Init() {
	p.input.Init()
}

type projNEInt16DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projNEInt16DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt16DecimalConstOp) Init() {
	p.input.Init()
}

type projNEInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projNEInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projNEInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projNEInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projNEInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projNEInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projNEInt32Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projNEInt32Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt32Float64ConstOp) Init() {
	p.input.Init()
}

type projNEInt32DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projNEInt32DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt32DecimalConstOp) Init() {
	p.input.Init()
}

type projNEInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projNEInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projNEInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projNEInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projNEInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projNEInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projNEInt64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projNEInt64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt64Float64ConstOp) Init() {
	p.input.Init()
}

type projNEInt64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projNEInt64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEInt64DecimalConstOp) Init() {
	p.input.Init()
}

type projNEFloat64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projNEFloat64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEFloat64Int16ConstOp) Init() {
	p.input.Init()
}

type projNEFloat64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projNEFloat64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEFloat64Int32ConstOp) Init() {
	p.input.Init()
}

type projNEFloat64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projNEFloat64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEFloat64Int64ConstOp) Init() {
	p.input.Init()
}

type projNEFloat64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projNEFloat64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEFloat64Float64ConstOp) Init() {
	p.input.Init()
}

type projNEFloat64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projNEFloat64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEFloat64DecimalConstOp) Init() {
	p.input.Init()
}

type projNETimestampTimestampConstOp struct {
	projConstOpBase
	constArg time.Time
}

func (p projNETimestampTimestampConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Times
	col = vec.Timestamp()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if arg.Before(p.constArg) {
							cmpResult = -1
						} else if p.constArg.Before(arg) {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if arg.Before(p.constArg) {
							cmpResult = -1
						} else if p.constArg.Before(arg) {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if arg.Before(p.constArg) {
						cmpResult = -1
					} else if p.constArg.Before(arg) {
						cmpResult = 1
					} else {
						cmpResult = 0
					}
					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if arg.Before(p.constArg) {
						cmpResult = -1
					} else if p.constArg.Before(arg) {
						cmpResult = 1
					} else {
						cmpResult = 0
					}
					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNETimestampTimestampConstOp) Init() {
	p.input.Init()
}

type projNEIntervalIntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projNEIntervalIntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = arg.Compare(p.constArg)
						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = arg.Compare(p.constArg)
						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = arg.Compare(p.constArg)
					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = arg.Compare(p.constArg)
					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEIntervalIntervalConstOp) Init() {
	p.input.Init()
}

type projNEDatumDatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projNEDatumDatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					{
						var cmpResult int

						cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

						projCol[i] = cmpResult != 0
					}

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					{
						var cmpResult int

						cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

						projCol[i] = cmpResult != 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				{
					var cmpResult int

					cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

					projCol[i] = cmpResult != 0
				}

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				{
					var cmpResult int

					cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

					projCol[i] = cmpResult != 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projNEDatumDatumConstOp) Init() {
	p.input.Init()
}

type projLTBoolBoolConstOp struct {
	projConstOpBase
	constArg bool
}

func (p projLTBoolBoolConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Bools
	col = vec.Bool()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if !arg && p.constArg {
							cmpResult = -1
						} else if arg && !p.constArg {
							cmpResult = 1
						} else {
							cmpResult = 0
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if !arg && p.constArg {
							cmpResult = -1
						} else if arg && !p.constArg {
							cmpResult = 1
						} else {
							cmpResult = 0
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if !arg && p.constArg {
						cmpResult = -1
					} else if arg && !p.constArg {
						cmpResult = 1
					} else {
						cmpResult = 0
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if !arg && p.constArg {
						cmpResult = -1
					} else if arg && !p.constArg {
						cmpResult = 1
					} else {
						cmpResult = 0
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTBoolBoolConstOp) Init() {
	p.input.Init()
}

type projLTBytesBytesConstOp struct {
	projConstOpBase
	constArg []byte
}

func (p projLTBytesBytesConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col *coldata.Bytes
	col = vec.Bytes()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = bytes.Compare(arg, p.constArg)
						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = bytes.Compare(arg, p.constArg)
						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = bytes.Compare(arg, p.constArg)
					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = bytes.Compare(arg, p.constArg)
					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTBytesBytesConstOp) Init() {
	p.input.Init()
}

type projLTDecimalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLTDecimalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTDecimalInt16ConstOp) Init() {
	p.input.Init()
}

type projLTDecimalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLTDecimalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTDecimalInt32ConstOp) Init() {
	p.input.Init()
}

type projLTDecimalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLTDecimalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTDecimalInt64ConstOp) Init() {
	p.input.Init()
}

type projLTDecimalFloat64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projLTDecimalFloat64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTDecimalFloat64ConstOp) Init() {
	p.input.Init()
}

type projLTDecimalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projLTDecimalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = tree.CompareDecimals(&arg, &p.constArg)
						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = tree.CompareDecimals(&arg, &p.constArg)
						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = tree.CompareDecimals(&arg, &p.constArg)
					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = tree.CompareDecimals(&arg, &p.constArg)
					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTDecimalDecimalConstOp) Init() {
	p.input.Init()
}

type projLTInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLTInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projLTInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLTInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projLTInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLTInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projLTInt16Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projLTInt16Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt16Float64ConstOp) Init() {
	p.input.Init()
}

type projLTInt16DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projLTInt16DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt16DecimalConstOp) Init() {
	p.input.Init()
}

type projLTInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLTInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projLTInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLTInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projLTInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLTInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projLTInt32Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projLTInt32Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt32Float64ConstOp) Init() {
	p.input.Init()
}

type projLTInt32DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projLTInt32DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt32DecimalConstOp) Init() {
	p.input.Init()
}

type projLTInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLTInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projLTInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLTInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projLTInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLTInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projLTInt64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projLTInt64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt64Float64ConstOp) Init() {
	p.input.Init()
}

type projLTInt64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projLTInt64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTInt64DecimalConstOp) Init() {
	p.input.Init()
}

type projLTFloat64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLTFloat64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTFloat64Int16ConstOp) Init() {
	p.input.Init()
}

type projLTFloat64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLTFloat64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTFloat64Int32ConstOp) Init() {
	p.input.Init()
}

type projLTFloat64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLTFloat64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTFloat64Int64ConstOp) Init() {
	p.input.Init()
}

type projLTFloat64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projLTFloat64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTFloat64Float64ConstOp) Init() {
	p.input.Init()
}

type projLTFloat64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projLTFloat64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTFloat64DecimalConstOp) Init() {
	p.input.Init()
}

type projLTTimestampTimestampConstOp struct {
	projConstOpBase
	constArg time.Time
}

func (p projLTTimestampTimestampConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Times
	col = vec.Timestamp()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if arg.Before(p.constArg) {
							cmpResult = -1
						} else if p.constArg.Before(arg) {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if arg.Before(p.constArg) {
							cmpResult = -1
						} else if p.constArg.Before(arg) {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if arg.Before(p.constArg) {
						cmpResult = -1
					} else if p.constArg.Before(arg) {
						cmpResult = 1
					} else {
						cmpResult = 0
					}
					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if arg.Before(p.constArg) {
						cmpResult = -1
					} else if p.constArg.Before(arg) {
						cmpResult = 1
					} else {
						cmpResult = 0
					}
					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTTimestampTimestampConstOp) Init() {
	p.input.Init()
}

type projLTIntervalIntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projLTIntervalIntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = arg.Compare(p.constArg)
						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = arg.Compare(p.constArg)
						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = arg.Compare(p.constArg)
					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = arg.Compare(p.constArg)
					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTIntervalIntervalConstOp) Init() {
	p.input.Init()
}

type projLTDatumDatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projLTDatumDatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					{
						var cmpResult int

						cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

						projCol[i] = cmpResult < 0
					}

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					{
						var cmpResult int

						cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

						projCol[i] = cmpResult < 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				{
					var cmpResult int

					cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

					projCol[i] = cmpResult < 0
				}

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				{
					var cmpResult int

					cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

					projCol[i] = cmpResult < 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLTDatumDatumConstOp) Init() {
	p.input.Init()
}

type projLEBoolBoolConstOp struct {
	projConstOpBase
	constArg bool
}

func (p projLEBoolBoolConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Bools
	col = vec.Bool()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if !arg && p.constArg {
							cmpResult = -1
						} else if arg && !p.constArg {
							cmpResult = 1
						} else {
							cmpResult = 0
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if !arg && p.constArg {
							cmpResult = -1
						} else if arg && !p.constArg {
							cmpResult = 1
						} else {
							cmpResult = 0
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if !arg && p.constArg {
						cmpResult = -1
					} else if arg && !p.constArg {
						cmpResult = 1
					} else {
						cmpResult = 0
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if !arg && p.constArg {
						cmpResult = -1
					} else if arg && !p.constArg {
						cmpResult = 1
					} else {
						cmpResult = 0
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEBoolBoolConstOp) Init() {
	p.input.Init()
}

type projLEBytesBytesConstOp struct {
	projConstOpBase
	constArg []byte
}

func (p projLEBytesBytesConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col *coldata.Bytes
	col = vec.Bytes()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = bytes.Compare(arg, p.constArg)
						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = bytes.Compare(arg, p.constArg)
						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = bytes.Compare(arg, p.constArg)
					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = bytes.Compare(arg, p.constArg)
					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEBytesBytesConstOp) Init() {
	p.input.Init()
}

type projLEDecimalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLEDecimalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEDecimalInt16ConstOp) Init() {
	p.input.Init()
}

type projLEDecimalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLEDecimalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEDecimalInt32ConstOp) Init() {
	p.input.Init()
}

type projLEDecimalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLEDecimalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEDecimalInt64ConstOp) Init() {
	p.input.Init()
}

type projLEDecimalFloat64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projLEDecimalFloat64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEDecimalFloat64ConstOp) Init() {
	p.input.Init()
}

type projLEDecimalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projLEDecimalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = tree.CompareDecimals(&arg, &p.constArg)
						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = tree.CompareDecimals(&arg, &p.constArg)
						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = tree.CompareDecimals(&arg, &p.constArg)
					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = tree.CompareDecimals(&arg, &p.constArg)
					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEDecimalDecimalConstOp) Init() {
	p.input.Init()
}

type projLEInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLEInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projLEInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLEInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projLEInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLEInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projLEInt16Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projLEInt16Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt16Float64ConstOp) Init() {
	p.input.Init()
}

type projLEInt16DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projLEInt16DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt16DecimalConstOp) Init() {
	p.input.Init()
}

type projLEInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLEInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projLEInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLEInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projLEInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLEInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projLEInt32Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projLEInt32Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt32Float64ConstOp) Init() {
	p.input.Init()
}

type projLEInt32DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projLEInt32DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt32DecimalConstOp) Init() {
	p.input.Init()
}

type projLEInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLEInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projLEInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLEInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projLEInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLEInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projLEInt64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projLEInt64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt64Float64ConstOp) Init() {
	p.input.Init()
}

type projLEInt64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projLEInt64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEInt64DecimalConstOp) Init() {
	p.input.Init()
}

type projLEFloat64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projLEFloat64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEFloat64Int16ConstOp) Init() {
	p.input.Init()
}

type projLEFloat64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projLEFloat64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEFloat64Int32ConstOp) Init() {
	p.input.Init()
}

type projLEFloat64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projLEFloat64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEFloat64Int64ConstOp) Init() {
	p.input.Init()
}

type projLEFloat64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projLEFloat64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEFloat64Float64ConstOp) Init() {
	p.input.Init()
}

type projLEFloat64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projLEFloat64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEFloat64DecimalConstOp) Init() {
	p.input.Init()
}

type projLETimestampTimestampConstOp struct {
	projConstOpBase
	constArg time.Time
}

func (p projLETimestampTimestampConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Times
	col = vec.Timestamp()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if arg.Before(p.constArg) {
							cmpResult = -1
						} else if p.constArg.Before(arg) {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if arg.Before(p.constArg) {
							cmpResult = -1
						} else if p.constArg.Before(arg) {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if arg.Before(p.constArg) {
						cmpResult = -1
					} else if p.constArg.Before(arg) {
						cmpResult = 1
					} else {
						cmpResult = 0
					}
					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if arg.Before(p.constArg) {
						cmpResult = -1
					} else if p.constArg.Before(arg) {
						cmpResult = 1
					} else {
						cmpResult = 0
					}
					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLETimestampTimestampConstOp) Init() {
	p.input.Init()
}

type projLEIntervalIntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projLEIntervalIntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = arg.Compare(p.constArg)
						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = arg.Compare(p.constArg)
						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = arg.Compare(p.constArg)
					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = arg.Compare(p.constArg)
					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEIntervalIntervalConstOp) Init() {
	p.input.Init()
}

type projLEDatumDatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projLEDatumDatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					{
						var cmpResult int

						cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

						projCol[i] = cmpResult <= 0
					}

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					{
						var cmpResult int

						cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

						projCol[i] = cmpResult <= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				{
					var cmpResult int

					cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

					projCol[i] = cmpResult <= 0
				}

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				{
					var cmpResult int

					cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

					projCol[i] = cmpResult <= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projLEDatumDatumConstOp) Init() {
	p.input.Init()
}

type projGTBoolBoolConstOp struct {
	projConstOpBase
	constArg bool
}

func (p projGTBoolBoolConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Bools
	col = vec.Bool()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if !arg && p.constArg {
							cmpResult = -1
						} else if arg && !p.constArg {
							cmpResult = 1
						} else {
							cmpResult = 0
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if !arg && p.constArg {
							cmpResult = -1
						} else if arg && !p.constArg {
							cmpResult = 1
						} else {
							cmpResult = 0
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if !arg && p.constArg {
						cmpResult = -1
					} else if arg && !p.constArg {
						cmpResult = 1
					} else {
						cmpResult = 0
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if !arg && p.constArg {
						cmpResult = -1
					} else if arg && !p.constArg {
						cmpResult = 1
					} else {
						cmpResult = 0
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTBoolBoolConstOp) Init() {
	p.input.Init()
}

type projGTBytesBytesConstOp struct {
	projConstOpBase
	constArg []byte
}

func (p projGTBytesBytesConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col *coldata.Bytes
	col = vec.Bytes()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = bytes.Compare(arg, p.constArg)
						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = bytes.Compare(arg, p.constArg)
						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = bytes.Compare(arg, p.constArg)
					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = bytes.Compare(arg, p.constArg)
					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTBytesBytesConstOp) Init() {
	p.input.Init()
}

type projGTDecimalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projGTDecimalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTDecimalInt16ConstOp) Init() {
	p.input.Init()
}

type projGTDecimalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projGTDecimalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTDecimalInt32ConstOp) Init() {
	p.input.Init()
}

type projGTDecimalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projGTDecimalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTDecimalInt64ConstOp) Init() {
	p.input.Init()
}

type projGTDecimalFloat64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projGTDecimalFloat64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTDecimalFloat64ConstOp) Init() {
	p.input.Init()
}

type projGTDecimalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projGTDecimalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = tree.CompareDecimals(&arg, &p.constArg)
						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = tree.CompareDecimals(&arg, &p.constArg)
						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = tree.CompareDecimals(&arg, &p.constArg)
					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = tree.CompareDecimals(&arg, &p.constArg)
					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTDecimalDecimalConstOp) Init() {
	p.input.Init()
}

type projGTInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projGTInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projGTInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projGTInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projGTInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projGTInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projGTInt16Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projGTInt16Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt16Float64ConstOp) Init() {
	p.input.Init()
}

type projGTInt16DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projGTInt16DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt16DecimalConstOp) Init() {
	p.input.Init()
}

type projGTInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projGTInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projGTInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projGTInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projGTInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projGTInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projGTInt32Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projGTInt32Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt32Float64ConstOp) Init() {
	p.input.Init()
}

type projGTInt32DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projGTInt32DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt32DecimalConstOp) Init() {
	p.input.Init()
}

type projGTInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projGTInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projGTInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projGTInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projGTInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projGTInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projGTInt64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projGTInt64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt64Float64ConstOp) Init() {
	p.input.Init()
}

type projGTInt64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projGTInt64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTInt64DecimalConstOp) Init() {
	p.input.Init()
}

type projGTFloat64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projGTFloat64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTFloat64Int16ConstOp) Init() {
	p.input.Init()
}

type projGTFloat64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projGTFloat64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTFloat64Int32ConstOp) Init() {
	p.input.Init()
}

type projGTFloat64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projGTFloat64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTFloat64Int64ConstOp) Init() {
	p.input.Init()
}

type projGTFloat64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projGTFloat64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTFloat64Float64ConstOp) Init() {
	p.input.Init()
}

type projGTFloat64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projGTFloat64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTFloat64DecimalConstOp) Init() {
	p.input.Init()
}

type projGTTimestampTimestampConstOp struct {
	projConstOpBase
	constArg time.Time
}

func (p projGTTimestampTimestampConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Times
	col = vec.Timestamp()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if arg.Before(p.constArg) {
							cmpResult = -1
						} else if p.constArg.Before(arg) {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if arg.Before(p.constArg) {
							cmpResult = -1
						} else if p.constArg.Before(arg) {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if arg.Before(p.constArg) {
						cmpResult = -1
					} else if p.constArg.Before(arg) {
						cmpResult = 1
					} else {
						cmpResult = 0
					}
					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if arg.Before(p.constArg) {
						cmpResult = -1
					} else if p.constArg.Before(arg) {
						cmpResult = 1
					} else {
						cmpResult = 0
					}
					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTTimestampTimestampConstOp) Init() {
	p.input.Init()
}

type projGTIntervalIntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projGTIntervalIntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = arg.Compare(p.constArg)
						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = arg.Compare(p.constArg)
						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = arg.Compare(p.constArg)
					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = arg.Compare(p.constArg)
					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTIntervalIntervalConstOp) Init() {
	p.input.Init()
}

type projGTDatumDatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projGTDatumDatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					{
						var cmpResult int

						cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

						projCol[i] = cmpResult > 0
					}

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					{
						var cmpResult int

						cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

						projCol[i] = cmpResult > 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				{
					var cmpResult int

					cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

					projCol[i] = cmpResult > 0
				}

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				{
					var cmpResult int

					cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

					projCol[i] = cmpResult > 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGTDatumDatumConstOp) Init() {
	p.input.Init()
}

type projGEBoolBoolConstOp struct {
	projConstOpBase
	constArg bool
}

func (p projGEBoolBoolConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Bools
	col = vec.Bool()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if !arg && p.constArg {
							cmpResult = -1
						} else if arg && !p.constArg {
							cmpResult = 1
						} else {
							cmpResult = 0
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if !arg && p.constArg {
							cmpResult = -1
						} else if arg && !p.constArg {
							cmpResult = 1
						} else {
							cmpResult = 0
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if !arg && p.constArg {
						cmpResult = -1
					} else if arg && !p.constArg {
						cmpResult = 1
					} else {
						cmpResult = 0
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if !arg && p.constArg {
						cmpResult = -1
					} else if arg && !p.constArg {
						cmpResult = 1
					} else {
						cmpResult = 0
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEBoolBoolConstOp) Init() {
	p.input.Init()
}

type projGEBytesBytesConstOp struct {
	projConstOpBase
	constArg []byte
}

func (p projGEBytesBytesConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col *coldata.Bytes
	col = vec.Bytes()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = bytes.Compare(arg, p.constArg)
						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = bytes.Compare(arg, p.constArg)
						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = bytes.Compare(arg, p.constArg)
					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col
			_ = 0
			_ = n
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = bytes.Compare(arg, p.constArg)
					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEBytesBytesConstOp) Init() {
	p.input.Init()
}

type projGEDecimalInt16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projGEDecimalInt16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEDecimalInt16ConstOp) Init() {
	p.input.Init()
}

type projGEDecimalInt32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projGEDecimalInt32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEDecimalInt32ConstOp) Init() {
	p.input.Init()
}

type projGEDecimalInt64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projGEDecimalInt64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(p.constArg))
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(p.constArg))
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEDecimalInt64ConstOp) Init() {
	p.input.Init()
}

type projGEDecimalFloat64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projGEDecimalFloat64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(&arg, tmpDec)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(p.constArg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(&arg, tmpDec)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEDecimalFloat64ConstOp) Init() {
	p.input.Init()
}

type projGEDecimalDecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projGEDecimalDecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Decimals
	col = vec.Decimal()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = tree.CompareDecimals(&arg, &p.constArg)
						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = tree.CompareDecimals(&arg, &p.constArg)
						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = tree.CompareDecimals(&arg, &p.constArg)
					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = tree.CompareDecimals(&arg, &p.constArg)
					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEDecimalDecimalConstOp) Init() {
	p.input.Init()
}

type projGEInt16Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projGEInt16Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt16Int16ConstOp) Init() {
	p.input.Init()
}

type projGEInt16Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projGEInt16Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt16Int32ConstOp) Init() {
	p.input.Init()
}

type projGEInt16Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projGEInt16Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt16Int64ConstOp) Init() {
	p.input.Init()
}

type projGEInt16Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projGEInt16Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt16Float64ConstOp) Init() {
	p.input.Init()
}

type projGEInt16DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projGEInt16DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int16s
	col = vec.Int16()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt16DecimalConstOp) Init() {
	p.input.Init()
}

type projGEInt32Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projGEInt32Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt32Int16ConstOp) Init() {
	p.input.Init()
}

type projGEInt32Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projGEInt32Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt32Int32ConstOp) Init() {
	p.input.Init()
}

type projGEInt32Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projGEInt32Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt32Int64ConstOp) Init() {
	p.input.Init()
}

type projGEInt32Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projGEInt32Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt32Float64ConstOp) Init() {
	p.input.Init()
}

type projGEInt32DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projGEInt32DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int32s
	col = vec.Int32()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt32DecimalConstOp) Init() {
	p.input.Init()
}

type projGEInt64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projGEInt64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt64Int16ConstOp) Init() {
	p.input.Init()
}

type projGEInt64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projGEInt64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt64Int32ConstOp) Init() {
	p.input.Init()
}

type projGEInt64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projGEInt64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := int64(arg), int64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else {
								cmpResult = 0
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := int64(arg), int64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt64Int64ConstOp) Init() {
	p.input.Init()
}

type projGEInt64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projGEInt64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if false {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if false {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt64Float64ConstOp) Init() {
	p.input.Init()
}

type projGEInt64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projGEInt64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Int64s
	col = vec.Int64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							tmpDec.SetInt64(int64(arg))
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						tmpDec.SetInt64(int64(arg))
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEInt64DecimalConstOp) Init() {
	p.input.Init()
}

type projGEFloat64Int16ConstOp struct {
	projConstOpBase
	constArg int16
}

func (p projGEFloat64Int16ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEFloat64Int16ConstOp) Init() {
	p.input.Init()
}

type projGEFloat64Int32ConstOp struct {
	projConstOpBase
	constArg int32
}

func (p projGEFloat64Int32ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEFloat64Int32ConstOp) Init() {
	p.input.Init()
}

type projGEFloat64Int64ConstOp struct {
	projConstOpBase
	constArg int64
}

func (p projGEFloat64Int64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if false {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if false {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEFloat64Int64ConstOp) Init() {
	p.input.Init()
}

type projGEFloat64Float64ConstOp struct {
	projConstOpBase
	constArg float64
}

func (p projGEFloat64Float64ConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							a, b := float64(arg), float64(p.constArg)
							if a < b {
								cmpResult = -1
							} else if a > b {
								cmpResult = 1
							} else if a == b {
								cmpResult = 0
							} else if math.IsNaN(a) {
								if math.IsNaN(b) {
									cmpResult = 0
								} else {
									cmpResult = -1
								}
							} else {
								cmpResult = 1
							}
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						a, b := float64(arg), float64(p.constArg)
						if a < b {
							cmpResult = -1
						} else if a > b {
							cmpResult = 1
						} else if a == b {
							cmpResult = 0
						} else if math.IsNaN(a) {
							if math.IsNaN(b) {
								cmpResult = 0
							} else {
								cmpResult = -1
							}
						} else {
							cmpResult = 1
						}
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEFloat64Float64ConstOp) Init() {
	p.input.Init()
}

type projGEFloat64DecimalConstOp struct {
	projConstOpBase
	constArg apd.Decimal
}

func (p projGEFloat64DecimalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Float64s
	col = vec.Float64()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						{
							tmpDec := &_overloadHelper.tmpDec1
							if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
								colexecerror.ExpectedError(err)
							}
							cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
						}

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					{
						tmpDec := &_overloadHelper.tmpDec1
						if _, err := tmpDec.SetFloat64(float64(arg)); err != nil {
							colexecerror.ExpectedError(err)
						}
						cmpResult = tree.CompareDecimals(tmpDec, &p.constArg)
					}

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEFloat64DecimalConstOp) Init() {
	p.input.Init()
}

type projGETimestampTimestampConstOp struct {
	projConstOpBase
	constArg time.Time
}

func (p projGETimestampTimestampConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Times
	col = vec.Timestamp()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if arg.Before(p.constArg) {
							cmpResult = -1
						} else if p.constArg.Before(arg) {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int

						if arg.Before(p.constArg) {
							cmpResult = -1
						} else if p.constArg.Before(arg) {
							cmpResult = 1
						} else {
							cmpResult = 0
						}
						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if arg.Before(p.constArg) {
						cmpResult = -1
					} else if p.constArg.Before(arg) {
						cmpResult = 1
					} else {
						cmpResult = 0
					}
					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int

					if arg.Before(p.constArg) {
						cmpResult = -1
					} else if p.constArg.Before(arg) {
						cmpResult = 1
					} else {
						cmpResult = 0
					}
					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGETimestampTimestampConstOp) Init() {
	p.input.Init()
}

type projGEIntervalIntervalConstOp struct {
	projConstOpBase
	constArg duration.Duration
}

func (p projGEIntervalIntervalConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.Durations
	col = vec.Interval()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = arg.Compare(p.constArg)
						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i) //gcassert:inline

					{
						var cmpResult int
						cmpResult = arg.Compare(p.constArg)
						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = arg.Compare(p.constArg)
					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col[0:n]
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i) //gcassert:inline

				{
					var cmpResult int
					cmpResult = arg.Compare(p.constArg)
					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEIntervalIntervalConstOp) Init() {
	p.input.Init()
}

type projGEDatumDatumConstOp struct {
	projConstOpBase
	constArg interface{}
}

func (p projGEDatumDatumConstOp) Next(ctx context.Context) coldata.Batch {
	// In order to inline the templated code of overloads, we need to have a
	// `_overloadHelper` local variable of type `overloadHelper`.
	_overloadHelper := p.overloadHelper
	// However, the scratch is not used in all of the projection operators, so
	// we add this to go around "unused" error.
	_ = _overloadHelper
	batch := p.input.Next(ctx)
	n := batch.Length()
	if n == 0 {
		return coldata.ZeroBatch
	}
	vec := batch.ColVec(p.colIdx)
	var col coldata.DatumVec
	col = vec.Datum()
	projVec := batch.ColVec(p.outputIdx)
	if projVec.MaybeHasNulls() {
		// We need to make sure that there are no left over null values in the
		// output vector.
		projVec.Nulls().UnsetNulls()
	}
	projCol := projVec.Bool()
	if vec.Nulls().MaybeHasNulls() {
		colNulls := vec.Nulls()
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					{
						var cmpResult int

						cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

						projCol[i] = cmpResult >= 0
					}

				}
			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				if !colNulls.NullAt(i) {
					// We only want to perform the projection operation if the value is not null.
					arg := col.Get(i)

					{
						var cmpResult int

						cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

						projCol[i] = cmpResult >= 0
					}

				}
			}
		}
		colNullsCopy := colNulls.Copy()
		projVec.SetNulls(&colNullsCopy)
	} else {
		if sel := batch.Selection(); sel != nil {
			sel = sel[:n]
			for _, i := range sel {
				arg := col.Get(i)

				{
					var cmpResult int

					cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

					projCol[i] = cmpResult >= 0
				}

			}
		} else {
			col = col.Slice(0, n)
			_ = projCol.Get(n - 1) //gcassert:inline
			for i := 0; i < n; i++ {
				arg := col.Get(i)

				{
					var cmpResult int

					cmpResult = arg.(*coldataext.Datum).CompareDatum(col, p.constArg)

					projCol[i] = cmpResult >= 0
				}

			}
		}
	}
	// Although we didn't change the length of the batch, it is necessary to set
	// the length anyway (this helps maintaining the invariant of flat bytes).
	batch.SetLength(n)
	return batch
}

func (p projGEDatumDatumConstOp) Init() {
	p.input.Init()
}

// GetProjectionRConstOperator returns the appropriate constant
// projection operator for the given left and right column types and operation.
func GetProjectionRConstOperator(
	allocator *colmem.Allocator,
	leftType *types.T,
	rightType *types.T,
	outputType *types.T,
	op tree.Operator,
	input colexecbase.Operator,
	colIdx int,
	constArg tree.Datum,
	outputIdx int,
	binFn *tree.BinOp,
	evalCtx *tree.EvalContext,
) (colexecbase.Operator, error) {
	input = newVectorTypeEnforcer(allocator, input, outputType, outputIdx)
	projConstOpBase := projConstOpBase{
		OneInputNode:   NewOneInputNode(input),
		allocator:      allocator,
		colIdx:         colIdx,
		outputIdx:      outputIdx,
		overloadHelper: overloadHelper{binFn: binFn, evalCtx: evalCtx},
	}
	var (
		c   interface{}
		err error
	)
	c, err = GetDatumToPhysicalFn(rightType)(constArg)
	if err != nil {
		return nil, err
	}
	switch op.(type) {
	case tree.BinaryOperator:
		switch op {
		case tree.Bitand:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projBitandInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projBitandInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projBitandInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projBitandInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projBitandInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projBitandInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projBitandInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projBitandInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projBitandInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projBitandDatumDatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			}
		case tree.Bitor:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projBitorInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projBitorInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projBitorInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projBitorInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projBitorInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projBitorInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projBitorInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projBitorInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projBitorInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projBitorDatumDatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			}
		case tree.Bitxor:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projBitxorInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projBitxorInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projBitxorInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projBitxorInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projBitxorInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projBitxorInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projBitxorInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projBitxorInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projBitxorInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projBitxorDatumDatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			}
		case tree.Plus:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.DecimalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projPlusDecimalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projPlusDecimalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projPlusDecimalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPlusDecimalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projPlusInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projPlusInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projPlusInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPlusInt16DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPlusInt16DatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projPlusInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projPlusInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projPlusInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPlusInt32DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPlusInt32DatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projPlusInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projPlusInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projPlusInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPlusInt64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPlusInt64DatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			case types.FloatFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPlusFloat64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					}
				}
			case types.TimestampTZFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPlusTimestampIntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					}
				}
			case types.IntervalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.TimestampTZFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPlusIntervalTimestampConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(time.Time),
							}, nil
						}
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPlusIntervalIntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPlusIntervalDatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPlusDatumIntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projPlusDatumInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projPlusDatumInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projPlusDatumInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				}
			}
		case tree.Minus:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.DecimalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projMinusDecimalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projMinusDecimalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projMinusDecimalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusDecimalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projMinusInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projMinusInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projMinusInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusInt16DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusInt16DatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projMinusInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projMinusInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projMinusInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusInt32DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusInt32DatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projMinusInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projMinusInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projMinusInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusInt64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusInt64DatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			case types.FloatFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusFloat64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					}
				}
			case types.TimestampTZFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.TimestampTZFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusTimestampTimestampConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(time.Time),
							}, nil
						}
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusTimestampIntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					}
				}
			case types.IntervalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusIntervalIntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusIntervalDatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusDatumDatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusDatumIntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					case types.BytesFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMinusDatumBytesConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.([]byte),
							}, nil
						}
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projMinusDatumInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projMinusDatumInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projMinusDatumInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				}
			}
		case tree.Mult:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.DecimalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projMultDecimalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projMultDecimalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projMultDecimalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMultDecimalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMultDecimalIntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					}
				}
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projMultInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projMultInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projMultInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMultInt16DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMultInt16IntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projMultInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projMultInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projMultInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMultInt32DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMultInt32IntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projMultInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projMultInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projMultInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMultInt64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMultInt64IntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					}
				}
			case types.FloatFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMultFloat64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMultFloat64IntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					}
				}
			case types.IntervalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projMultIntervalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projMultIntervalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projMultIntervalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMultIntervalFloat64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projMultIntervalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			}
		case tree.Div:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.DecimalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projDivDecimalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projDivDecimalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projDivDecimalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projDivDecimalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projDivInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projDivInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projDivInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projDivInt16DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projDivInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projDivInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projDivInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projDivInt32DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projDivInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projDivInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projDivInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projDivInt64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.FloatFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projDivFloat64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					}
				}
			case types.IntervalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projDivIntervalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projDivIntervalFloat64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					}
				}
			}
		case tree.FloorDiv:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.DecimalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projFloorDivDecimalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projFloorDivDecimalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projFloorDivDecimalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projFloorDivDecimalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projFloorDivInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projFloorDivInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projFloorDivInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projFloorDivInt16DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projFloorDivInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projFloorDivInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projFloorDivInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projFloorDivInt32DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projFloorDivInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projFloorDivInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projFloorDivInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projFloorDivInt64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.FloatFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projFloorDivFloat64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					}
				}
			}
		case tree.Mod:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.DecimalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projModDecimalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projModDecimalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projModDecimalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projModDecimalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projModInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projModInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projModInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projModInt16DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projModInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projModInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projModInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projModInt32DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projModInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projModInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projModInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projModInt64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.FloatFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projModFloat64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					}
				}
			}
		case tree.Pow:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.DecimalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projPowDecimalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projPowDecimalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projPowDecimalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPowDecimalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projPowInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projPowInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projPowInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPowInt16DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projPowInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projPowInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projPowInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPowInt32DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projPowInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projPowInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projPowInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPowInt64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.FloatFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projPowFloat64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					}
				}
			}
		case tree.Concat:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.BytesFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BytesFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projConcatBytesBytesConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.([]byte),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projConcatDatumDatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			}
		case tree.LShift:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLShiftInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLShiftInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLShiftInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLShiftInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLShiftInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLShiftInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLShiftInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLShiftInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLShiftInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLShiftDatumInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLShiftDatumInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLShiftDatumInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				}
			}
		case tree.RShift:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projRShiftInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projRShiftInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projRShiftInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projRShiftInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projRShiftInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projRShiftInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projRShiftInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projRShiftInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projRShiftInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projRShiftDatumInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projRShiftDatumInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projRShiftDatumInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				}
			}
		case tree.JSONFetchVal:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BytesFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projJSONFetchValDatumBytesConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.([]byte),
							}, nil
						}
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projJSONFetchValDatumInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projJSONFetchValDatumInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projJSONFetchValDatumInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					}
				}
			}
		}
	case tree.ComparisonOperator:
		switch op {
		case tree.EQ:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.BoolFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BoolFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQBoolBoolConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(bool),
							}, nil
						}
					}
				}
			case types.BytesFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BytesFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQBytesBytesConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.([]byte),
							}, nil
						}
					}
				}
			case types.DecimalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projEQDecimalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projEQDecimalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projEQDecimalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQDecimalFloat64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQDecimalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projEQInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projEQInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projEQInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQInt16Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQInt16DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projEQInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projEQInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projEQInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQInt32Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQInt32DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projEQInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projEQInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projEQInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQInt64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQInt64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.FloatFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projEQFloat64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projEQFloat64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projEQFloat64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQFloat64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQFloat64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.TimestampTZFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.TimestampTZFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQTimestampTimestampConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(time.Time),
							}, nil
						}
					}
				}
			case types.IntervalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQIntervalIntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projEQDatumDatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			}
		case tree.NE:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.BoolFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BoolFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEBoolBoolConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(bool),
							}, nil
						}
					}
				}
			case types.BytesFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BytesFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEBytesBytesConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.([]byte),
							}, nil
						}
					}
				}
			case types.DecimalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projNEDecimalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projNEDecimalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projNEDecimalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEDecimalFloat64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEDecimalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projNEInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projNEInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projNEInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEInt16Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEInt16DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projNEInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projNEInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projNEInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEInt32Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEInt32DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projNEInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projNEInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projNEInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEInt64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEInt64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.FloatFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projNEFloat64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projNEFloat64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projNEFloat64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEFloat64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEFloat64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.TimestampTZFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.TimestampTZFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNETimestampTimestampConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(time.Time),
							}, nil
						}
					}
				}
			case types.IntervalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEIntervalIntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projNEDatumDatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			}
		case tree.LT:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.BoolFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BoolFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTBoolBoolConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(bool),
							}, nil
						}
					}
				}
			case types.BytesFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BytesFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTBytesBytesConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.([]byte),
							}, nil
						}
					}
				}
			case types.DecimalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLTDecimalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLTDecimalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLTDecimalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTDecimalFloat64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTDecimalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLTInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLTInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLTInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTInt16Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTInt16DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLTInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLTInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLTInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTInt32Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTInt32DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLTInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLTInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLTInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTInt64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTInt64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.FloatFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLTFloat64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLTFloat64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLTFloat64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTFloat64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTFloat64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.TimestampTZFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.TimestampTZFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTTimestampTimestampConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(time.Time),
							}, nil
						}
					}
				}
			case types.IntervalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTIntervalIntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLTDatumDatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			}
		case tree.LE:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.BoolFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BoolFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEBoolBoolConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(bool),
							}, nil
						}
					}
				}
			case types.BytesFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BytesFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEBytesBytesConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.([]byte),
							}, nil
						}
					}
				}
			case types.DecimalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLEDecimalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLEDecimalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLEDecimalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEDecimalFloat64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEDecimalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLEInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLEInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLEInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEInt16Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEInt16DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLEInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLEInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLEInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEInt32Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEInt32DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLEInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLEInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLEInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEInt64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEInt64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.FloatFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projLEFloat64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projLEFloat64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projLEFloat64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEFloat64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEFloat64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.TimestampTZFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.TimestampTZFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLETimestampTimestampConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(time.Time),
							}, nil
						}
					}
				}
			case types.IntervalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEIntervalIntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projLEDatumDatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			}
		case tree.GT:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.BoolFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BoolFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTBoolBoolConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(bool),
							}, nil
						}
					}
				}
			case types.BytesFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BytesFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTBytesBytesConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.([]byte),
							}, nil
						}
					}
				}
			case types.DecimalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projGTDecimalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projGTDecimalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projGTDecimalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTDecimalFloat64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTDecimalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projGTInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projGTInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projGTInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTInt16Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTInt16DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projGTInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projGTInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projGTInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTInt32Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTInt32DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projGTInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projGTInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projGTInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTInt64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTInt64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.FloatFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projGTFloat64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projGTFloat64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projGTFloat64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTFloat64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTFloat64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.TimestampTZFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.TimestampTZFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTTimestampTimestampConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(time.Time),
							}, nil
						}
					}
				}
			case types.IntervalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTIntervalIntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGTDatumDatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			}
		case tree.GE:
			switch typeconv.TypeFamilyToCanonicalTypeFamily(leftType.Family()) {
			case types.BoolFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BoolFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEBoolBoolConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(bool),
							}, nil
						}
					}
				}
			case types.BytesFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.BytesFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEBytesBytesConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.([]byte),
							}, nil
						}
					}
				}
			case types.DecimalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projGEDecimalInt16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projGEDecimalInt32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projGEDecimalInt64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEDecimalFloat64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEDecimalDecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.IntFamily:
				switch leftType.Width() {
				case 16:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projGEInt16Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projGEInt16Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projGEInt16Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEInt16Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEInt16DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case 32:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projGEInt32Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projGEInt32Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projGEInt32Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEInt32Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEInt32DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projGEInt64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projGEInt64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projGEInt64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEInt64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEInt64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.FloatFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntFamily:
						switch rightType.Width() {
						case 16:
							return &projGEFloat64Int16ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int16),
							}, nil
						case 32:
							return &projGEFloat64Int32ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int32),
							}, nil
						case -1:
						default:
							return &projGEFloat64Int64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(int64),
							}, nil
						}
					case types.FloatFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEFloat64Float64ConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(float64),
							}, nil
						}
					case types.DecimalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEFloat64DecimalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(apd.Decimal),
							}, nil
						}
					}
				}
			case types.TimestampTZFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.TimestampTZFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGETimestampTimestampConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(time.Time),
							}, nil
						}
					}
				}
			case types.IntervalFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case types.IntervalFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEIntervalIntervalConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(duration.Duration),
							}, nil
						}
					}
				}
			case typeconv.DatumVecCanonicalTypeFamily:
				switch leftType.Width() {
				case -1:
				default:
					switch typeconv.TypeFamilyToCanonicalTypeFamily(rightType.Family()) {
					case typeconv.DatumVecCanonicalTypeFamily:
						switch rightType.Width() {
						case -1:
						default:
							return &projGEDatumDatumConstOp{
								projConstOpBase: projConstOpBase,
								constArg:        c.(interface{}),
							}, nil
						}
					}
				}
			}
		}
	}
	return nil, errors.Errorf("couldn't find overload for %s %s %s", leftType.Name(), op, rightType.Name())
}
