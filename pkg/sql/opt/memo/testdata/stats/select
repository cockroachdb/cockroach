exec-ddl
CREATE TABLE a (x INT PRIMARY KEY, y INT)
----

exec-ddl
CREATE TABLE b (x INT, z INT NOT NULL)
----

exec-ddl
ALTER TABLE a INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 5000,
    "distinct_count": 5000
  },
  {
    "columns": ["y"],
    "created_at": "2018-01-01 1:30:00.00000+00:00",
    "row_count": 4000,
    "distinct_count": 400
  }
]'
----

exec-ddl
ALTER TABLE b INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 5000
  },
  {
    "columns": ["z"],
    "created_at": "2018-01-01 1:30:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 100
  },
  {
    "columns": ["rowid"],
    "created_at": "2018-01-01 1:30:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 10000
  }
]'
----

norm
SELECT * FROM a WHERE true
----
scan a
 ├── columns: x:1(int!null) y:2(int)
 ├── stats: [rows=4000]
 ├── key: (1)
 └── fd: (1)-->(2)

norm
SELECT * FROM a WHERE false
----
values
 ├── columns: x:1(int!null) y:2(int!null)
 ├── cardinality: [0 - 0]
 ├── stats: [rows=0]
 ├── key: ()
 └── fd: ()-->(1,2)

# Distinct values calculation with constraints.
norm
SELECT * FROM b WHERE x = 1 AND z = 2 AND rowid >= 5 AND rowid <= 8
----
project
 ├── columns: x:1(int!null) z:2(int!null)
 ├── cardinality: [0 - 4]
 ├── stats: [rows=0.8118008]
 ├── fd: ()-->(1,2)
 └── select
      ├── columns: x:1(int!null) z:2(int!null) rowid:3(int!null)
      ├── cardinality: [0 - 4]
      ├── stats: [rows=0.8118008, distinct(1)=0.8118008, null(1)=0, distinct(2)=0.8118008, null(2)=0, distinct(3)=0.8118008, null(3)=0, distinct(1,2)=0.8118008, null(1,2)=0, distinct(1-3)=0.8118008, null(1-3)=0]
      ├── key: (3)
      ├── fd: ()-->(1,2)
      ├── scan b
      │    ├── columns: x:1(int) z:2(int!null) rowid:3(int!null)
      │    ├── stats: [rows=10000, distinct(1)=5000, null(1)=0, distinct(2)=100, null(2)=0, distinct(3)=10000, null(3)=0, distinct(1,2)=10000, null(1,2)=0, distinct(1-3)=10000, null(1-3)=0]
      │    ├── key: (3)
      │    └── fd: (3)-->(1,2)
      └── filters
           ├── (rowid:3 >= 5) AND (rowid:3 <= 8) [type=bool, outer=(3), constraints=(/3: [/5 - /8]; tight)]
           ├── x:1 = 1 [type=bool, outer=(1), constraints=(/1: [/1 - /1]; tight), fd=()-->(1)]
           └── z:2 = 2 [type=bool, outer=(2), constraints=(/2: [/2 - /2]; tight), fd=()-->(2)]

# Can't determine stats from filter.
norm
SELECT * FROM a WHERE x + y < 10
----
select
 ├── columns: x:1(int!null) y:2(int)
 ├── immutable
 ├── stats: [rows=1333.33333]
 ├── key: (1)
 ├── fd: (1)-->(2)
 ├── scan a
 │    ├── columns: x:1(int!null) y:2(int)
 │    ├── stats: [rows=4000]
 │    ├── key: (1)
 │    └── fd: (1)-->(2)
 └── filters
      └── (x:1 + y:2) < 10 [type=bool, outer=(1,2), immutable]

# Remaining filter.
norm
SELECT * FROM a WHERE y = 5 AND x + y < 10
----
select
 ├── columns: x:1(int!null) y:2(int!null)
 ├── stats: [rows=9.33333333, distinct(1)=9.33333333, null(1)=0, distinct(2)=1, null(2)=0, distinct(1,2)=9.33333333, null(1,2)=0]
 ├── key: (1)
 ├── fd: ()-->(2)
 ├── scan a
 │    ├── columns: x:1(int!null) y:2(int)
 │    ├── stats: [rows=4000, distinct(1)=4000, null(1)=0, distinct(2)=400, null(2)=0, distinct(1,2)=4000, null(1,2)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2)
 └── filters
      ├── y:2 = 5 [type=bool, outer=(2), constraints=(/2: [/5 - /5]; tight), fd=()-->(2)]
      └── x:1 < 5 [type=bool, outer=(1), constraints=(/1: (/NULL - /4]; tight)]

# Contradiction.
norm
SELECT * FROM a WHERE x IS NULL
----
select
 ├── columns: x:1(int!null) y:2(int)
 ├── cardinality: [0 - 1]
 ├── stats: [rows=1, distinct(1)=1, null(1)=0]
 ├── key: ()
 ├── fd: ()-->(1,2)
 ├── scan a
 │    ├── columns: x:1(int!null) y:2(int)
 │    ├── stats: [rows=4000, distinct(1)=4000, null(1)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2)
 └── filters
      └── x:1 IS NULL [type=bool, outer=(1), constraints=(/1: [/NULL - /NULL]; tight), fd=()-->(1)]

norm
SELECT sum(x) FROM b WHERE x > 1000 AND x <= 2000 GROUP BY z
----
project
 ├── columns: sum:4(decimal!null)
 ├── stats: [rows=100]
 └── group-by
      ├── columns: z:2(int!null) sum:4(decimal!null)
      ├── grouping columns: z:2(int!null)
      ├── stats: [rows=100, distinct(2)=100, null(2)=0]
      ├── key: (2)
      ├── fd: (2)-->(4)
      ├── select
      │    ├── columns: x:1(int!null) z:2(int!null)
      │    ├── stats: [rows=2000, distinct(1)=1000, null(1)=0, distinct(2)=100, null(2)=0]
      │    ├── scan b
      │    │    ├── columns: x:1(int) z:2(int!null)
      │    │    └── stats: [rows=10000, distinct(1)=5000, null(1)=0, distinct(2)=100, null(2)=0]
      │    └── filters
      │         └── (x:1 > 1000) AND (x:1 <= 2000) [type=bool, outer=(1), constraints=(/1: [/1001 - /2000]; tight)]
      └── aggregations
           └── sum [as=sum:4, type=decimal, outer=(1)]
                └── x:1 [type=int]

# Regression: statistics builder panics when end key is NULL when it's trying
# to compute start/end int boundaries.
exec-ddl
CREATE TABLE idx (x INT PRIMARY KEY, y INT, z INT, INDEX yz (y DESC, z))
----

opt
SELECT y FROM idx WHERE y < 5 AND z < 10
----
project
 ├── columns: y:2(int!null)
 ├── stats: [rows=311.111111]
 └── select
      ├── columns: y:2(int!null) z:3(int!null)
      ├── stats: [rows=311.111111, distinct(2)=33.3333333, null(2)=0, distinct(3)=33.3333333, null(3)=0, distinct(2,3)=311.111111, null(2,3)=0]
      ├── scan idx@yz
      │    ├── columns: y:2(int!null) z:3(int)
      │    ├── constraint: /-2/3/1: (/4/NULL - /NULL)
      │    └── stats: [rows=333.333333, distinct(2)=33.3333333, null(2)=0]
      └── filters
           └── z:3 < 10 [type=bool, outer=(3), constraints=(/3: (/NULL - /9]; tight)]

# Regression: certain queries could cause a NaN expected number of rows via a divide-by-zero.
exec-ddl
CREATE TABLE tab0(pk INTEGER PRIMARY KEY, col0 INTEGER, col1 FLOAT, col2 TEXT, col3 INTEGER, col4 FLOAT, col5 TEXT)
----

# Note: it's not clear that this still tests the above issue, but I have left
# it here anyway as an interesting test case. I've added another query below
# to regression-test the divide-by-zero issue.
opt
SELECT pk FROM tab0 WHERE
  col0 = 1 AND
  col0 = 2 AND
  (col0 = 1 OR col0 IN (SELECT col3 FROM tab0)) AND
  (col0 = 1 OR col0 IN (SELECT col3 FROM tab0))
----
values
 ├── columns: pk:1(int!null)
 ├── cardinality: [0 - 0]
 ├── stats: [rows=0]
 ├── key: ()
 └── fd: ()-->(1)

exec-ddl
ALTER TABLE tab0 INJECT STATISTICS '[
{
  "columns": ["col0"],
  "created_at": "2018-01-01 1:00:00.00000+00:00",
  "row_count": 100,
  "distinct_count": 0,
  "null_count": 100
},
{
  "columns": ["col3"],
  "created_at": "2018-01-01 1:00:00.00000+00:00",
  "row_count": 100,
  "distinct_count": 10
}
]'
----

opt
SELECT count(*) FROM (SELECT * FROM tab0 WHERE col3 = 10) GROUP BY col0
----
project
 ├── columns: count:8(int!null)
 ├── stats: [rows=0.999973439]
 └── group-by
      ├── columns: col0:2(int) count_rows:8(int!null)
      ├── grouping columns: col0:2(int)
      ├── stats: [rows=0.999973439, distinct(2)=0.999973439, null(2)=0.999973439]
      ├── key: (2)
      ├── fd: (2)-->(8)
      ├── select
      │    ├── columns: col0:2(int) col3:5(int!null)
      │    ├── stats: [rows=10, distinct(2)=0.999973439, null(2)=10, distinct(5)=1, null(5)=0]
      │    ├── fd: ()-->(5)
      │    ├── scan tab0
      │    │    ├── columns: col0:2(int) col3:5(int)
      │    │    └── stats: [rows=100, distinct(2)=1, null(2)=100, distinct(5)=10, null(5)=0]
      │    └── filters
      │         └── col3:5 = 10 [type=bool, outer=(5), constraints=(/5: [/10 - /10]; tight), fd=()-->(5)]
      └── aggregations
           └── count-rows [as=count_rows:8, type=int]


exec-ddl
CREATE TABLE customers (id INT PRIMARY KEY, name STRING, state STRING)
----

exec-ddl
CREATE TABLE order_history (order_id INT, item_id INT, customer_id INT, year INT)
----

exec-ddl
CREATE TABLE district (d_id INT, d_w_id INT, d_name STRING, PRIMARY KEY(d_id, d_w_id))
----

exec-ddl
ALTER TABLE district INJECT STATISTICS '[
{
  "columns": ["d_id"],
  "created_at": "2018-01-01 1:00:00.00000+00:00",
  "row_count": 100,
  "distinct_count": 10
},
{
  "columns": ["d_w_id"],
  "created_at": "2018-01-01 1:30:00.00000+00:00",
  "row_count": 100,
  "distinct_count": 10
},
{
  "columns": ["d_name"],
  "created_at": "2018-01-01 1:30:00.00000+00:00",
  "row_count": 100,
  "distinct_count": 100
}
]'
----

# This tests selectivityFromReducedCols.
# Since the reduced column set is (d_id, d_name), and
# both columns have distinct count 1, we expect this
# to calculate selectivity through selectivityFromReducedCols.
# The output is the same as the naive approach.
norm
SELECT * FROM district WHERE d_id = 1 AND d_name='bobs_burgers'
----
select
 ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string!null)
 ├── stats: [rows=0.91, distinct(1)=0.91, null(1)=0, distinct(3)=0.91, null(3)=0, distinct(1,3)=0.91, null(1,3)=0]
 ├── key: (2)
 ├── fd: ()-->(1,3)
 ├── scan district
 │    ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string)
 │    ├── stats: [rows=100, distinct(1)=10, null(1)=0, distinct(2)=10, null(2)=0, distinct(3)=100, null(3)=0, distinct(1,3)=100, null(1,3)=0]
 │    ├── key: (1,2)
 │    └── fd: (1,2)-->(3)
 └── filters
      ├── d_id:1 = 1 [type=bool, outer=(1), constraints=(/1: [/1 - /1]; tight), fd=()-->(1)]
      └── d_name:3 = 'bobs_burgers' [type=bool, outer=(3), constraints=(/3: [/'bobs_burgers' - /'bobs_burgers']; tight), fd=()-->(3)]

norm
SELECT * FROM district WHERE d_id = 1 and d_name LIKE 'bob'
----
select
 ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string!null)
 ├── stats: [rows=0.91, distinct(1)=0.91, null(1)=0, distinct(3)=0.91, null(3)=0, distinct(1,3)=0.91, null(1,3)=0]
 ├── key: (2)
 ├── fd: ()-->(1,3)
 ├── scan district
 │    ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string)
 │    ├── stats: [rows=100, distinct(1)=10, null(1)=0, distinct(2)=10, null(2)=0, distinct(3)=100, null(3)=0, distinct(1,3)=100, null(1,3)=0]
 │    ├── key: (1,2)
 │    └── fd: (1,2)-->(3)
 └── filters
      ├── d_id:1 = 1 [type=bool, outer=(1), constraints=(/1: [/1 - /1]; tight), fd=()-->(1)]
      └── d_name:3 LIKE 'bob' [type=bool, outer=(3), constraints=(/3: [/'bob' - /'bob']; tight), fd=()-->(3)]

# This tests selectivityFromReducedCols.
# Since (1,2)-->(3) in order to use selectivityFromReducedCols,
# both (1,2) must have distinct=1 after applying the filter. Since
# d_id is a range constraint, this fails, and we fall back to the
# naive estimation for selectivity.
norm
SELECT * FROM district WHERE d_id > 1 AND d_id < 10 AND d_w_id=10 AND d_name='bobs_burgers'
----
select
 ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string!null)
 ├── cardinality: [0 - 8]
 ├── stats: [rows=0.827, distinct(1)=0.827, null(1)=0, distinct(2)=0.827, null(2)=0, distinct(3)=0.827, null(3)=0, distinct(2,3)=0.827, null(2,3)=0, distinct(1-3)=0.827, null(1-3)=0]
 ├── key: (1)
 ├── fd: ()-->(2,3)
 ├── scan district
 │    ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string)
 │    ├── stats: [rows=100, distinct(1)=10, null(1)=0, distinct(2)=10, null(2)=0, distinct(3)=100, null(3)=0, distinct(2,3)=100, null(2,3)=0, distinct(1-3)=100, null(1-3)=0]
 │    ├── key: (1,2)
 │    └── fd: (1,2)-->(3)
 └── filters
      ├── (d_id:1 > 1) AND (d_id:1 < 10) [type=bool, outer=(1), constraints=(/1: [/2 - /9]; tight)]
      ├── d_w_id:2 = 10 [type=bool, outer=(2), constraints=(/2: [/10 - /10]; tight), fd=()-->(2)]
      └── d_name:3 = 'bobs_burgers' [type=bool, outer=(3), constraints=(/3: [/'bobs_burgers' - /'bobs_burgers']; tight), fd=()-->(3)]

# This tests selectivityFromReducedCols
# We don't apply the selectivity on d_name since (1,2)-->3.
norm
SELECT * FROM district WHERE d_id = 1 AND d_w_id=10 AND d_name='hello'
----
select
 ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string!null)
 ├── cardinality: [0 - 1]
 ├── stats: [rows=1, distinct(1)=1, null(1)=0, distinct(2)=1, null(2)=0, distinct(3)=1, null(3)=0, distinct(1,2)=1, null(1,2)=0]
 ├── key: ()
 ├── fd: ()-->(1-3)
 ├── scan district
 │    ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string)
 │    ├── stats: [rows=100, distinct(1)=10, null(1)=0, distinct(2)=10, null(2)=0, distinct(3)=100, null(3)=0, distinct(1,2)=100, null(1,2)=0]
 │    ├── key: (1,2)
 │    └── fd: (1,2)-->(3)
 └── filters
      ├── d_id:1 = 1 [type=bool, outer=(1), constraints=(/1: [/1 - /1]; tight), fd=()-->(1)]
      ├── d_w_id:2 = 10 [type=bool, outer=(2), constraints=(/2: [/10 - /10]; tight), fd=()-->(2)]
      └── d_name:3 = 'hello' [type=bool, outer=(3), constraints=(/3: [/'hello' - /'hello']; tight), fd=()-->(3)]

exec-ddl
ALTER TABLE customers INJECT STATISTICS '[
{
  "columns": ["name"],
  "created_at": "2018-01-01 1:00:00.00000+00:00",
  "row_count": 10000,
  "distinct_count": 500
},
{
  "columns": ["id"],
  "created_at": "2018-01-01 1:30:00.00000+00:00",
  "row_count": 10000,
  "distinct_count": 10000
}
]'
----

# This tests selectivityFromReducedCols
# The following two tests cases are paired together. The first has
# one constraint, one on single non-key column. The second  query has two
# constraints on columns which form a determinant, dependent FD pair.
# The dependent column in this FD pair is from the first test case.
# This series of tests demonstrates that the selectivity
# contribution for a pair of (determinant, dependent) FDs is the
# selectivity of the determinant.
# 1/2 join-subquery-selectivityFromReducedCols tests

build
SELECT * FROM (SELECT * FROM customers, order_history WHERE id = customer_id)
WHERE name='andy'
----
select
 ├── columns: id:1(int!null) name:2(string!null) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 ├── stats: [rows=2.29713221, distinct(2)=1, null(2)=0]
 ├── fd: ()-->(2), (1)-->(3), (1)==(6), (6)==(1)
 ├── project
 │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 │    ├── stats: [rows=990, distinct(1)=99, null(1)=0, distinct(2)=430.972148, null(2)=0, distinct(6)=99, null(6)=0]
 │    ├── fd: (1)-->(2,3), (1)==(6), (6)==(1)
 │    └── select
 │         ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int) rowid:8(int!null)
 │         ├── stats: [rows=990, distinct(1)=99, null(1)=0, distinct(2)=430.972148, null(2)=0, distinct(6)=99, null(6)=0]
 │         ├── key: (8)
 │         ├── fd: (1)-->(2,3), (8)-->(4-7), (1)==(6), (6)==(1)
 │         ├── inner-join (cross)
 │         │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    ├── stats: [rows=10000000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=0, distinct(6)=100, null(6)=100000, distinct(8)=1000, null(8)=0]
 │         │    ├── key: (1,8)
 │         │    ├── fd: (1)-->(2,3), (8)-->(4-7)
 │         │    ├── scan customers
 │         │    │    ├── columns: id:1(int!null) name:2(string) state:3(string)
 │         │    │    ├── stats: [rows=10000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=0]
 │         │    │    ├── key: (1)
 │         │    │    └── fd: (1)-->(2,3)
 │         │    ├── scan order_history
 │         │    │    ├── columns: order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    │    ├── stats: [rows=1000, distinct(6)=100, null(6)=10, distinct(8)=1000, null(8)=0]
 │         │    │    ├── key: (8)
 │         │    │    └── fd: (8)-->(4-7)
 │         │    └── filters (true)
 │         └── filters
 │              └── id:1 = customer_id:6 [type=bool, outer=(1,6), constraints=(/1: (/NULL - ]; /6: (/NULL - ]), fd=(1)==(6), (6)==(1)]
 └── filters
      └── name:2 = 'andy' [type=bool, outer=(2), constraints=(/2: [/'andy' - /'andy']; tight), fd=()-->(2)]

# This tests selectivityFromReducedCols
# The previous tests case and the following are paired together. The first has
# one constraint, one on single non-key column. The second  query has two
# constraints on columns which form a determinant, dependent FD pair.
# The dependent column in this FD pair is from the first test case.
# This series of tests demonstrates that the selectivity
# contribution for a pair of (determinant, dependent) FDs is the
# selectivity of the determinant.
# 2/2 join-subquery-selectivityFromReducedCols tests

build
SELECT * FROM (SELECT * FROM customers, order_history WHERE id = customer_id)
WHERE id = 1 AND name='andy'
----
select
 ├── columns: id:1(int!null) name:2(string!null) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 ├── stats: [rows=10, distinct(1)=1, null(1)=0, distinct(2)=1, null(2)=0]
 ├── fd: ()-->(1-3,6), (1)==(6), (6)==(1)
 ├── project
 │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 │    ├── stats: [rows=990, distinct(1)=99, null(1)=0, distinct(2)=430.972148, null(2)=0, distinct(6)=99, null(6)=0]
 │    ├── fd: (1)-->(2,3), (1)==(6), (6)==(1)
 │    └── select
 │         ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int) rowid:8(int!null)
 │         ├── stats: [rows=990, distinct(1)=99, null(1)=0, distinct(2)=430.972148, null(2)=0, distinct(6)=99, null(6)=0]
 │         ├── key: (8)
 │         ├── fd: (1)-->(2,3), (8)-->(4-7), (1)==(6), (6)==(1)
 │         ├── inner-join (cross)
 │         │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    ├── stats: [rows=10000000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=0, distinct(6)=100, null(6)=100000, distinct(8)=1000, null(8)=0]
 │         │    ├── key: (1,8)
 │         │    ├── fd: (1)-->(2,3), (8)-->(4-7)
 │         │    ├── scan customers
 │         │    │    ├── columns: id:1(int!null) name:2(string) state:3(string)
 │         │    │    ├── stats: [rows=10000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=0]
 │         │    │    ├── key: (1)
 │         │    │    └── fd: (1)-->(2,3)
 │         │    ├── scan order_history
 │         │    │    ├── columns: order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    │    ├── stats: [rows=1000, distinct(6)=100, null(6)=10, distinct(8)=1000, null(8)=0]
 │         │    │    ├── key: (8)
 │         │    │    └── fd: (8)-->(4-7)
 │         │    └── filters (true)
 │         └── filters
 │              └── id:1 = customer_id:6 [type=bool, outer=(1,6), constraints=(/1: (/NULL - ]; /6: (/NULL - ]), fd=(1)==(6), (6)==(1)]
 └── filters
      └── (id:1 = 1) AND (name:2 = 'andy') [type=bool, outer=(1,2), constraints=(/1: [/1 - /1]; /2: [/'andy' - /'andy']; tight), fd=()-->(1,2)]

# Test equality conditions where all have distinct count 1.
norm
SELECT * FROM order_history WHERE item_id = order_id AND item_id = customer_id AND customer_id = 5
----
select
 ├── columns: order_id:1(int!null) item_id:2(int!null) customer_id:3(int!null) year:4(int)
 ├── stats: [rows=0.901, distinct(1)=0.901, null(1)=0, distinct(2)=0.901, null(2)=0, distinct(3)=0.901, null(3)=0, distinct(1-3)=0.901, null(1-3)=0]
 ├── fd: ()-->(1-3)
 ├── scan order_history
 │    ├── columns: order_id:1(int) item_id:2(int) customer_id:3(int) year:4(int)
 │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10, distinct(3)=100, null(3)=10, distinct(1-3)=1000, null(1-3)=0.001]
 └── filters
      ├── order_id:1 = 5 [type=bool, outer=(1), constraints=(/1: [/5 - /5]; tight), fd=()-->(1)]
      ├── item_id:2 = 5 [type=bool, outer=(2), constraints=(/2: [/5 - /5]; tight), fd=()-->(2)]
      └── customer_id:3 = 5 [type=bool, outer=(3), constraints=(/3: [/5 - /5]; tight), fd=()-->(3)]

# Test equality condition with another condition on one of the attributes.
norm
SELECT * FROM order_history WHERE item_id = order_id AND item_id < 5 AND item_id > 0
----
select
 ├── columns: order_id:1(int!null) item_id:2(int!null) customer_id:3(int) year:4(int)
 ├── stats: [rows=0.99, distinct(1)=0.99, null(1)=0, distinct(2)=0.99, null(2)=0]
 ├── fd: (1)==(2), (2)==(1)
 ├── scan order_history
 │    ├── columns: order_id:1(int) item_id:2(int) customer_id:3(int) year:4(int)
 │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10]
 └── filters
      ├── (item_id:2 < 5) AND (item_id:2 > 0) [type=bool, outer=(2), constraints=(/2: [/1 - /4]; tight)]
      └── item_id:2 = order_id:1 [type=bool, outer=(1,2), constraints=(/1: (/NULL - ]; /2: (/NULL - ]), fd=(1)==(2), (2)==(1)]

# Test equality condition with another condition on a different attribute.
norm
SELECT * FROM order_history WHERE item_id = order_id AND customer_id < 5 AND customer_id > 0
----
select
 ├── columns: order_id:1(int!null) item_id:2(int!null) customer_id:3(int!null) year:4(int)
 ├── stats: [rows=0.9801, distinct(1)=0.9801, null(1)=0, distinct(2)=0.9801, null(2)=0, distinct(3)=0.9801, null(3)=0]
 ├── fd: (1)==(2), (2)==(1)
 ├── scan order_history
 │    ├── columns: order_id:1(int) item_id:2(int) customer_id:3(int) year:4(int)
 │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10, distinct(3)=100, null(3)=10]
 └── filters
      ├── (customer_id:3 < 5) AND (customer_id:3 > 0) [type=bool, outer=(3), constraints=(/3: [/1 - /4]; tight)]
      └── item_id:2 = order_id:1 [type=bool, outer=(1,2), constraints=(/1: (/NULL - ]; /2: (/NULL - ]), fd=(1)==(2), (2)==(1)]

# Test equality condition with another filter condition without a constraint.
norm
SELECT * FROM order_history WHERE item_id = order_id AND customer_id % 2 = 0
----
select
 ├── columns: order_id:1(int!null) item_id:2(int!null) customer_id:3(int) year:4(int)
 ├── immutable
 ├── stats: [rows=3.267, distinct(1)=3.267, null(1)=0, distinct(2)=3.267, null(2)=0]
 ├── fd: (1)==(2), (2)==(1)
 ├── scan order_history
 │    ├── columns: order_id:1(int) item_id:2(int) customer_id:3(int) year:4(int)
 │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10]
 └── filters
      ├── item_id:2 = order_id:1 [type=bool, outer=(1,2), constraints=(/1: (/NULL - ]; /2: (/NULL - ]), fd=(1)==(2), (2)==(1)]
      └── (customer_id:3 % 2) = 0 [type=bool, outer=(3), immutable]

exec-ddl
CREATE TABLE c (x INT, z INT NOT NULL, UNIQUE INDEX x_idx (x))
----

# Test that the distinct and null counts for x are estimated correctly (since it's a weak
# key).
norm
SELECT * FROM c WHERE x >= 0 AND x < 100
----
select
 ├── columns: x:1(int!null) z:2(int!null)
 ├── cardinality: [0 - 100]
 ├── stats: [rows=100, distinct(1)=100, null(1)=0]
 ├── key: (1)
 ├── fd: (1)-->(2)
 ├── scan c
 │    ├── columns: x:1(int) z:2(int!null)
 │    ├── stats: [rows=1000, distinct(1)=991, null(1)=10, distinct(2)=100, null(2)=0]
 │    ├── lax-key: (1,2)
 │    └── fd: (1)~~>(2)
 └── filters
      └── (x:1 >= 0) AND (x:1 < 100) [type=bool, outer=(1), constraints=(/1: [/0 - /99]; tight)]

exec-ddl
CREATE TABLE uvw (u INT, v INT, w INT)
----

# Test selectivity calculations by applying the two constraints in different
# orders.
norm
SELECT * FROM uvw WHERE u=v AND u=10
----
select
 ├── columns: u:1(int!null) v:2(int!null) w:3(int)
 ├── stats: [rows=0.910810811, distinct(1)=0.910810811, null(1)=0, distinct(2)=0.910810811, null(2)=0, distinct(1,2)=0.910810811, null(1,2)=0]
 ├── fd: ()-->(1,2)
 ├── scan uvw
 │    ├── columns: u:1(int) v:2(int) w:3(int)
 │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10, distinct(1,2)=1000, null(1,2)=0.1]
 └── filters
      ├── v:2 = 10 [type=bool, outer=(2), constraints=(/2: [/10 - /10]; tight), fd=()-->(2)]
      └── u:1 = 10 [type=bool, outer=(1), constraints=(/1: [/10 - /10]; tight), fd=()-->(1)]

norm disable=MergeSelects
SELECT * FROM (SELECT * FROM uvw WHERE u=10) WHERE u=v
----
select
 ├── columns: u:1(int!null) v:2(int!null) w:3(int)
 ├── stats: [rows=1.03537072, distinct(1)=1, null(1)=0, distinct(2)=1, null(2)=0]
 ├── fd: ()-->(1,2), (1)==(2), (2)==(1)
 ├── select
 │    ├── columns: u:1(int!null) v:2(int) w:3(int)
 │    ├── stats: [rows=10, distinct(1)=1, null(1)=0, distinct(2)=9.5617925, null(2)=0.1]
 │    ├── fd: ()-->(1)
 │    ├── scan uvw
 │    │    ├── columns: u:1(int) v:2(int) w:3(int)
 │    │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10]
 │    └── filters
 │         └── u:1 = 10 [type=bool, outer=(1), constraints=(/1: [/10 - /10]; tight), fd=()-->(1)]
 └── filters
      └── u:1 = v:2 [type=bool, outer=(1,2), constraints=(/1: (/NULL - ]; /2: (/NULL - ]), fd=(1)==(2), (2)==(1)]

norm disable=MergeSelects
SELECT * FROM (SELECT * FROM uvw WHERE u=v) WHERE u=10
----
select
 ├── columns: u:1(int!null) v:2(int!null) w:3(int)
 ├── stats: [rows=1, distinct(1)=1, null(1)=0]
 ├── fd: ()-->(1,2), (1)==(2), (2)==(1)
 ├── select
 │    ├── columns: u:1(int!null) v:2(int!null) w:3(int)
 │    ├── stats: [rows=9.801, distinct(1)=9.801, null(1)=0, distinct(2)=9.801, null(2)=0]
 │    ├── fd: (1)==(2), (2)==(1)
 │    ├── scan uvw
 │    │    ├── columns: u:1(int) v:2(int) w:3(int)
 │    │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10]
 │    └── filters
 │         └── u:1 = v:2 [type=bool, outer=(1,2), constraints=(/1: (/NULL - ]; /2: (/NULL - ]), fd=(1)==(2), (2)==(1)]
 └── filters
      └── u:1 = 10 [type=bool, outer=(1), constraints=(/1: [/10 - /10]; tight), fd=()-->(1)]

exec-ddl
CREATE TABLE lineitem
(
    l_orderkey int NOT NULL,
    l_partkey int NOT NULL,
    l_suppkey int NOT NULL,
    l_linenumber int NOT NULL,
    l_quantity float NOT NULL,
    l_extendedprice float NOT NULL,
    l_discount float NOT NULL,
    l_tax float NOT NULL,
    l_returnflag char(1) NOT NULL,
    l_linestatus char(1) NOT NULL,
    l_shipdate date NOT NULL,
    l_commitdate date NOT NULL,
    l_receiptdate date NOT NULL,
    l_shipinstruct char(25) NOT NULL,
    l_shipmode char(10) NOT NULL,
    l_comment varchar(44) NOT NULL,
    PRIMARY KEY (l_orderkey, l_linenumber),
    INDEX l_ok (l_orderkey ASC),
    INDEX l_pk (l_partkey ASC),
    INDEX l_sk (l_suppkey ASC),
    INDEX l_sd (l_shipdate ASC),
    INDEX l_cd (l_commitdate ASC),
    INDEX l_rd (l_receiptdate ASC),
    INDEX l_pk_sk (l_partkey ASC, l_suppkey ASC),
    INDEX l_sk_pk (l_suppkey ASC, l_partkey ASC)
);
----

# We can determine that there are exactly 30 days for this range.
opt
SELECT *
FROM lineitem
WHERE
    l_shipdate >= DATE '1995-09-01'
    AND l_shipdate < DATE '1995-10-01';
----
select
 ├── columns: l_orderkey:1(int!null) l_partkey:2(int!null) l_suppkey:3(int!null) l_linenumber:4(int!null) l_quantity:5(float!null) l_extendedprice:6(float!null) l_discount:7(float!null) l_tax:8(float!null) l_returnflag:9(char!null) l_linestatus:10(char!null) l_shipdate:11(date!null) l_commitdate:12(date!null) l_receiptdate:13(date!null) l_shipinstruct:14(char!null) l_shipmode:15(char!null) l_comment:16(varchar!null)
 ├── stats: [rows=300, distinct(11)=30, null(11)=0]
 ├── key: (1,4)
 ├── fd: (1,4)-->(2,3,5-16)
 ├── scan lineitem
 │    ├── columns: l_orderkey:1(int!null) l_partkey:2(int!null) l_suppkey:3(int!null) l_linenumber:4(int!null) l_quantity:5(float!null) l_extendedprice:6(float!null) l_discount:7(float!null) l_tax:8(float!null) l_returnflag:9(char!null) l_linestatus:10(char!null) l_shipdate:11(date!null) l_commitdate:12(date!null) l_receiptdate:13(date!null) l_shipinstruct:14(char!null) l_shipmode:15(char!null) l_comment:16(varchar!null)
 │    ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(2)=100, null(2)=0, distinct(3)=100, null(3)=0, distinct(4)=100, null(4)=0, distinct(5)=100, null(5)=0, distinct(6)=100, null(6)=0, distinct(7)=100, null(7)=0, distinct(8)=100, null(8)=0, distinct(9)=100, null(9)=0, distinct(10)=100, null(10)=0, distinct(11)=100, null(11)=0, distinct(12)=100, null(12)=0, distinct(13)=100, null(13)=0, distinct(14)=100, null(14)=0, distinct(15)=100, null(15)=0, distinct(16)=100, null(16)=0]
 │    ├── key: (1,4)
 │    └── fd: (1,4)-->(2,3,5-16)
 └── filters
      └── (l_shipdate:11 >= '1995-09-01') AND (l_shipdate:11 < '1995-10-01') [type=bool, outer=(11), constraints=(/11: [/'1995-09-01' - /'1995-09-30']; tight)]

# We cannot determine the number of distinct values exactly since the upper
# bound of the date range is compared to a timestamp rather than a date.
opt
SELECT *
FROM lineitem
WHERE
    l_shipdate >= DATE '1995-09-01'
    AND l_shipdate::timestamptz < DATE '1995-10-01';
----
index-join lineitem
 ├── columns: l_orderkey:1(int!null) l_partkey:2(int!null) l_suppkey:3(int!null) l_linenumber:4(int!null) l_quantity:5(float!null) l_extendedprice:6(float!null) l_discount:7(float!null) l_tax:8(float!null) l_returnflag:9(char!null) l_linestatus:10(char!null) l_shipdate:11(date!null) l_commitdate:12(date!null) l_receiptdate:13(date!null) l_shipinstruct:14(char!null) l_shipmode:15(char!null) l_comment:16(varchar!null)
 ├── stable
 ├── stats: [rows=111.111111, distinct(11)=33.3333333, null(11)=0]
 ├── key: (1,4)
 ├── fd: (1,4)-->(2,3,5-16)
 └── select
      ├── columns: l_orderkey:1(int!null) l_linenumber:4(int!null) l_shipdate:11(date!null)
      ├── stable
      ├── stats: [rows=111.111111]
      ├── key: (1,4)
      ├── fd: (1,4)-->(11)
      ├── scan lineitem@l_sd
      │    ├── columns: l_orderkey:1(int!null) l_linenumber:4(int!null) l_shipdate:11(date!null)
      │    ├── constraint: /11/1/4: [/'1995-09-01' - ]
      │    ├── stats: [rows=333.333333, distinct(1)=98.265847, null(1)=0, distinct(4)=98.265847, null(4)=0, distinct(11)=33.3333333, null(11)=0]
      │    ├── key: (1,4)
      │    └── fd: (1,4)-->(11)
      └── filters
           └── l_shipdate:11::TIMESTAMPTZ < '1995-10-01' [type=bool, outer=(11), stable]

# These queries should generate zigzag joins in xform rules. The column statistics
# should be comparable between the norm'd and fully optimized expressions.
opt colstat=11 colstat=12 colstat=(11,12)
SELECT l_shipdate, l_commitdate, l_orderkey, l_linenumber
FROM lineitem
WHERE
    l_shipdate = DATE '1995-09-01'
    AND l_commitdate = DATE '1995-08-01';
----
inner-join (zigzag lineitem@l_sd lineitem@l_cd)
 ├── columns: l_shipdate:11(date!null) l_commitdate:12(date!null) l_orderkey:1(int!null) l_linenumber:4(int!null)
 ├── eq columns: [1 4] = [1 4]
 ├── left fixed columns: [11] = ['1995-09-01']
 ├── right fixed columns: [12] = ['1995-08-01']
 ├── stats: [rows=0.91, distinct(11)=0.91, null(11)=0, distinct(12)=0.91, null(12)=0, distinct(11,12)=0.91, null(11,12)=0]
 ├── key: (1,4)
 ├── fd: ()-->(11,12)
 └── filters
      ├── l_shipdate:11 = '1995-09-01' [type=bool, outer=(11), constraints=(/11: [/'1995-09-01' - /'1995-09-01']; tight), fd=()-->(11)]
      └── l_commitdate:12 = '1995-08-01' [type=bool, outer=(12), constraints=(/12: [/'1995-08-01' - /'1995-08-01']; tight), fd=()-->(12)]

norm colstat=11 colstat=12 colstat=(11,12)
SELECT l_shipdate, l_commitdate, l_orderkey, l_linenumber
FROM lineitem
WHERE
    l_shipdate = DATE '1995-09-01'
    AND l_commitdate = DATE '1995-08-01';
----
select
 ├── columns: l_shipdate:11(date!null) l_commitdate:12(date!null) l_orderkey:1(int!null) l_linenumber:4(int!null)
 ├── stats: [rows=0.91, distinct(11)=0.91, null(11)=0, distinct(12)=0.91, null(12)=0, distinct(11,12)=0.91, null(11,12)=0]
 ├── key: (1,4)
 ├── fd: ()-->(11,12)
 ├── scan lineitem
 │    ├── columns: l_orderkey:1(int!null) l_linenumber:4(int!null) l_shipdate:11(date!null) l_commitdate:12(date!null)
 │    ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(4)=100, null(4)=0, distinct(11)=100, null(11)=0, distinct(12)=100, null(12)=0, distinct(11,12)=1000, null(11,12)=0]
 │    ├── key: (1,4)
 │    └── fd: (1,4)-->(11,12)
 └── filters
      ├── l_shipdate:11 = '1995-09-01' [type=bool, outer=(11), constraints=(/11: [/'1995-09-01' - /'1995-09-01']; tight), fd=()-->(11)]
      └── l_commitdate:12 = '1995-08-01' [type=bool, outer=(12), constraints=(/12: [/'1995-08-01' - /'1995-08-01']; tight), fd=()-->(12)]

# These queries should also generate zigzag joins in xform rules, like the
# ones above. These zigzag joins should be nested inside a lookup join on
# the primary index. Since the zigzag join lies in a new memo group, we will
# see the zigzag-join-specific stats/logprops build and colStat functions in
# action. Again, the colstats of the inner zigzag expression should be
# reasonably close to those of the full normalized select expression.
opt colstat=11 colstat=12 colstat=(11,12)
SELECT l_shipdate, l_commitdate, l_orderkey, l_linenumber, l_quantity
FROM lineitem
WHERE
    l_shipdate = DATE '1995-09-01'
    AND l_commitdate = DATE '1995-08-01';
----
inner-join (lookup lineitem)
 ├── columns: l_shipdate:11(date!null) l_commitdate:12(date!null) l_orderkey:1(int!null) l_linenumber:4(int!null) l_quantity:5(float!null)
 ├── key columns: [1 4] = [1 4]
 ├── lookup columns are key
 ├── stats: [rows=0.91, distinct(11)=0.91, null(11)=0, distinct(12)=0.91, null(12)=0, distinct(11,12)=0.91, null(11,12)=0]
 ├── key: (1,4)
 ├── fd: ()-->(11,12), (1,4)-->(5)
 ├── inner-join (zigzag lineitem@l_sd lineitem@l_cd)
 │    ├── columns: l_orderkey:1(int!null) l_linenumber:4(int!null) l_shipdate:11(date!null) l_commitdate:12(date!null)
 │    ├── eq columns: [1 4] = [1 4]
 │    ├── left fixed columns: [11] = ['1995-09-01']
 │    ├── right fixed columns: [12] = ['1995-08-01']
 │    ├── stats: [rows=0.91, distinct(11)=0.91, null(11)=0, distinct(12)=0.91, null(12)=0, distinct(11,12)=0.91, null(11,12)=0]
 │    ├── fd: ()-->(11,12)
 │    └── filters
 │         ├── l_shipdate:11 = '1995-09-01' [type=bool, outer=(11), constraints=(/11: [/'1995-09-01' - /'1995-09-01']; tight), fd=()-->(11)]
 │         └── l_commitdate:12 = '1995-08-01' [type=bool, outer=(12), constraints=(/12: [/'1995-08-01' - /'1995-08-01']; tight), fd=()-->(12)]
 └── filters (true)

norm colstat=11 colstat=12 colstat=(11,12)
SELECT l_shipdate, l_commitdate, l_orderkey, l_linenumber, l_quantity
FROM lineitem
WHERE
    l_shipdate = DATE '1995-09-01'
    AND l_commitdate = DATE '1995-08-01';
----
select
 ├── columns: l_shipdate:11(date!null) l_commitdate:12(date!null) l_orderkey:1(int!null) l_linenumber:4(int!null) l_quantity:5(float!null)
 ├── stats: [rows=0.91, distinct(11)=0.91, null(11)=0, distinct(12)=0.91, null(12)=0, distinct(11,12)=0.91, null(11,12)=0]
 ├── key: (1,4)
 ├── fd: ()-->(11,12), (1,4)-->(5)
 ├── scan lineitem
 │    ├── columns: l_orderkey:1(int!null) l_linenumber:4(int!null) l_quantity:5(float!null) l_shipdate:11(date!null) l_commitdate:12(date!null)
 │    ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(4)=100, null(4)=0, distinct(5)=100, null(5)=0, distinct(11)=100, null(11)=0, distinct(12)=100, null(12)=0, distinct(11,12)=1000, null(11,12)=0]
 │    ├── key: (1,4)
 │    └── fd: (1,4)-->(5,11,12)
 └── filters
      ├── l_shipdate:11 = '1995-09-01' [type=bool, outer=(11), constraints=(/11: [/'1995-09-01' - /'1995-09-01']; tight), fd=()-->(11)]
      └── l_commitdate:12 = '1995-08-01' [type=bool, outer=(12), constraints=(/12: [/'1995-08-01' - /'1995-08-01']; tight), fd=()-->(12)]

# Create a table with an inverted index to test statistics around
# JSON containment filter operators and zigzag joins.
exec-ddl
CREATE TABLE tjson (a INT PRIMARY KEY, b JSON, c JSON, INVERTED INDEX b_idx (b))
----

exec-ddl
ALTER TABLE tjson INJECT STATISTICS '[
  {
    "columns": ["a"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 5000,
    "distinct_count": 5000
  },
  {
    "columns": ["b"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 5000,
    "distinct_count": 2500
  },
  {
    "columns": ["c"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 5000,
    "distinct_count": 2500
  }
]'
----

# Should generate a scan on the inverted index.
opt
SELECT * FROM tjson WHERE b @> '{"a":"b"}'
----
index-join tjson
 ├── columns: a:1(int!null) b:2(jsonb) c:3(jsonb)
 ├── immutable
 ├── stats: [rows=555.555556]
 ├── key: (1)
 ├── fd: (1)-->(2,3)
 └── scan tjson@b_idx
      ├── columns: a:1(int!null)
      ├── constraint: /2/1: [/'{"a": "b"}' - /'{"a": "b"}']
      ├── stats: [rows=555.555556]
      └── key: (1)

# Should generate a zigzag join on the inverted index. Row count should be
# strictly lower than the above scan.
opt
SELECT * FROM tjson WHERE b @> '{"a":"b", "c":"d"}'
----
inner-join (lookup tjson)
 ├── columns: a:1(int!null) b:2(jsonb) c:3(jsonb)
 ├── key columns: [1] = [1]
 ├── lookup columns are key
 ├── immutable
 ├── stats: [rows=61.7283951]
 ├── key: (1)
 ├── fd: (1)-->(2,3)
 ├── inner-join (zigzag tjson@b_idx tjson@b_idx)
 │    ├── columns: a:1(int!null)
 │    ├── eq columns: [1] = [1]
 │    ├── left fixed columns: [2] = ['{"a": "b"}']
 │    ├── right fixed columns: [2] = ['{"c": "d"}']
 │    ├── stats: [rows=61.7283951, distinct(1)=61.7283951, null(1)=0]
 │    └── filters (true)
 └── filters
      └── b:2 @> '{"a": "b", "c": "d"}' [type=bool, outer=(2), immutable]

# Should generate a select on the table with a JSON filter, since c does not
# have an inverted index.
opt
SELECT * FROM tjson WHERE c @> '{"a":"b"}'
----
select
 ├── columns: a:1(int!null) b:2(jsonb) c:3(jsonb)
 ├── immutable
 ├── stats: [rows=555.555556]
 ├── key: (1)
 ├── fd: (1)-->(2,3)
 ├── scan tjson
 │    ├── columns: a:1(int!null) b:2(jsonb) c:3(jsonb)
 │    ├── stats: [rows=5000, distinct(1)=5000, null(1)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2,3)
 └── filters
      └── c:3 @> '{"a": "b"}' [type=bool, outer=(3), immutable]

# Should have a lower row count than the above case, due to a containment query
# on 2 json paths.
opt
SELECT * FROM tjson WHERE c @> '{"a":"b", "c":"d"}'
----
select
 ├── columns: a:1(int!null) b:2(jsonb) c:3(jsonb)
 ├── immutable
 ├── stats: [rows=61.7283951]
 ├── key: (1)
 ├── fd: (1)-->(2,3)
 ├── scan tjson
 │    ├── columns: a:1(int!null) b:2(jsonb) c:3(jsonb)
 │    ├── stats: [rows=5000, distinct(1)=5000, null(1)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2,3)
 └── filters
      └── c:3 @> '{"a": "b", "c": "d"}' [type=bool, outer=(3), immutable]

# Bump up null counts.
exec-ddl
ALTER TABLE a INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 5000,
    "distinct_count": 5000
  },
  {
    "columns": ["y"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 4000,
    "distinct_count": 400,
    "null_count": 1000
  }
]'
----

exec-ddl
ALTER TABLE b INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 5000,
    "null_count": 2000
  },
  {
    "columns": ["z"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 100
  },
  {
    "columns": ["rowid"],
    "created_at": "2018-01-01 1:30:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 10000
  }
]'
----

exec-ddl
ALTER TABLE c INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 5000,
    "null_count": 5000
  },
  {
    "columns": ["z"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 10000
  }
]'
----

# Distinct values calculation with constraints.
norm
SELECT * FROM b WHERE x = 1 AND z = 2 AND rowid >= 5 AND rowid <= 8
----
project
 ├── columns: x:1(int!null) z:2(int!null)
 ├── cardinality: [0 - 4]
 ├── stats: [rows=0.811440928]
 ├── fd: ()-->(1,2)
 └── select
      ├── columns: x:1(int!null) z:2(int!null) rowid:3(int!null)
      ├── cardinality: [0 - 4]
      ├── stats: [rows=0.811440928, distinct(1)=0.811440928, null(1)=0, distinct(2)=0.811440928, null(2)=0, distinct(3)=0.811440928, null(3)=0, distinct(1,2)=0.811440928, null(1,2)=0, distinct(1-3)=0.811440928, null(1-3)=0]
      ├── key: (3)
      ├── fd: ()-->(1,2)
      ├── scan b
      │    ├── columns: x:1(int) z:2(int!null) rowid:3(int!null)
      │    ├── stats: [rows=10000, distinct(1)=5000, null(1)=2000, distinct(2)=100, null(2)=0, distinct(3)=10000, null(3)=0, distinct(1,2)=10000, null(1,2)=0, distinct(1-3)=10000, null(1-3)=0]
      │    ├── key: (3)
      │    └── fd: (3)-->(1,2)
      └── filters
           ├── (rowid:3 >= 5) AND (rowid:3 <= 8) [type=bool, outer=(3), constraints=(/3: [/5 - /8]; tight)]
           ├── x:1 = 1 [type=bool, outer=(1), constraints=(/1: [/1 - /1]; tight), fd=()-->(1)]
           └── z:2 = 2 [type=bool, outer=(2), constraints=(/2: [/2 - /2]; tight), fd=()-->(2)]

# Can't determine stats from filter.
norm
SELECT * FROM a WHERE x + y < 10
----
select
 ├── columns: x:1(int!null) y:2(int)
 ├── immutable
 ├── stats: [rows=1666.66667]
 ├── key: (1)
 ├── fd: (1)-->(2)
 ├── scan a
 │    ├── columns: x:1(int!null) y:2(int)
 │    ├── stats: [rows=5000]
 │    ├── key: (1)
 │    └── fd: (1)-->(2)
 └── filters
      └── (x:1 + y:2) < 10 [type=bool, outer=(1,2), immutable]

# Remaining filter.
norm
SELECT * FROM a WHERE y = 5 AND x + y < 10
----
select
 ├── columns: x:1(int!null) y:2(int!null)
 ├── stats: [rows=9.35672515, distinct(1)=9.35672515, null(1)=0, distinct(2)=1, null(2)=0, distinct(1,2)=9.35672515, null(1,2)=0]
 ├── key: (1)
 ├── fd: ()-->(2)
 ├── scan a
 │    ├── columns: x:1(int!null) y:2(int)
 │    ├── stats: [rows=5000, distinct(1)=5000, null(1)=0, distinct(2)=400, null(2)=1000, distinct(1,2)=5000, null(1,2)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2)
 └── filters
      ├── y:2 = 5 [type=bool, outer=(2), constraints=(/2: [/5 - /5]; tight), fd=()-->(2)]
      └── x:1 < 5 [type=bool, outer=(1), constraints=(/1: (/NULL - /4]; tight)]

# Test that the null count for x is propagated correctly (since it's a weak
# key).
norm
SELECT * FROM c WHERE x >= 0 AND x < 100
----
select
 ├── columns: x:1(int!null) z:2(int!null)
 ├── cardinality: [0 - 100]
 ├── stats: [rows=100, distinct(1)=100, null(1)=0]
 ├── key: (1)
 ├── fd: (1)-->(2)
 ├── scan c
 │    ├── columns: x:1(int) z:2(int!null)
 │    ├── stats: [rows=10000, distinct(1)=5000, null(1)=5000, distinct(2)=10000, null(2)=0]
 │    ├── lax-key: (1,2)
 │    └── fd: (1)~~>(2)
 └── filters
      └── (x:1 >= 0) AND (x:1 < 100) [type=bool, outer=(1), constraints=(/1: [/0 - /99]; tight)]

# Bump up null counts
exec-ddl
ALTER TABLE customers INJECT STATISTICS '[
{
  "columns": ["name"],
  "created_at": "2018-01-01 1:00:00.00000+00:00",
  "row_count": 10000,
  "distinct_count": 500,
  "null_count": 2000
},
{
  "columns": ["id"],
  "created_at": "2018-01-01 1:30:00.00000+00:00",
  "row_count": 10000,
  "distinct_count": 10000
}
]'
----

# This tests selectivityFromReducedCols
# The following two tests cases are paired together. The first has
# one constraint, one on single non-key column. The second  query has two
# constraints on columns which form a determinant, dependent FD pair.
# The dependent column in this FD pair is from the first test case.
# This series of tests demonstrates that the selectivity
# contribution for a pair of (determinant, dependent) FDs is the
# selectivity of the determinant.
# 1/2 join-subquery-selectivityFromReducedCols tests

build
SELECT * FROM (SELECT * FROM customers, order_history WHERE id = customer_id)
WHERE name='andy'
----
select
 ├── columns: id:1(int!null) name:2(string!null) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 ├── stats: [rows=1.84197978, distinct(2)=1, null(2)=0]
 ├── fd: ()-->(2), (1)-->(3), (1)==(6), (6)==(1)
 ├── project
 │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 │    ├── stats: [rows=990, distinct(1)=99, null(1)=0, distinct(2)=430.972148, null(2)=198, distinct(6)=99, null(6)=0]
 │    ├── fd: (1)-->(2,3), (1)==(6), (6)==(1)
 │    └── select
 │         ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int) rowid:8(int!null)
 │         ├── stats: [rows=990, distinct(1)=99, null(1)=0, distinct(2)=430.972148, null(2)=198, distinct(6)=99, null(6)=0]
 │         ├── key: (8)
 │         ├── fd: (1)-->(2,3), (8)-->(4-7), (1)==(6), (6)==(1)
 │         ├── inner-join (cross)
 │         │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    ├── stats: [rows=10000000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=2000000, distinct(6)=100, null(6)=100000, distinct(8)=1000, null(8)=0]
 │         │    ├── key: (1,8)
 │         │    ├── fd: (1)-->(2,3), (8)-->(4-7)
 │         │    ├── scan customers
 │         │    │    ├── columns: id:1(int!null) name:2(string) state:3(string)
 │         │    │    ├── stats: [rows=10000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=2000]
 │         │    │    ├── key: (1)
 │         │    │    └── fd: (1)-->(2,3)
 │         │    ├── scan order_history
 │         │    │    ├── columns: order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    │    ├── stats: [rows=1000, distinct(6)=100, null(6)=10, distinct(8)=1000, null(8)=0]
 │         │    │    ├── key: (8)
 │         │    │    └── fd: (8)-->(4-7)
 │         │    └── filters (true)
 │         └── filters
 │              └── id:1 = customer_id:6 [type=bool, outer=(1,6), constraints=(/1: (/NULL - ]; /6: (/NULL - ]), fd=(1)==(6), (6)==(1)]
 └── filters
      └── name:2 = 'andy' [type=bool, outer=(2), constraints=(/2: [/'andy' - /'andy']; tight), fd=()-->(2)]

# This tests selectivityFromReducedCols
# The previous tests case and the following are paired together. The first has
# one constraint, one on single non-key column. The second  query has two
# constraints on columns which form a determinant, dependent FD pair.
# The dependent column in this FD pair is from the first test case.
# This series of tests demonstrates that the selectivity
# contribution for a pair of (determinant, dependent) FDs is the
# selectivity of the determinant.
# 2/2 join-subquery-selectivityFromReducedCols tests

build
SELECT * FROM (SELECT * FROM customers, order_history WHERE id = customer_id)
WHERE id = 1 AND name='andy'
----
select
 ├── columns: id:1(int!null) name:2(string!null) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 ├── stats: [rows=8, distinct(1)=1, null(1)=0, distinct(2)=1, null(2)=0]
 ├── fd: ()-->(1-3,6), (1)==(6), (6)==(1)
 ├── project
 │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 │    ├── stats: [rows=990, distinct(1)=99, null(1)=0, distinct(2)=430.972148, null(2)=198, distinct(6)=99, null(6)=0]
 │    ├── fd: (1)-->(2,3), (1)==(6), (6)==(1)
 │    └── select
 │         ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int) rowid:8(int!null)
 │         ├── stats: [rows=990, distinct(1)=99, null(1)=0, distinct(2)=430.972148, null(2)=198, distinct(6)=99, null(6)=0]
 │         ├── key: (8)
 │         ├── fd: (1)-->(2,3), (8)-->(4-7), (1)==(6), (6)==(1)
 │         ├── inner-join (cross)
 │         │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    ├── stats: [rows=10000000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=2000000, distinct(6)=100, null(6)=100000, distinct(8)=1000, null(8)=0]
 │         │    ├── key: (1,8)
 │         │    ├── fd: (1)-->(2,3), (8)-->(4-7)
 │         │    ├── scan customers
 │         │    │    ├── columns: id:1(int!null) name:2(string) state:3(string)
 │         │    │    ├── stats: [rows=10000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=2000]
 │         │    │    ├── key: (1)
 │         │    │    └── fd: (1)-->(2,3)
 │         │    ├── scan order_history
 │         │    │    ├── columns: order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    │    ├── stats: [rows=1000, distinct(6)=100, null(6)=10, distinct(8)=1000, null(8)=0]
 │         │    │    ├── key: (8)
 │         │    │    └── fd: (8)-->(4-7)
 │         │    └── filters (true)
 │         └── filters
 │              └── id:1 = customer_id:6 [type=bool, outer=(1,6), constraints=(/1: (/NULL - ]; /6: (/NULL - ]), fd=(1)==(6), (6)==(1)]
 └── filters
      └── (id:1 = 1) AND (name:2 = 'andy') [type=bool, outer=(1,2), constraints=(/1: [/1 - /1]; /2: [/'andy' - /'andy']; tight), fd=()-->(1,2)]

exec-ddl
CREATE TABLE nulls (x INT, y INT);
----

exec-ddl
ALTER TABLE nulls INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 1000,
    "distinct_count": 0,
    "null_count": 1000
  }
]'
----

build
SELECT * FROM nulls WHERE x = y
----
project
 ├── columns: x:1(int!null) y:2(int!null)
 ├── stats: [rows=9.9e-10]
 ├── fd: (1)==(2), (2)==(1)
 └── select
      ├── columns: x:1(int!null) y:2(int!null) rowid:3(int!null)
      ├── stats: [rows=9.9e-10, distinct(1)=1e-10, null(1)=0, distinct(2)=1e-10, null(2)=0]
      ├── key: (3)
      ├── fd: (3)-->(1,2), (1)==(2), (2)==(1)
      ├── scan nulls
      │    ├── columns: x:1(int) y:2(int) rowid:3(int!null)
      │    ├── stats: [rows=1000, distinct(1)=1, null(1)=1000, distinct(2)=100, null(2)=10, distinct(3)=1000, null(3)=0]
      │    ├── key: (3)
      │    └── fd: (3)-->(1,2)
      └── filters
           └── x:1 = y:2 [type=bool, outer=(1,2), constraints=(/1: (/NULL - ]; /2: (/NULL - ]), fd=(1)==(2), (2)==(1)]

build
SELECT * FROM nulls WHERE x IS NULL
----
project
 ├── columns: x:1(int) y:2(int)
 ├── stats: [rows=1000]
 ├── fd: ()-->(1)
 └── select
      ├── columns: x:1(int) y:2(int) rowid:3(int!null)
      ├── stats: [rows=1000, distinct(1)=1, null(1)=1000]
      ├── key: (3)
      ├── fd: ()-->(1), (3)-->(2)
      ├── scan nulls
      │    ├── columns: x:1(int) y:2(int) rowid:3(int!null)
      │    ├── stats: [rows=1000, distinct(1)=1, null(1)=1000, distinct(3)=1000, null(3)=0]
      │    ├── key: (3)
      │    └── fd: (3)-->(1,2)
      └── filters
           └── x:1 IS NULL [type=bool, outer=(1), constraints=(/1: [/NULL - /NULL]; tight), fd=()-->(1)]


# Regression test for #34440. Ensure the null count for x is less than or equal
# to the row count.
build colstat=1
SELECT * FROM nulls WHERE y = 3
----
project
 ├── columns: x:1(int) y:2(int!null)
 ├── stats: [rows=10, distinct(1)=0.999956829, null(1)=10]
 ├── fd: ()-->(2)
 └── select
      ├── columns: x:1(int) y:2(int!null) rowid:3(int!null)
      ├── stats: [rows=10, distinct(1)=0.999956829, null(1)=10, distinct(2)=1, null(2)=0]
      ├── key: (3)
      ├── fd: ()-->(2), (3)-->(1)
      ├── scan nulls
      │    ├── columns: x:1(int) y:2(int) rowid:3(int!null)
      │    ├── stats: [rows=1000, distinct(1)=1, null(1)=1000, distinct(2)=100, null(2)=10, distinct(3)=1000, null(3)=0]
      │    ├── key: (3)
      │    └── fd: (3)-->(1,2)
      └── filters
           └── y:2 = 3 [type=bool, outer=(2), constraints=(/2: [/3 - /3]; tight), fd=()-->(2)]

# Sample testcase using exprgen.
expr colstat=1 colstat=2
(Select 
  (FakeRel
    [
      (OutputCols [ (NewColumn "a" "int") (NewColumn "b" "int") (NewColumn "c" "int")] )
      (Cardinality "-")
      (Stats `[
        {
          "columns": ["a"],
          "distinct_count": 100,
          "null_count": 0, 
          "row_count": 100, 
          "created_at": "2018-01-01 1:00:00.00000+00:00"
        },
        {
          "columns": ["b"],
          "distinct_count": 20,
          "null_count": 5, 
          "row_count": 100, 
          "created_at": "2018-01-01 1:00:00.00000+00:00"
        }
      ]`)
    ]
  )
  [ (Eq (Var "b") (Const 1 "int")) ]
)
----
select
 ├── columns: a:1(int) b:2(int!null) c:3(int)
 ├── stats: [rows=5, distinct(1)=5, null(1)=0, distinct(2)=1, null(2)=0]
 ├── fd: ()-->(2)
 ├── fake-rel
 │    ├── columns: a:1(int) b:2(int) c:3(int)
 │    └── stats: [rows=100, distinct(1)=100, null(1)=0, distinct(2)=20, null(2)=5]
 └── filters
      └── b:2 = 1 [type=bool, outer=(2), constraints=(/2: [/1 - /1]; tight), fd=()-->(2)]

# Regression test for #37754.
norm
SELECT
    1
FROM
    (
        VALUES
            (true, NULL, B'001000101101110'),
            (true, e'19\x1e':::STRING, NULL)
    )
        AS t (_bool, _string, _bit)
GROUP BY
    _string, _bit
HAVING
    min(_bool)
----
project
 ├── columns: "?column?":5(int!null)
 ├── cardinality: [0 - 2]
 ├── stats: [rows=1]
 ├── fd: ()-->(5)
 ├── select
 │    ├── columns: column2:2(string) column3:3(varbit) min:4(bool!null)
 │    ├── cardinality: [0 - 2]
 │    ├── stats: [rows=1, distinct(4)=1, null(4)=0]
 │    ├── key: (2,3)
 │    ├── fd: ()-->(4)
 │    ├── group-by
 │    │    ├── columns: column2:2(string) column3:3(varbit) min:4(bool!null)
 │    │    ├── grouping columns: column2:2(string) column3:3(varbit)
 │    │    ├── cardinality: [1 - 2]
 │    │    ├── stats: [rows=2, distinct(4)=2, null(4)=0, distinct(2,3)=2, null(2,3)=0]
 │    │    ├── key: (2,3)
 │    │    ├── fd: (2,3)-->(4)
 │    │    ├── values
 │    │    │    ├── columns: column1:1(bool!null) column2:2(string) column3:3(varbit)
 │    │    │    ├── cardinality: [2 - 2]
 │    │    │    ├── stats: [rows=2, distinct(2,3)=2, null(2,3)=0]
 │    │    │    ├── (true, NULL, B'001000101101110') [type=tuple{bool, string, varbit}]
 │    │    │    └── (true, e'19\x1e', NULL) [type=tuple{bool, string, varbit}]
 │    │    └── aggregations
 │    │         └── min [as=min:4, type=bool, outer=(1)]
 │    │              └── column1:1 [type=bool]
 │    └── filters
 │         └── min:4 [type=bool, outer=(4), constraints=(/4: [/true - /true]; tight), fd=()-->(4)]
 └── projections
      └── 1 [as="?column?":5, type=int]

# Test that distinct count estimates are correct for date ranges.
exec-ddl
CREATE TABLE date_test (
    k INT PRIMARY KEY,
    d1 date NOT NULL,
    d2 date NOT NULL,
    d3 date NOT NULL,
    INDEX d1_idx (d1 ASC),
    INDEX d2_idx (d2 DESC)
)
----

opt
SELECT d1 FROM date_test WHERE d1 > DATE '1995-10-01' AND d1 < DATE '1995-11-01'
----
scan date_test@d1_idx
 ├── columns: d1:2(date!null)
 ├── constraint: /2/1: [/'1995-10-02' - /'1995-10-31']
 └── stats: [rows=300, distinct(2)=30, null(2)=0]

opt
SELECT d1 FROM date_test WHERE d1 >= DATE '1995-10-01' AND d1 <= DATE '1995-11-01'
----
scan date_test@d1_idx
 ├── columns: d1:2(date!null)
 ├── constraint: /2/1: [/'1995-10-01' - /'1995-11-01']
 └── stats: [rows=320, distinct(2)=32, null(2)=0]

opt
SELECT d2 FROM date_test WHERE d2 > DATE '1903-10-01' AND d2 <= DATE '1903-11-01'
----
scan date_test@d2_idx
 ├── columns: d2:3(date!null)
 ├── constraint: /-3/1: [/'1903-11-01' - /'1903-10-02']
 └── stats: [rows=310, distinct(3)=31, null(3)=0]

opt
SELECT d2 FROM date_test WHERE d2 >= DATE '2003-10-01' AND d2 < DATE '2003-11-01'
----
scan date_test@d2_idx
 ├── columns: d2:3(date!null)
 ├── constraint: /-3/1: [/'2003-10-31' - /'2003-10-01']
 └── stats: [rows=310, distinct(3)=31, null(3)=0]

opt
SELECT d3 FROM date_test WHERE d3 >= DATE '2003-10-01' AND d3 < DATE '2003-11-01'
----
select
 ├── columns: d3:4(date!null)
 ├── stats: [rows=310, distinct(4)=31, null(4)=0]
 ├── scan date_test
 │    ├── columns: d3:4(date!null)
 │    └── stats: [rows=1000, distinct(4)=100, null(4)=0]
 └── filters
      └── (d3:4 >= '2003-10-01') AND (d3:4 < '2003-11-01') [type=bool, outer=(4), constraints=(/4: [/'2003-10-01' - /'2003-10-31']; tight)]

opt
SELECT d3 FROM date_test WHERE d3 >= DATE '1903-10-01' AND d3 < DATE '2003-10-01'
----
select
 ├── columns: d3:4(date!null)
 ├── stats: [rows=1000, distinct(4)=100, null(4)=0]
 ├── scan date_test
 │    ├── columns: d3:4(date!null)
 │    └── stats: [rows=1000, distinct(4)=100, null(4)=0]
 └── filters
      └── (d3:4 >= '1903-10-01') AND (d3:4 < '2003-10-01') [type=bool, outer=(4), constraints=(/4: [/'1903-10-01' - /'2003-09-30']; tight)]

# Regression test for #38344. Avoid floating point precision errors.
exec-ddl
CREATE TABLE t38344 (x BOOL)
----

exec-ddl
ALTER TABLE t38344 INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 20000000000,
    "distinct_count": 1,
    "null_count": 20000000000
  }
]'
----

norm disable=InlineWith
WITH t(x) AS (
  SELECT (t1.x::int << 5533)::bool OR t2.x  AS x
  FROM t38344 AS t1 LEFT JOIN t38344 AS t2 ON true
)
SELECT x FROM t WHERE x
----
with &1 (t)
 ├── columns: x:6(bool!null)
 ├── immutable
 ├── stats: [rows=1.98e+20, distinct(6)=1, null(6)=0]
 ├── fd: ()-->(6)
 ├── project
 │    ├── columns: x:5(bool)
 │    ├── immutable
 │    ├── stats: [rows=4e+20]
 │    ├── left-join (cross)
 │    │    ├── columns: t1.x:1(bool) t2.x:3(bool)
 │    │    ├── multiplicity: left-rows(one-or-more), right-rows(zero-or-more)
 │    │    ├── stats: [rows=4e+20]
 │    │    ├── scan t1
 │    │    │    ├── columns: t1.x:1(bool)
 │    │    │    └── stats: [rows=2e+10]
 │    │    ├── scan t2
 │    │    │    ├── columns: t2.x:3(bool)
 │    │    │    └── stats: [rows=2e+10]
 │    │    └── filters (true)
 │    └── projections
 │         └── (t1.x:1::INT8 << 5533)::BOOL OR t2.x:3 [as=x:5, type=bool, outer=(1,3), immutable]
 └── select
      ├── columns: x:6(bool!null)
      ├── stats: [rows=1.98e+20, distinct(6)=1, null(6)=0]
      ├── fd: ()-->(6)
      ├── with-scan &1 (t)
      │    ├── columns: x:6(bool)
      │    ├── mapping:
      │    │    └──  x:5(bool) => x:6(bool)
      │    └── stats: [rows=4e+20, distinct(6)=3, null(6)=4e+18]
      └── filters
           └── x:6 [type=bool, outer=(6), constraints=(/6: [/true - /true]; tight), fd=()-->(6)]

# Regression test for #38375. Avoid floating point precision errors.
exec-ddl
CREATE TABLE t38375 (x INT, y INT)
----

exec-ddl
ALTER TABLE t38375 INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 20000000000,
    "distinct_count": 20000000000,
    "null_count": 20000000000
  },
  {
    "columns": ["y"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 20000000000,
    "distinct_count": 10,
    "null_count": 0
  }
]'
----

opt colstat=2
SELECT * FROM t38375 WHERE x = 1
----
select
 ├── columns: x:1(int!null) y:2(int)
 ├── stats: [rows=2, distinct(1)=1, null(1)=0, distinct(2)=1.81269262, null(2)=0]
 ├── fd: ()-->(1)
 ├── scan t38375
 │    ├── columns: x:1(int) y:2(int)
 │    └── stats: [rows=2e+10, distinct(1)=2e+10, null(1)=2e+10, distinct(2)=10, null(2)=0]
 └── filters
      └── x:1 = 1 [type=bool, outer=(1), constraints=(/1: [/1 - /1]; tight), fd=()-->(1)]

# Support OR constraints.
exec-ddl
CREATE TABLE nation
(
    n_nationkey int PRIMARY KEY,
    n_name char(25) NOT NULL,
    n_regionkey int NOT NULL,
    neighbor char(25) NOT NULL,
    INDEX n_rk (n_regionkey ASC)
)
----

exec-ddl
ALTER TABLE nation INJECT STATISTICS '[
  {
    "columns": ["n_name"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 1000000,
    "distinct_count": 2,
    "null_count": 0
  }
]'
----

opt
SELECT * FROM nation WHERE n_name = 'FRANCE' OR n_name = 'GERMANY'
----
select
 ├── columns: n_nationkey:1(int!null) n_name:2(char!null) n_regionkey:3(int!null) neighbor:4(char!null)
 ├── stats: [rows=1000000, distinct(2)=2, null(2)=0]
 ├── key: (1)
 ├── fd: (1)-->(2-4)
 ├── scan nation
 │    ├── columns: n_nationkey:1(int!null) n_name:2(char!null) n_regionkey:3(int!null) neighbor:4(char!null)
 │    ├── stats: [rows=1000000, distinct(1)=1000000, null(1)=0, distinct(2)=2, null(2)=0, distinct(3)=100000, null(3)=0, distinct(4)=100000, null(4)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2-4)
 └── filters
      └── (n_name:2 = 'FRANCE') OR (n_name:2 = 'GERMANY') [type=bool, outer=(2), constraints=(/2: [/'FRANCE' - /'FRANCE'] [/'GERMANY' - /'GERMANY']; tight)]

opt
SELECT * FROM nation WHERE (n_name = 'FRANCE' AND neighbor = 'GERMANY') OR (n_name = 'GERMANY' AND neighbor = 'FRANCE')
----
select
 ├── columns: n_nationkey:1(int!null) n_name:2(char!null) n_regionkey:3(int!null) neighbor:4(char!null)
 ├── stats: [rows=6.66666667, distinct(2)=2, null(2)=0, distinct(4)=2, null(4)=0]
 ├── key: (1)
 ├── fd: (1)-->(2-4)
 ├── scan nation
 │    ├── columns: n_nationkey:1(int!null) n_name:2(char!null) n_regionkey:3(int!null) neighbor:4(char!null)
 │    ├── stats: [rows=1000000, distinct(1)=1000000, null(1)=0, distinct(2)=2, null(2)=0, distinct(3)=100000, null(3)=0, distinct(4)=100000, null(4)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2-4)
 └── filters
      └── ((n_name:2 = 'FRANCE') AND (neighbor:4 = 'GERMANY')) OR ((n_name:2 = 'GERMANY') AND (neighbor:4 = 'FRANCE')) [type=bool, outer=(2,4), constraints=(/2: [/'FRANCE' - /'FRANCE'] [/'GERMANY' - /'GERMANY']; /4: [/'FRANCE' - /'FRANCE'] [/'GERMANY' - /'GERMANY'])]

opt
SELECT * FROM nation WHERE (n_name, neighbor) in (('FRANCE', 'GERMANY'), ('GERMANY', 'FRANCE'))
----
select
 ├── columns: n_nationkey:1(int!null) n_name:2(char!null) n_regionkey:3(int!null) neighbor:4(char!null)
 ├── stats: [rows=20, distinct(2)=2, null(2)=0, distinct(4)=2, null(4)=0]
 ├── key: (1)
 ├── fd: (1)-->(2-4)
 ├── scan nation
 │    ├── columns: n_nationkey:1(int!null) n_name:2(char!null) n_regionkey:3(int!null) neighbor:4(char!null)
 │    ├── stats: [rows=1000000, distinct(1)=1000000, null(1)=0, distinct(2)=2, null(2)=0, distinct(3)=100000, null(3)=0, distinct(4)=100000, null(4)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2-4)
 └── filters
      └── (n_name:2, neighbor:4) IN (('FRANCE', 'GERMANY'), ('GERMANY', 'FRANCE')) [type=bool, outer=(2,4), constraints=(/2/4: [/'FRANCE'/'GERMANY' - /'FRANCE'/'GERMANY'] [/'GERMANY'/'FRANCE' - /'GERMANY'/'FRANCE']; /4: [/'FRANCE' - /'FRANCE'] [/'GERMANY' - /'GERMANY']; tight)]

# Make sure the that histogram and distinct counts don't interfere with each
# other during selectivity calculation.
exec-ddl
CREATE TABLE hist_and_distinct (
  a INT,
  b INT,
  c INT,
  d INT,
  INDEX idx_a (a)
)
----

exec-ddl
ALTER TABLE hist_and_distinct INJECT STATISTICS '[
  {
    "columns": ["a"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 1000,
    "distinct_count": 40,
    "histo_col_type": "int",
    "histo_buckets": [
      {"num_eq": 0, "num_range": 0, "distinct_range": 0, "upper_bound": "0"},
      {"num_eq": 10, "num_range": 90, "distinct_range": 9, "upper_bound": "10"},
      {"num_eq": 20, "num_range": 180, "distinct_range": 9, "upper_bound": "20"},
      {"num_eq": 30, "num_range": 270, "distinct_range": 9, "upper_bound": "30"},
      {"num_eq": 40, "num_range": 360, "distinct_range": 9, "upper_bound": "40"}
    ]
  },
  {
    "columns": ["b"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 1000,
    "distinct_count": 5
  },
  {
    "columns": ["c"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 1000,
    "distinct_count": 5
  },
  {
    "columns": ["d"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 1000,
    "distinct_count": 120
  }
]'
----

norm
SELECT * FROM hist_and_distinct WHERE a = 10 AND b = 10 AND c = 10 AND d >= 10 AND d < 100
----
select
 ├── columns: a:1(int!null) b:2(int!null) c:3(int!null) d:4(int!null)
 ├── stats: [rows=0.39, distinct(1)=0.39, null(1)=0, distinct(2)=0.39, null(2)=0, distinct(3)=0.39, null(3)=0, distinct(4)=0.39, null(4)=0, distinct(1-3)=0.39, null(1-3)=0, distinct(1-4)=0.39, null(1-4)=0]
 │   histogram(1)=  0 0.39
 │                <--- 10
 ├── fd: ()-->(1-3)
 ├── scan hist_and_distinct
 │    ├── columns: a:1(int) b:2(int) c:3(int) d:4(int)
 │    └── stats: [rows=1000, distinct(1)=40, null(1)=0, distinct(2)=5, null(2)=0, distinct(3)=5, null(3)=0, distinct(4)=120, null(4)=0, distinct(1-3)=1000, null(1-3)=0, distinct(1-4)=1000, null(1-4)=0]
 │        histogram(1)=  0  0  90  10  180  20  270  30  360  40
 │                     <--- 0 ---- 10 ----- 20 ----- 30 ----- 40
 └── filters
      ├── (d:4 >= 10) AND (d:4 < 100) [type=bool, outer=(4), constraints=(/4: [/10 - /99]; tight)]
      ├── a:1 = 10 [type=bool, outer=(1), constraints=(/1: [/10 - /10]; tight), fd=()-->(1)]
      ├── b:2 = 10 [type=bool, outer=(2), constraints=(/2: [/10 - /10]; tight), fd=()-->(2)]
      └── c:3 = 10 [type=bool, outer=(3), constraints=(/3: [/10 - /10]; tight), fd=()-->(3)]

# Test that a histogram on a boolean column is used.
exec-ddl
CREATE TABLE hist_bool (a INT, b BOOL)
----

exec-ddl
ALTER TABLE hist_bool INJECT STATISTICS '[
  {
    "columns": ["a"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 1000,
    "distinct_count": 40
  },
  {
    "columns": ["b"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 1000,
    "distinct_count": 2,
    "histo_col_type": "BOOL",
    "histo_buckets": [
      {"num_eq": 900, "num_range": 0, "distinct_range": 0, "upper_bound": "false"},
      {"num_eq": 100, "num_range": 0, "distinct_range": 0, "upper_bound": "true"}
    ]
  }
]'
----

norm
SELECT * FROM hist_bool WHERE b = false
----
select
 ├── columns: a:1(int) b:2(bool!null)
 ├── stats: [rows=900, distinct(2)=1, null(2)=0]
 │   histogram(2)=  0   900
 │                <--- false
 ├── fd: ()-->(2)
 ├── scan hist_bool
 │    ├── columns: a:1(int) b:2(bool)
 │    └── stats: [rows=1000, distinct(2)=2, null(2)=0]
 │        histogram(2)=  0   900   0  100
 │                     <--- false --- true
 └── filters
      └── b:2 = false [type=bool, outer=(2), constraints=(/2: [/false - /false]; tight), fd=()-->(2)]

exec-ddl
CREATE TABLE t0(c0 INT)
----

exec-ddl
CREATE VIEW v0(c0) AS SELECT CASE WHEN t0.c0 > 0 THEN 1 ELSE t0.rowid END FROM t0
----

exec-ddl
ALTER TABLE t0 INJECT STATISTICS '[
  {
    "columns": ["c0"],
    "created_at": "2020-01-28 03:02:57.841772+00:00",
    "row_count": 3,
    "distinct_count": 1,
    "null_count": 3
  },
  {
    "columns": ["rowid"],
    "created_at": "2020-01-28 03:03:03.012072+00:00",
    "row_count": 2,
    "distinct_count": 2,
    "null_count": 0,
    "histo_buckets": [
      {
        "distinct_range": 0,
        "num_eq": 1,
        "num_range": 0,
        "upper_bound": "3"
      },
      {
        "distinct_range": 0,
        "num_eq": 1,
        "num_range": 0,
        "upper_bound": "4"
      }
    ],
    "histo_col_type": "INT8"
  }
]'
----

# Regression test for #44418. Make sure inconsistent stats don't cause an
# error.
norm
SELECT * FROM v0 WHERE v0.c0 > 0
----
select
 ├── columns: c0:3(int!null)
 ├── stats: [rows=1, distinct(3)=1, null(3)=0]
 ├── project
 │    ├── columns: rowid:3(int)
 │    ├── stats: [rows=2, distinct(3)=2, null(3)=0]
 │    ├── scan t0
 │    │    ├── columns: c0:1(int) t0.rowid:2(int!null)
 │    │    ├── stats: [rows=2, distinct(1,2)=2, null(1,2)=0]
 │    │    ├── key: (2)
 │    │    └── fd: (2)-->(1)
 │    └── projections
 │         └── CASE WHEN c0:1 > 0 THEN 1 ELSE t0.rowid:2 END [as=rowid:3, type=int, outer=(1,2)]
 └── filters
      └── rowid:3 > 0 [type=bool, outer=(3), constraints=(/3: [/1 - ]; tight)]

exec-ddl
ALTER TABLE a INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2020-01-28 03:02:57.841772+00:00",
    "row_count": 3,
    "distinct_count": 3
  }
]'
----

# Regression test for #44563. Set a lower bound on the distinct count from
# the multi-span constraint.
norm
SELECT * FROM a WHERE x <= 5 OR x = 10 OR x = 15
----
select
 ├── columns: x:1(int!null) y:2(int)
 ├── stats: [rows=2, distinct(1)=2, null(1)=0]
 ├── key: (1)
 ├── fd: (1)-->(2)
 ├── scan a
 │    ├── columns: x:1(int!null) y:2(int)
 │    ├── stats: [rows=3, distinct(1)=3, null(1)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2)
 └── filters
      └── ((x:1 <= 5) OR (x:1 = 10)) OR (x:1 = 15) [type=bool, outer=(1), constraints=(/1: (/NULL - /5] [/10 - /10] [/15 - /15]; tight)]

exec-ddl
CREATE TABLE data (
  user_id UUID NOT NULL,
  name VARCHAR(255) NULL,
  created TIMESTAMPTZ,
  INDEX user_id_idx (user_id ASC),
  INDEX name_idx (name ASC),
  INDEX created_idx (created ASC)
)
----

exec-ddl
ALTER TABLE data INJECT STATISTICS '[
  {
    "columns": ["user_id"],
    "created_at": "2020-01-28 03:02:57.841772+00:00",
    "row_count": 10000,
    "distinct_count": 1000,
    "null_count": 0,
    "histo_buckets": [
      {
        "distinct_range": 0,
        "num_eq": 1,
        "num_range": 0,
        "upper_bound": "3b57b3e4-a68a-9b47-2752-e365d7d8954e"
      },
      {
        "distinct_range": 499,
        "num_eq": 1,
        "num_range": 4998,
        "upper_bound": "6b49a786-387b-d5a2-6582-4e963eb4d537"
      },
      {
        "distinct_range": 499,
        "num_eq": 1,
        "num_range": 4999,
        "upper_bound": "d9739a48-d5be-9a62-e752-34d877e56ba5"
      }
    ],
    "histo_col_type": "UUID"
  },
  {
    "columns": ["name"],
    "created_at": "2020-01-28 03:03:03.012072+00:00",
    "row_count": 10000,
    "distinct_count": 1000,
    "null_count": 0,
    "histo_buckets": [
      {
        "distinct_range": 0,
        "num_eq": 1,
        "num_range": 0,
        "upper_bound": "a"
      },
      {
        "distinct_range": 499,
        "num_eq": 1,
        "num_range": 4998,
        "upper_bound": "b"
      },
      {
        "distinct_range": 499,
        "num_eq": 1,
        "num_range": 4999,
        "upper_bound": "c"
      }
    ],
    "histo_col_type": "STRING"
  },
  {
    "columns": ["created"],
    "created_at": "2020-01-28 03:03:03.012072+00:00",
    "row_count": 10000,
    "distinct_count": 10000,
    "null_count": 0,
    "histo_buckets": [
      {
        "distinct_range": 0,
        "num_eq": 1,
        "num_range": 0,
        "upper_bound": "2020-02-11 07:25:00+00:00"
      },
      {
        "distinct_range": 4998,
        "num_eq": 1,
        "num_range": 4998,
        "upper_bound": "2020-03-21 06:45:41+00:00"
      },
      {
        "distinct_range": 4999,
        "num_eq": 1,
        "num_range": 4999,
        "upper_bound": "2020-04-21 06:25:41+00:00"
      }
    ],
    "histo_col_type": "TIMESTAMPTZ"
  }
]'
----

# Make sure that using a histogram produces correct stats with equality
# predicates on data types such as UUID, string, and timepstamptz.
norm
SELECT * FROM data WHERE user_id = '679d3e56-b985-63d2-5442-e4ba7a8479e3'
----
select
 ├── columns: user_id:1(uuid!null) name:2(varchar) created:3(timestamptz)
 ├── stats: [rows=10.0160321, distinct(1)=1, null(1)=0]
 │   histogram(1)=  0                  10.016
 │                <--- '679d3e56-b985-63d2-5442-e4ba7a8479e3'
 ├── fd: ()-->(1)
 ├── scan data
 │    ├── columns: user_id:1(uuid!null) name:2(varchar) created:3(timestamptz)
 │    └── stats: [rows=10000, distinct(1)=1000, null(1)=0]
 │        histogram(1)=  0                    1                     4998                    1                     4999                    1
 │                     <--- '3b57b3e4-a68a-9b47-2752-e365d7d8954e' ------ '6b49a786-387b-d5a2-6582-4e963eb4d537' ------ 'd9739a48-d5be-9a62-e752-34d877e56ba5'
 └── filters
      └── user_id:1 = '679d3e56-b985-63d2-5442-e4ba7a8479e3' [type=bool, outer=(1), constraints=(/1: [/'679d3e56-b985-63d2-5442-e4ba7a8479e3' - /'679d3e56-b985-63d2-5442-e4ba7a8479e3']; tight), fd=()-->(1)]

norm
SELECT * FROM data WHERE name = 'abc'
----
select
 ├── columns: user_id:1(uuid!null) name:2(varchar!null) created:3(timestamptz)
 ├── stats: [rows=10.0160321, distinct(2)=1, null(2)=0]
 │   histogram(2)=  0 10.016
 │                <--- 'abc'
 ├── fd: ()-->(2)
 ├── scan data
 │    ├── columns: user_id:1(uuid!null) name:2(varchar) created:3(timestamptz)
 │    └── stats: [rows=10000, distinct(1)=1000, null(1)=0, distinct(2)=1000, null(2)=0]
 │        histogram(1)=  0                    1                     4998                    1                     4999                    1
 │                     <--- '3b57b3e4-a68a-9b47-2752-e365d7d8954e' ------ '6b49a786-387b-d5a2-6582-4e963eb4d537' ------ 'd9739a48-d5be-9a62-e752-34d877e56ba5'
 │        histogram(2)=  0   1   4998   1   4999   1
 │                     <--- 'a' ------ 'b' ------ 'c'
 └── filters
      └── name:2 = 'abc' [type=bool, outer=(2), constraints=(/2: [/'abc' - /'abc']; tight), fd=()-->(2)]

norm
SELECT * FROM data WHERE created = '2020-04-11 06:25:41+00:00'
----
select
 ├── columns: user_id:1(uuid!null) name:2(varchar) created:3(timestamptz!null)
 ├── stats: [rows=1, distinct(3)=1, null(3)=0]
 │   histogram(3)=  0               1
 │                <--- '2020-04-11 06:25:41+00:00'
 ├── fd: ()-->(3)
 ├── scan data
 │    ├── columns: user_id:1(uuid!null) name:2(varchar) created:3(timestamptz)
 │    └── stats: [rows=10000, distinct(1)=1000, null(1)=0, distinct(3)=10000, null(3)=0]
 │        histogram(1)=  0                    1                     4998                    1                     4999                    1
 │                     <--- '3b57b3e4-a68a-9b47-2752-e365d7d8954e' ------ '6b49a786-387b-d5a2-6582-4e963eb4d537' ------ 'd9739a48-d5be-9a62-e752-34d877e56ba5'
 │        histogram(3)=  0               1               4998               1               4999               1
 │                     <--- '2020-02-11 07:25:00+00:00' ------ '2020-03-21 06:45:41+00:00' ------ '2020-04-21 06:25:41+00:00'
 └── filters
      └── created:3 = '2020-04-11 06:25:41+00:00' [type=bool, outer=(3), constraints=(/3: [/'2020-04-11 06:25:41+00:00' - /'2020-04-11 06:25:41+00:00']; tight), fd=()-->(3)]

exec-ddl
ALTER TABLE a INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2020-05-26 03:02:57.841772+00:00",
    "row_count": 1000,
    "distinct_count": 1000
  }
]'
----

# Regression test for #48828. Stats for BETWEEN SYMMETRIC should be based on
# the tight constraint rather than calculated as 1/3rd of the cardinality.
norm
SELECT * FROM a WHERE x BETWEEN SYMMETRIC 25 and 50
----
select
 ├── columns: x:1(int!null) y:2(int)
 ├── cardinality: [0 - 26]
 ├── stats: [rows=26, distinct(1)=26, null(1)=0]
 ├── key: (1)
 ├── fd: (1)-->(2)
 ├── scan a
 │    ├── columns: x:1(int!null) y:2(int)
 │    ├── stats: [rows=1000, distinct(1)=1000, null(1)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2)
 └── filters
      └── ((x:1 >= 25) AND (x:1 <= 50)) OR ((x:1 >= 50) AND (x:1 <= 25)) [type=bool, outer=(1), constraints=(/1: [/25 - /50]; tight)]

exec-ddl
ALTER TABLE b INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2020-01-28 03:02:57.841772+00:00",
    "row_count": 10000,
    "distinct_count": 1000
  },
  {
    "columns": ["z"],
    "created_at": "2020-01-28 03:02:57.841772+00:00",
    "row_count": 10000,
    "distinct_count": 100
  } ,
  {
    "columns": ["x","z"],
    "created_at": "2020-01-28 03:02:57.841772+00:00",
    "row_count": 10000,
    "distinct_count": 1500
  }
]'
----

# Multi-column stats test.
build
SELECT * FROM b WHERE x = 1 AND z = 2
----
project
 ├── columns: x:1(int!null) z:2(int!null)
 ├── stats: [rows=6.01]
 ├── fd: ()-->(1,2)
 └── select
      ├── columns: x:1(int!null) z:2(int!null) rowid:3(int!null)
      ├── stats: [rows=6.01, distinct(1)=1, null(1)=0, distinct(2)=1, null(2)=0, distinct(1,2)=1, null(1,2)=0]
      ├── key: (3)
      ├── fd: ()-->(1,2)
      ├── scan b
      │    ├── columns: x:1(int) z:2(int!null) rowid:3(int!null)
      │    ├── stats: [rows=10000, distinct(1)=1000, null(1)=0, distinct(2)=100, null(2)=0, distinct(3)=10000, null(3)=0, distinct(1,2)=1500, null(1,2)=0]
      │    ├── key: (3)
      │    └── fd: (3)-->(1,2)
      └── filters
           └── (x:1 = 1) AND (z:2 = 2) [type=bool, outer=(1,2), constraints=(/1: [/1 - /1]; /2: [/2 - /2]; tight), fd=()-->(1,2)]
