exec-ddl
CREATE TABLE a (x INT PRIMARY KEY, y INT)
----

exec-ddl
CREATE TABLE b (x INT, z INT NOT NULL)
----

exec-ddl
ALTER TABLE a INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 5000,
    "distinct_count": 5000
  },
  {
    "columns": ["y"],
    "created_at": "2018-01-01 1:30:00.00000+00:00",
    "row_count": 4000,
    "distinct_count": 400
  }
]'
----

exec-ddl
ALTER TABLE b INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 5000
  },
  {
    "columns": ["z"],
    "created_at": "2018-01-01 1:30:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 100
  },
  {
    "columns": ["rowid"],
    "created_at": "2018-01-01 1:30:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 10000
  }
]'
----

norm
SELECT * FROM a WHERE true
----
scan a
 ├── columns: x:1(int!null) y:2(int)
 ├── stats: [rows=4000]
 ├── key: (1)
 └── fd: (1)-->(2)

norm
SELECT * FROM a WHERE false
----
values
 ├── columns: x:1(int!null) y:2(int!null)
 ├── cardinality: [0 - 0]
 ├── stats: [rows=0]
 ├── key: ()
 └── fd: ()-->(1,2)

# Distinct values calculation with constraints.
norm
SELECT * FROM b WHERE x = 1 AND z = 2 AND rowid >= 5 AND rowid <= 8
----
project
 ├── columns: x:1(int!null) z:2(int!null)
 ├── stats: [rows=8e-06]
 ├── fd: ()-->(1,2)
 └── select
      ├── columns: x:1(int!null) z:2(int!null) rowid:3(int!null)
      ├── stats: [rows=8e-06, distinct(1)=8e-06, null(1)=0, distinct(2)=8e-06, null(2)=0, distinct(3)=8e-06, null(3)=0]
      ├── key: (3)
      ├── fd: ()-->(1,2)
      ├── scan b
      │    ├── columns: x:1(int) z:2(int!null) rowid:3(int!null)
      │    ├── stats: [rows=10000, distinct(1)=5000, null(1)=0, distinct(2)=100, null(2)=0, distinct(3)=10000, null(3)=0]
      │    ├── key: (3)
      │    └── fd: (3)-->(1,2)
      └── filters
           ├── (rowid >= 5) AND (rowid <= 8) [type=bool, outer=(3), constraints=(/3: [/5 - /8]; tight)]
           ├── x = 1 [type=bool, outer=(1), constraints=(/1: [/1 - /1]; tight), fd=()-->(1)]
           └── z = 2 [type=bool, outer=(2), constraints=(/2: [/2 - /2]; tight), fd=()-->(2)]

# Can't determine stats from filter.
norm
SELECT * FROM a WHERE x + y < 10
----
select
 ├── columns: x:1(int!null) y:2(int)
 ├── stats: [rows=1333.33333, distinct(1)=1333.33333, null(1)=0]
 ├── key: (1)
 ├── fd: (1)-->(2)
 ├── scan a
 │    ├── columns: x:1(int!null) y:2(int)
 │    ├── stats: [rows=4000, distinct(1)=4000, null(1)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2)
 └── filters
      └── (x + y) < 10 [type=bool, outer=(1,2)]

# Remaining filter.
norm
SELECT * FROM a WHERE y = 5 AND x + y < 10
----
select
 ├── columns: x:1(int!null) y:2(int!null)
 ├── stats: [rows=3.33333333, distinct(1)=3.33333333, null(1)=0, distinct(2)=1, null(2)=0]
 ├── key: (1)
 ├── fd: ()-->(2)
 ├── scan a
 │    ├── columns: x:1(int!null) y:2(int)
 │    ├── stats: [rows=4000, distinct(1)=4000, null(1)=0, distinct(2)=400, null(2)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2)
 └── filters
      ├── y = 5 [type=bool, outer=(2), constraints=(/2: [/5 - /5]; tight), fd=()-->(2)]
      └── (x + y) < 10 [type=bool, outer=(1,2)]

# Contradiction.
norm
SELECT * FROM a WHERE x IS NULL
----
select
 ├── columns: x:1(int!null) y:2(int)
 ├── cardinality: [0 - 1]
 ├── stats: [rows=1, distinct(1)=1, null(1)=0]
 ├── key: ()
 ├── fd: ()-->(1,2)
 ├── scan a
 │    ├── columns: x:1(int!null) y:2(int)
 │    ├── stats: [rows=4000, distinct(1)=4000, null(1)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2)
 └── filters
      └── x IS NULL [type=bool, outer=(1), constraints=(/1: [/NULL - /NULL]; tight), fd=()-->(1)]

norm
SELECT sum(x) FROM b WHERE x > 1000 AND x <= 2000 GROUP BY z
----
project
 ├── columns: sum:4(decimal)
 ├── stats: [rows=100]
 └── group-by
      ├── columns: z:2(int!null) sum:4(decimal)
      ├── grouping columns: z:2(int!null)
      ├── stats: [rows=100, distinct(2)=100, null(2)=0]
      ├── key: (2)
      ├── fd: (2)-->(4)
      ├── select
      │    ├── columns: x:1(int!null) z:2(int!null)
      │    ├── stats: [rows=2000, distinct(1)=1000, null(1)=0, distinct(2)=100, null(2)=0]
      │    ├── scan b
      │    │    ├── columns: x:1(int) z:2(int!null)
      │    │    └── stats: [rows=10000, distinct(1)=5000, null(1)=0, distinct(2)=100, null(2)=0]
      │    └── filters
      │         └── (x > 1000) AND (x <= 2000) [type=bool, outer=(1), constraints=(/1: [/1001 - /2000]; tight)]
      └── aggregations
           └── sum [type=decimal, outer=(1)]
                └── variable: x [type=int]

# Regression: statistics builder panics when end key is NULL when it's trying
# to compute start/end int boundaries.
exec-ddl
CREATE TABLE idx (x INT PRIMARY KEY, y INT, z INT, INDEX yz (y DESC, z))
----

opt
SELECT y FROM idx WHERE y < 5 AND z < 10
----
project
 ├── columns: y:2(int!null)
 ├── stats: [rows=108.9]
 └── select
      ├── columns: y:2(int!null) z:3(int!null)
      ├── stats: [rows=108.9, distinct(2)=33.3333333, null(2)=0, distinct(3)=33.3333333, null(3)=0]
      ├── scan idx@yz
      │    ├── columns: y:2(int!null) z:3(int)
      │    ├── constraint: /-2/3/1: (/4/NULL - /NULL)
      │    └── stats: [rows=330, distinct(2)=33.3333333, null(2)=0, distinct(3)=100, null(3)=10]
      └── filters
           └── z < 10 [type=bool, outer=(3), constraints=(/3: (/NULL - /9]; tight)]

# Regression: certain queries could cause a NaN expected number of rows via a divide-by-zero.
exec-ddl
CREATE TABLE tab0(pk INTEGER PRIMARY KEY, col0 INTEGER, col1 FLOAT, col2 TEXT, col3 INTEGER, col4 FLOAT, col5 TEXT)
----

# Note: it's not clear that this still tests the above issue, but I have left
# it here anyway as an interesting test case. I've added another query below
# to regression-test the divide-by-zero issue.
opt
SELECT pk FROM tab0 WHERE
  col0 = 1 AND
  col0 = 2 AND
  (col0 = 1 OR col0 IN (SELECT col3 FROM tab0)) AND
  (col0 = 1 OR col0 IN (SELECT col3 FROM tab0))
----
values
 ├── columns: pk:1(int!null)
 ├── cardinality: [0 - 0]
 ├── stats: [rows=0]
 ├── key: ()
 └── fd: ()-->(1)

exec-ddl
ALTER TABLE tab0 INJECT STATISTICS '[
{
  "columns": ["col0"],
  "created_at": "2018-01-01 1:00:00.00000+00:00",
  "row_count": 100,
  "distinct_count": 0,
  "null_count": 100
},
{
  "columns": ["col3"],
  "created_at": "2018-01-01 1:00:00.00000+00:00",
  "row_count": 100,
  "distinct_count": 10
}
]'
----

opt
SELECT count(*) FROM (SELECT * FROM tab0 WHERE col3 = 10) GROUP BY col0
----
project
 ├── columns: count:8(int)
 ├── stats: [rows=0.999973439]
 └── group-by
      ├── columns: col0:2(int) count_rows:8(int)
      ├── grouping columns: col0:2(int)
      ├── stats: [rows=0.999973439, distinct(2)=0.999973439, null(2)=0.999973439]
      ├── key: (2)
      ├── fd: (2)-->(8)
      ├── select
      │    ├── columns: col0:2(int) col3:5(int!null)
      │    ├── stats: [rows=10, distinct(2)=0.999973439, null(2)=10, distinct(5)=1, null(5)=0]
      │    ├── fd: ()-->(5)
      │    ├── scan tab0
      │    │    ├── columns: col0:2(int) col3:5(int)
      │    │    └── stats: [rows=100, distinct(2)=1, null(2)=100, distinct(5)=10, null(5)=0]
      │    └── filters
      │         └── col3 = 10 [type=bool, outer=(5), constraints=(/5: [/10 - /10]; tight), fd=()-->(5)]
      └── aggregations
           └── count-rows [type=int]


exec-ddl
CREATE TABLE customers (id INT PRIMARY KEY, name STRING, state STRING)
----

exec-ddl
CREATE TABLE order_history (order_id INT, item_id INT, customer_id INT, year INT)
----

exec-ddl
CREATE TABLE district (d_id INT, d_w_id INT, d_name STRING, PRIMARY KEY(d_id, d_w_id))
----

exec-ddl
ALTER TABLE district INJECT STATISTICS '[
{
  "columns": ["d_id"],
  "created_at": "2018-01-01 1:00:00.00000+00:00",
  "row_count": 100,
  "distinct_count": 10
},
{
  "columns": ["d_w_id"],
  "created_at": "2018-01-01 1:30:00.00000+00:00",
  "row_count": 100,
  "distinct_count": 10
},
{
  "columns": ["d_name"],
  "created_at": "2018-01-01 1:30:00.00000+00:00",
  "row_count": 100,
  "distinct_count": 100
}
]'
----

# This tests selectivityFromReducedCols.
# Since the reduced column set is (d_id, d_name), and
# both columns have distinct count 1, we expect this
# to calculate selectivity through selectivityFromReducedCols.
# The output is the same as the naive approach.
norm
SELECT * FROM district WHERE d_id = 1 AND d_name='bobs_burgers'
----
select
 ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string!null)
 ├── stats: [rows=0.1, distinct(1)=0.1, null(1)=0, distinct(2)=0.0995511979, null(2)=0, distinct(3)=0.1, null(3)=0]
 ├── key: (2)
 ├── fd: ()-->(1,3)
 ├── scan district
 │    ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string)
 │    ├── stats: [rows=100, distinct(1)=10, null(1)=0, distinct(2)=10, null(2)=0, distinct(3)=100, null(3)=0]
 │    ├── key: (1,2)
 │    └── fd: (1,2)-->(3)
 └── filters
      ├── d_id = 1 [type=bool, outer=(1), constraints=(/1: [/1 - /1]; tight), fd=()-->(1)]
      └── d_name = 'bobs_burgers' [type=bool, outer=(3), constraints=(/3: [/'bobs_burgers' - /'bobs_burgers']; tight), fd=()-->(3)]

norm
SELECT * FROM district WHERE d_id = 1 and d_name LIKE 'bob'
----
select
 ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string!null)
 ├── stats: [rows=0.1, distinct(1)=0.1, null(1)=0, distinct(2)=0.0995511979, null(2)=0, distinct(3)=0.1, null(3)=0]
 ├── key: (2)
 ├── fd: ()-->(1,3)
 ├── scan district
 │    ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string)
 │    ├── stats: [rows=100, distinct(1)=10, null(1)=0, distinct(2)=10, null(2)=0, distinct(3)=100, null(3)=0]
 │    ├── key: (1,2)
 │    └── fd: (1,2)-->(3)
 └── filters
      ├── d_id = 1 [type=bool, outer=(1), constraints=(/1: [/1 - /1]; tight), fd=()-->(1)]
      └── d_name LIKE 'bob' [type=bool, outer=(3), constraints=(/3: [/'bob' - /'bob']; tight), fd=()-->(3)]

# This tests selectivityFromReducedCols.
# Since (1,2)-->(3) in order to use selectivityFromReducedCols,
# both (1,2) must have distinct=1 after applying the filter. Since
# d_id is a range constraint, this fails, and we fall back to the
# naive estimation for selectivity.
norm
SELECT * FROM district WHERE d_id > 1 AND d_id < 10 AND d_w_id=10 AND d_name='bobs_burgers'
----
select
 ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string!null)
 ├── stats: [rows=0.08, distinct(1)=0.08, null(1)=0, distinct(2)=0.08, null(2)=0, distinct(3)=0.08, null(3)=0]
 ├── key: (1)
 ├── fd: ()-->(2,3)
 ├── scan district
 │    ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string)
 │    ├── stats: [rows=100, distinct(1)=10, null(1)=0, distinct(2)=10, null(2)=0, distinct(3)=100, null(3)=0]
 │    ├── key: (1,2)
 │    └── fd: (1,2)-->(3)
 └── filters
      ├── (d_id > 1) AND (d_id < 10) [type=bool, outer=(1), constraints=(/1: [/2 - /9]; tight)]
      ├── d_w_id = 10 [type=bool, outer=(2), constraints=(/2: [/10 - /10]; tight), fd=()-->(2)]
      └── d_name = 'bobs_burgers' [type=bool, outer=(3), constraints=(/3: [/'bobs_burgers' - /'bobs_burgers']; tight), fd=()-->(3)]

# This tests selectivityFromReducedCols
# We don't apply the selectivity on d_name since (1,2)-->3.
norm
SELECT * FROM district WHERE d_id = 1 AND d_w_id=10 AND d_name='hello'
----
select
 ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string!null)
 ├── cardinality: [0 - 1]
 ├── stats: [rows=1, distinct(1)=1, null(1)=0, distinct(2)=1, null(2)=0, distinct(3)=1, null(3)=0]
 ├── key: ()
 ├── fd: ()-->(1-3)
 ├── scan district
 │    ├── columns: d_id:1(int!null) d_w_id:2(int!null) d_name:3(string)
 │    ├── stats: [rows=100, distinct(1)=10, null(1)=0, distinct(2)=10, null(2)=0, distinct(3)=100, null(3)=0]
 │    ├── key: (1,2)
 │    └── fd: (1,2)-->(3)
 └── filters
      ├── d_id = 1 [type=bool, outer=(1), constraints=(/1: [/1 - /1]; tight), fd=()-->(1)]
      ├── d_w_id = 10 [type=bool, outer=(2), constraints=(/2: [/10 - /10]; tight), fd=()-->(2)]
      └── d_name = 'hello' [type=bool, outer=(3), constraints=(/3: [/'hello' - /'hello']; tight), fd=()-->(3)]

exec-ddl
ALTER TABLE customers INJECT STATISTICS '[
{
  "columns": ["name"],
  "created_at": "2018-01-01 1:00:00.00000+00:00",
  "row_count": 10000,
  "distinct_count": 500
},
{
  "columns": ["id"],
  "created_at": "2018-01-01 1:30:00.00000+00:00",
  "row_count": 10000,
  "distinct_count": 10000
}
]'
----

# This tests selectivityFromReducedCols
# The following two tests cases are paired together. The first has
# one constraint, one on single non-key column. The second  query has two
# constraints on columns which form a determinant, dependent FD pair.
# The dependent column in this FD pair is from the first test case.
# This series of tests demonstrates that the selectivity
# contribution for a pair of (determinant, dependent) FDs is the
# selectivity of the determinant.
# 1/2 join-subquery-selectivityFromReducedCols tests

build
SELECT * FROM (SELECT * FROM customers, order_history WHERE id = customer_id)
WHERE name='andy'
----
select
 ├── columns: id:1(int!null) name:2(string!null) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 ├── stats: [rows=2.31299908, distinct(1)=2.28907214, null(1)=0, distinct(2)=1, null(2)=0, distinct(6)=2.28907214, null(6)=0]
 ├── fd: ()-->(2), (1)-->(3), (1)==(6), (6)==(1)
 ├── project
 │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 │    ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(2)=432.339125, null(2)=0, distinct(6)=100, null(6)=0]
 │    ├── fd: (1)-->(2,3), (1)==(6), (6)==(1)
 │    └── select
 │         ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int) rowid:8(int!null)
 │         ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(2)=432.339125, null(2)=0, distinct(6)=100, null(6)=0, distinct(8)=632.138954, null(8)=0]
 │         ├── key: (8)
 │         ├── fd: (1)-->(2,3), (8)-->(4-7), (1)==(6), (6)==(1)
 │         ├── inner-join (hash)
 │         │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    ├── stats: [rows=10000000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=0, distinct(6)=100, null(6)=100000, distinct(8)=1000, null(8)=0]
 │         │    ├── key: (1,8)
 │         │    ├── fd: (1)-->(2,3), (8)-->(4-7)
 │         │    ├── scan customers
 │         │    │    ├── columns: id:1(int!null) name:2(string) state:3(string)
 │         │    │    ├── stats: [rows=10000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=0]
 │         │    │    ├── key: (1)
 │         │    │    └── fd: (1)-->(2,3)
 │         │    ├── scan order_history
 │         │    │    ├── columns: order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    │    ├── stats: [rows=1000, distinct(6)=100, null(6)=10, distinct(8)=1000, null(8)=0]
 │         │    │    ├── key: (8)
 │         │    │    └── fd: (8)-->(4-7)
 │         │    └── filters (true)
 │         └── filters
 │              └── id = customer_id [type=bool, outer=(1,6), constraints=(/1: (/NULL - ]; /6: (/NULL - ]), fd=(1)==(6), (6)==(1)]
 └── filters
      └── name = 'andy' [type=bool, outer=(2), constraints=(/2: [/'andy' - /'andy']; tight), fd=()-->(2)]

# This tests selectivityFromReducedCols
# The previous tests case and the following are paired together. The first has
# one constraint, one on single non-key column. The second  query has two
# constraints on columns which form a determinant, dependent FD pair.
# The dependent column in this FD pair is from the first test case.
# This series of tests demonstrates that the selectivity
# contribution for a pair of (determinant, dependent) FDs is the
# selectivity of the determinant.
# 2/2 join-subquery-selectivityFromReducedCols tests

build
SELECT * FROM (SELECT * FROM customers, order_history WHERE id = customer_id)
WHERE id = 1 AND name='andy'
----
select
 ├── columns: id:1(int!null) name:2(string!null) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 ├── stats: [rows=10, distinct(1)=1, null(1)=0, distinct(2)=1, null(2)=0, distinct(6)=9.5617925, null(6)=0]
 ├── fd: ()-->(1-3,6), (1)==(6), (6)==(1)
 ├── project
 │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 │    ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(2)=432.339125, null(2)=0, distinct(6)=100, null(6)=0]
 │    ├── fd: (1)-->(2,3), (1)==(6), (6)==(1)
 │    └── select
 │         ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int) rowid:8(int!null)
 │         ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(2)=432.339125, null(2)=0, distinct(6)=100, null(6)=0, distinct(8)=632.138954, null(8)=0]
 │         ├── key: (8)
 │         ├── fd: (1)-->(2,3), (8)-->(4-7), (1)==(6), (6)==(1)
 │         ├── inner-join (hash)
 │         │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    ├── stats: [rows=10000000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=0, distinct(6)=100, null(6)=100000, distinct(8)=1000, null(8)=0]
 │         │    ├── key: (1,8)
 │         │    ├── fd: (1)-->(2,3), (8)-->(4-7)
 │         │    ├── scan customers
 │         │    │    ├── columns: id:1(int!null) name:2(string) state:3(string)
 │         │    │    ├── stats: [rows=10000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=0]
 │         │    │    ├── key: (1)
 │         │    │    └── fd: (1)-->(2,3)
 │         │    ├── scan order_history
 │         │    │    ├── columns: order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    │    ├── stats: [rows=1000, distinct(6)=100, null(6)=10, distinct(8)=1000, null(8)=0]
 │         │    │    ├── key: (8)
 │         │    │    └── fd: (8)-->(4-7)
 │         │    └── filters (true)
 │         └── filters
 │              └── id = customer_id [type=bool, outer=(1,6), constraints=(/1: (/NULL - ]; /6: (/NULL - ]), fd=(1)==(6), (6)==(1)]
 └── filters
      └── (id = 1) AND (name = 'andy') [type=bool, outer=(1,2), constraints=(/1: [/1 - /1]; /2: [/'andy' - /'andy']; tight), fd=()-->(1,2)]

# Test equality conditions where all have distinct count 1.
norm
SELECT * FROM order_history WHERE item_id = order_id AND item_id = customer_id AND customer_id = 5
----
select
 ├── columns: order_id:1(int!null) item_id:2(int!null) customer_id:3(int!null) year:4(int)
 ├── stats: [rows=0.99, distinct(1)=0.99, null(1)=0, distinct(2)=0.99, null(2)=0, distinct(3)=0.99, null(3)=0]
 ├── fd: ()-->(1-3), (1)==(2,3), (2)==(1,3), (3)==(1,2)
 ├── scan order_history
 │    ├── columns: order_id:1(int) item_id:2(int) customer_id:3(int) year:4(int)
 │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10, distinct(3)=100, null(3)=10]
 └── filters
      ├── item_id = order_id [type=bool, outer=(1,2), constraints=(/1: (/NULL - ]; /2: (/NULL - ]), fd=(1)==(2), (2)==(1)]
      ├── item_id = customer_id [type=bool, outer=(2,3), constraints=(/2: (/NULL - ]; /3: (/NULL - ]), fd=(2)==(3), (3)==(2)]
      └── customer_id = 5 [type=bool, outer=(3), constraints=(/3: [/5 - /5]; tight), fd=()-->(3)]

# Test equality condition with another condition on one of the attributes.
norm
SELECT * FROM order_history WHERE item_id = order_id AND item_id < 5 AND item_id > 0
----
select
 ├── columns: order_id:1(int!null) item_id:2(int!null) customer_id:3(int) year:4(int)
 ├── stats: [rows=0.99, distinct(1)=0.99, null(1)=0, distinct(2)=0.99, null(2)=0]
 ├── fd: (1)==(2), (2)==(1)
 ├── scan order_history
 │    ├── columns: order_id:1(int) item_id:2(int) customer_id:3(int) year:4(int)
 │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10]
 └── filters
      ├── (item_id < 5) AND (item_id > 0) [type=bool, outer=(2), constraints=(/2: [/1 - /4]; tight)]
      └── item_id = order_id [type=bool, outer=(1,2), constraints=(/1: (/NULL - ]; /2: (/NULL - ]), fd=(1)==(2), (2)==(1)]

# Test equality condition with another condition on a different attribute.
norm
SELECT * FROM order_history WHERE item_id = order_id AND customer_id < 5 AND customer_id > 0
----
select
 ├── columns: order_id:1(int!null) item_id:2(int!null) customer_id:3(int!null) year:4(int)
 ├── stats: [rows=0.99, distinct(1)=0.99, null(1)=0, distinct(2)=0.99, null(2)=0, distinct(3)=0.99, null(3)=0]
 ├── fd: (1)==(2), (2)==(1)
 ├── scan order_history
 │    ├── columns: order_id:1(int) item_id:2(int) customer_id:3(int) year:4(int)
 │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10, distinct(3)=100, null(3)=10]
 └── filters
      ├── (customer_id < 5) AND (customer_id > 0) [type=bool, outer=(3), constraints=(/3: [/1 - /4]; tight)]
      └── item_id = order_id [type=bool, outer=(1,2), constraints=(/1: (/NULL - ]; /2: (/NULL - ]), fd=(1)==(2), (2)==(1)]

# Test equality condition with another filter condition without a constraint.
norm
SELECT * FROM order_history WHERE item_id = order_id AND customer_id % 2 = 0
----
select
 ├── columns: order_id:1(int!null) item_id:2(int!null) customer_id:3(int) year:4(int)
 ├── stats: [rows=3.33333333, distinct(1)=3.33333333, null(1)=0, distinct(2)=3.33333333, null(2)=0]
 ├── fd: (1)==(2), (2)==(1)
 ├── scan order_history
 │    ├── columns: order_id:1(int) item_id:2(int) customer_id:3(int) year:4(int)
 │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10]
 └── filters
      ├── item_id = order_id [type=bool, outer=(1,2), constraints=(/1: (/NULL - ]; /2: (/NULL - ]), fd=(1)==(2), (2)==(1)]
      └── (customer_id % 2) = 0 [type=bool, outer=(3)]

exec-ddl
CREATE TABLE c (x INT, z INT NOT NULL, UNIQUE INDEX x_idx (x))
----

# Test that the distinct and null counts for x are estimated correctly (since it's a weak
# key).
norm
SELECT * FROM c WHERE x >= 0 AND x < 100
----
select
 ├── columns: x:1(int!null) z:2(int!null)
 ├── stats: [rows=99.8990918, distinct(1)=99.8990918, null(1)=0, distinct(2)=65.4824076, null(2)=0]
 ├── key: (1)
 ├── fd: (1)-->(2)
 ├── scan c
 │    ├── columns: x:1(int) z:2(int!null)
 │    ├── stats: [rows=1000, distinct(1)=991, null(1)=10, distinct(2)=100, null(2)=0]
 │    ├── lax-key: (1,2)
 │    └── fd: (1)~~>(2)
 └── filters
      └── (x >= 0) AND (x < 100) [type=bool, outer=(1), constraints=(/1: [/0 - /99]; tight)]

exec-ddl
CREATE TABLE uvw (u INT, v INT, w INT)
----

# Test selectivity calculations by applying the two constraints in different
# orders.
norm
SELECT * FROM uvw WHERE u=v AND u=10
----
select
 ├── columns: u:1(int!null) v:2(int!null) w:3(int)
 ├── stats: [rows=0.99, distinct(1)=0.99, null(1)=0, distinct(2)=0.99, null(2)=0]
 ├── fd: ()-->(1,2), (1)==(2), (2)==(1)
 ├── scan uvw
 │    ├── columns: u:1(int) v:2(int) w:3(int)
 │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10]
 └── filters
      ├── u = v [type=bool, outer=(1,2), constraints=(/1: (/NULL - ]; /2: (/NULL - ]), fd=(1)==(2), (2)==(1)]
      └── u = 10 [type=bool, outer=(1), constraints=(/1: [/10 - /10]; tight), fd=()-->(1)]

norm disable=MergeSelects
SELECT * FROM (SELECT * FROM uvw WHERE u=10) WHERE u=v
----
select
 ├── columns: u:1(int!null) v:2(int!null) w:3(int)
 ├── stats: [rows=1.04536248, distinct(1)=1, null(1)=0, distinct(2)=1, null(2)=0]
 ├── fd: ()-->(1,2), (1)==(2), (2)==(1)
 ├── select
 │    ├── columns: u:1(int!null) v:2(int) w:3(int)
 │    ├── stats: [rows=9.9, distinct(1)=1, null(1)=0, distinct(2)=9.47039924, null(2)=0.099]
 │    ├── fd: ()-->(1)
 │    ├── scan uvw
 │    │    ├── columns: u:1(int) v:2(int) w:3(int)
 │    │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10]
 │    └── filters
 │         └── u = 10 [type=bool, outer=(1), constraints=(/1: [/10 - /10]; tight), fd=()-->(1)]
 └── filters
      └── u = v [type=bool, outer=(1,2), constraints=(/1: (/NULL - ]; /2: (/NULL - ]), fd=(1)==(2), (2)==(1)]

norm disable=MergeSelects
SELECT * FROM (SELECT * FROM uvw WHERE u=v) WHERE u=10
----
select
 ├── columns: u:1(int!null) v:2(int!null) w:3(int)
 ├── stats: [rows=1, distinct(1)=1, null(1)=0, distinct(2)=1, null(2)=0]
 ├── fd: ()-->(1,2), (1)==(2), (2)==(1)
 ├── select
 │    ├── columns: u:1(int!null) v:2(int!null) w:3(int)
 │    ├── stats: [rows=10, distinct(1)=10, null(1)=0, distinct(2)=10, null(2)=0]
 │    ├── fd: (1)==(2), (2)==(1)
 │    ├── scan uvw
 │    │    ├── columns: u:1(int) v:2(int) w:3(int)
 │    │    └── stats: [rows=1000, distinct(1)=100, null(1)=10, distinct(2)=100, null(2)=10]
 │    └── filters
 │         └── u = v [type=bool, outer=(1,2), constraints=(/1: (/NULL - ]; /2: (/NULL - ]), fd=(1)==(2), (2)==(1)]
 └── filters
      └── u = 10 [type=bool, outer=(1), constraints=(/1: [/10 - /10]; tight), fd=()-->(1)]

exec-ddl
CREATE TABLE lineitem
(
    l_orderkey int NOT NULL,
    l_partkey int NOT NULL,
    l_suppkey int NOT NULL,
    l_linenumber int NOT NULL,
    l_quantity float NOT NULL,
    l_extendedprice float NOT NULL,
    l_discount float NOT NULL,
    l_tax float NOT NULL,
    l_returnflag char(1) NOT NULL,
    l_linestatus char(1) NOT NULL,
    l_shipdate date NOT NULL,
    l_commitdate date NOT NULL,
    l_receiptdate date NOT NULL,
    l_shipinstruct char(25) NOT NULL,
    l_shipmode char(10) NOT NULL,
    l_comment varchar(44) NOT NULL,
    PRIMARY KEY (l_orderkey, l_linenumber),
    INDEX l_ok (l_orderkey ASC),
    INDEX l_pk (l_partkey ASC),
    INDEX l_sk (l_suppkey ASC),
    INDEX l_sd (l_shipdate ASC),
    INDEX l_cd (l_commitdate ASC),
    INDEX l_rd (l_receiptdate ASC),
    INDEX l_pk_sk (l_partkey ASC, l_suppkey ASC),
    INDEX l_sk_pk (l_suppkey ASC, l_partkey ASC)
);
----

# We can determine that there are exactly 30 days for this range.
opt
SELECT *
FROM lineitem
WHERE
    l_shipdate >= DATE '1995-09-01'
    AND l_shipdate < DATE '1995-10-01';
----
select
 ├── columns: l_orderkey:1(int!null) l_partkey:2(int!null) l_suppkey:3(int!null) l_linenumber:4(int!null) l_quantity:5(float!null) l_extendedprice:6(float!null) l_discount:7(float!null) l_tax:8(float!null) l_returnflag:9(char!null) l_linestatus:10(char!null) l_shipdate:11(date!null) l_commitdate:12(date!null) l_receiptdate:13(date!null) l_shipinstruct:14(char!null) l_shipmode:15(char!null) l_comment:16(varchar!null)
 ├── stats: [rows=300, distinct(1)=97.1752475, null(1)=0, distinct(2)=97.1752475, null(2)=0, distinct(3)=97.1752475, null(3)=0, distinct(4)=97.1752475, null(4)=0, distinct(5)=97.1752475, null(5)=0, distinct(6)=97.1752475, null(6)=0, distinct(7)=97.1752475, null(7)=0, distinct(8)=97.1752475, null(8)=0, distinct(9)=97.1752475, null(9)=0, distinct(10)=97.1752475, null(10)=0, distinct(11)=30, null(11)=0, distinct(12)=97.1752475, null(12)=0, distinct(13)=97.1752475, null(13)=0, distinct(14)=97.1752475, null(14)=0, distinct(15)=97.1752475, null(15)=0, distinct(16)=97.1752475, null(16)=0]
 ├── key: (1,4)
 ├── fd: (1,4)-->(2,3,5-16)
 ├── scan lineitem
 │    ├── columns: l_orderkey:1(int!null) l_partkey:2(int!null) l_suppkey:3(int!null) l_linenumber:4(int!null) l_quantity:5(float!null) l_extendedprice:6(float!null) l_discount:7(float!null) l_tax:8(float!null) l_returnflag:9(char!null) l_linestatus:10(char!null) l_shipdate:11(date!null) l_commitdate:12(date!null) l_receiptdate:13(date!null) l_shipinstruct:14(char!null) l_shipmode:15(char!null) l_comment:16(varchar!null)
 │    ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(2)=100, null(2)=0, distinct(3)=100, null(3)=0, distinct(4)=100, null(4)=0, distinct(5)=100, null(5)=0, distinct(6)=100, null(6)=0, distinct(7)=100, null(7)=0, distinct(8)=100, null(8)=0, distinct(9)=100, null(9)=0, distinct(10)=100, null(10)=0, distinct(11)=100, null(11)=0, distinct(12)=100, null(12)=0, distinct(13)=100, null(13)=0, distinct(14)=100, null(14)=0, distinct(15)=100, null(15)=0, distinct(16)=100, null(16)=0]
 │    ├── key: (1,4)
 │    └── fd: (1,4)-->(2,3,5-16)
 └── filters
      └── (l_shipdate >= '1995-09-01') AND (l_shipdate < '1995-10-01') [type=bool, outer=(11), constraints=(/11: [/'1995-09-01' - /'1995-09-30']; tight)]

# We cannot determine the number of distinct values exactly since the upper
# bound of the date range is compared to a timestamp rather than a date.
opt
SELECT *
FROM lineitem
WHERE
    l_shipdate >= DATE '1995-09-01'
    AND l_shipdate::timestamptz < DATE '1995-10-01';
----
index-join lineitem
 ├── columns: l_orderkey:1(int!null) l_partkey:2(int!null) l_suppkey:3(int!null) l_linenumber:4(int!null) l_quantity:5(float!null) l_extendedprice:6(float!null) l_discount:7(float!null) l_tax:8(float!null) l_returnflag:9(char!null) l_linestatus:10(char!null) l_shipdate:11(date!null) l_commitdate:12(date!null) l_receiptdate:13(date!null) l_shipinstruct:14(char!null) l_shipmode:15(char!null) l_comment:16(varchar!null)
 ├── stats: [rows=111.111111, distinct(1)=69.2053852, null(1)=0, distinct(2)=69.2053852, null(2)=0, distinct(3)=69.2053852, null(3)=0, distinct(4)=69.2053852, null(4)=0, distinct(5)=69.2053852, null(5)=0, distinct(6)=69.2053852, null(6)=0, distinct(7)=69.2053852, null(7)=0, distinct(8)=69.2053852, null(8)=0, distinct(9)=69.2053852, null(9)=0, distinct(10)=69.2053852, null(10)=0, distinct(11)=33.3333333, null(11)=0, distinct(12)=69.2053852, null(12)=0, distinct(13)=69.2053852, null(13)=0, distinct(14)=69.2053852, null(14)=0, distinct(15)=69.2053852, null(15)=0, distinct(16)=69.2053852, null(16)=0]
 ├── key: (1,4)
 ├── fd: (1,4)-->(2,3,5-16)
 └── select
      ├── columns: l_orderkey:1(int!null) l_linenumber:4(int!null) l_shipdate:11(date!null)
      ├── stats: [rows=111.111111, distinct(1)=73.4303342, null(1)=0, distinct(4)=73.4303342, null(4)=0, distinct(11)=32.7552823, null(11)=0]
      ├── key: (1,4)
      ├── fd: (1,4)-->(11)
      ├── scan lineitem@l_sd
      │    ├── columns: l_orderkey:1(int!null) l_linenumber:4(int!null) l_shipdate:11(date!null)
      │    ├── constraint: /11/1/4: [/'1995-09-01' - ]
      │    ├── stats: [rows=333.333333, distinct(1)=98.265847, null(1)=0, distinct(4)=98.265847, null(4)=0, distinct(11)=33.3333333, null(11)=0]
      │    ├── key: (1,4)
      │    └── fd: (1,4)-->(11)
      └── filters
           └── l_shipdate::TIMESTAMPTZ < '1995-10-01' [type=bool, outer=(11)]

# These queries should generate zigzag joins in xform rules. The column statistics
# should be comparable between the norm'd and fully optimized expressions.
opt colstat=11 colstat=12 colstat=(11,12)
SELECT l_shipdate, l_commitdate, l_orderkey, l_linenumber
FROM lineitem
WHERE
    l_shipdate = DATE '1995-09-01'
    AND l_commitdate = DATE '1995-08-01';
----
inner-join (zigzag lineitem@l_sd lineitem@l_cd)
 ├── columns: l_shipdate:11(date!null) l_commitdate:12(date!null) l_orderkey:1(int!null) l_linenumber:4(int!null)
 ├── eq columns: [1 4] = [1 4]
 ├── left fixed columns: [11] = ['1995-09-01']
 ├── right fixed columns: [12] = ['1995-08-01']
 ├── stats: [rows=0.1, distinct(1)=0.099955012, null(1)=0, distinct(4)=0.099955012, null(4)=0, distinct(11)=0.1, null(11)=0, distinct(12)=0.1, null(12)=0, distinct(11,12)=0.1, null(11,12)=0]
 ├── key: (1,4)
 ├── fd: ()-->(11,12)
 └── filters
      ├── l_shipdate = '1995-09-01' [type=bool, outer=(11), constraints=(/11: [/'1995-09-01' - /'1995-09-01']; tight), fd=()-->(11)]
      └── l_commitdate = '1995-08-01' [type=bool, outer=(12), constraints=(/12: [/'1995-08-01' - /'1995-08-01']; tight), fd=()-->(12)]

norm colstat=11 colstat=12 colstat=(11,12)
SELECT l_shipdate, l_commitdate, l_orderkey, l_linenumber
FROM lineitem
WHERE
    l_shipdate = DATE '1995-09-01'
    AND l_commitdate = DATE '1995-08-01';
----
select
 ├── columns: l_shipdate:11(date!null) l_commitdate:12(date!null) l_orderkey:1(int!null) l_linenumber:4(int!null)
 ├── stats: [rows=0.1, distinct(1)=0.099955012, null(1)=0, distinct(4)=0.099955012, null(4)=0, distinct(11)=0.1, null(11)=0, distinct(12)=0.1, null(12)=0, distinct(11,12)=0.1, null(11,12)=0]
 ├── key: (1,4)
 ├── fd: ()-->(11,12)
 ├── scan lineitem
 │    ├── columns: l_orderkey:1(int!null) l_linenumber:4(int!null) l_shipdate:11(date!null) l_commitdate:12(date!null)
 │    ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(4)=100, null(4)=0, distinct(11)=100, null(11)=0, distinct(12)=100, null(12)=0, distinct(11,12)=1000, null(11,12)=0]
 │    ├── key: (1,4)
 │    └── fd: (1,4)-->(11,12)
 └── filters
      ├── l_shipdate = '1995-09-01' [type=bool, outer=(11), constraints=(/11: [/'1995-09-01' - /'1995-09-01']; tight), fd=()-->(11)]
      └── l_commitdate = '1995-08-01' [type=bool, outer=(12), constraints=(/12: [/'1995-08-01' - /'1995-08-01']; tight), fd=()-->(12)]

# These queries should also generate zigzag joins in xform rules, like the
# ones above. These zigzag joins should be nested inside a lookup join on
# the primary index. Since the zigzag join lies in a new memo group, we will
# see the zigzag-join-specific stats/logprops build and colStat functions in
# action. Again, the colstats of the inner zigzag expression should be
# reasonably close to those of the full normalized select expression.
opt colstat=11 colstat=12 colstat=(11,12)
SELECT l_shipdate, l_commitdate, l_orderkey, l_linenumber, l_quantity
FROM lineitem
WHERE
    l_shipdate = DATE '1995-09-01'
    AND l_commitdate = DATE '1995-08-01';
----
inner-join (lookup lineitem)
 ├── columns: l_shipdate:11(date!null) l_commitdate:12(date!null) l_orderkey:1(int!null) l_linenumber:4(int!null) l_quantity:5(float!null)
 ├── key columns: [1 4] = [1 4]
 ├── stats: [rows=0.1, distinct(1)=0.099955012, null(1)=0, distinct(4)=0.099955012, null(4)=0, distinct(5)=0.099955012, null(5)=0, distinct(11)=0.1, null(11)=0, distinct(12)=0.1, null(12)=0, distinct(11,12)=0.1, null(11,12)=0]
 ├── key: (1,4)
 ├── fd: ()-->(11,12), (1,4)-->(5)
 ├── inner-join (zigzag lineitem@l_sd lineitem@l_cd)
 │    ├── columns: l_orderkey:1(int!null) l_linenumber:4(int!null) l_shipdate:11(date!null) l_commitdate:12(date!null)
 │    ├── eq columns: [1 4] = [1 4]
 │    ├── left fixed columns: [11] = ['1995-09-01']
 │    ├── right fixed columns: [12] = ['1995-08-01']
 │    ├── stats: [rows=0.1, distinct(1)=0.099955012, null(1)=0, distinct(4)=0.099955012, null(4)=0, distinct(11)=0.1, null(11)=0, distinct(12)=0.1, null(12)=0]
 │    ├── fd: ()-->(11,12)
 │    └── filters
 │         ├── l_shipdate = '1995-09-01' [type=bool, outer=(11), constraints=(/11: [/'1995-09-01' - /'1995-09-01']; tight), fd=()-->(11)]
 │         └── l_commitdate = '1995-08-01' [type=bool, outer=(12), constraints=(/12: [/'1995-08-01' - /'1995-08-01']; tight), fd=()-->(12)]
 └── filters (true)

norm colstat=11 colstat=12 colstat=(11,12)
SELECT l_shipdate, l_commitdate, l_orderkey, l_linenumber, l_quantity
FROM lineitem
WHERE
    l_shipdate = DATE '1995-09-01'
    AND l_commitdate = DATE '1995-08-01';
----
select
 ├── columns: l_shipdate:11(date!null) l_commitdate:12(date!null) l_orderkey:1(int!null) l_linenumber:4(int!null) l_quantity:5(float!null)
 ├── stats: [rows=0.1, distinct(1)=0.099955012, null(1)=0, distinct(4)=0.099955012, null(4)=0, distinct(5)=0.099955012, null(5)=0, distinct(11)=0.1, null(11)=0, distinct(12)=0.1, null(12)=0, distinct(11,12)=0.1, null(11,12)=0]
 ├── key: (1,4)
 ├── fd: ()-->(11,12), (1,4)-->(5)
 ├── scan lineitem
 │    ├── columns: l_orderkey:1(int!null) l_linenumber:4(int!null) l_quantity:5(float!null) l_shipdate:11(date!null) l_commitdate:12(date!null)
 │    ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(4)=100, null(4)=0, distinct(5)=100, null(5)=0, distinct(11)=100, null(11)=0, distinct(12)=100, null(12)=0, distinct(11,12)=1000, null(11,12)=0]
 │    ├── key: (1,4)
 │    └── fd: (1,4)-->(5,11,12)
 └── filters
      ├── l_shipdate = '1995-09-01' [type=bool, outer=(11), constraints=(/11: [/'1995-09-01' - /'1995-09-01']; tight), fd=()-->(11)]
      └── l_commitdate = '1995-08-01' [type=bool, outer=(12), constraints=(/12: [/'1995-08-01' - /'1995-08-01']; tight), fd=()-->(12)]

# Create a table with an inverted index to test statistics around
# JSON containment filter operators and zigzag joins.
exec-ddl
CREATE TABLE tjson (a INT PRIMARY KEY, b JSON, c JSON, INVERTED INDEX b_idx (b))
----

exec-ddl
ALTER TABLE tjson INJECT STATISTICS '[
  {
    "columns": ["a"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 5000,
    "distinct_count": 5000
  },
  {
    "columns": ["b"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 5000,
    "distinct_count": 2500
  },
  {
    "columns": ["c"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 5000,
    "distinct_count": 2500
  }
]'
----

# Should generate a scan on the inverted index.
opt
SELECT * FROM tjson WHERE b @> '{"a":"b"}'
----
index-join tjson
 ├── columns: a:1(int!null) b:2(jsonb) c:3(jsonb)
 ├── stats: [rows=555.555556, distinct(1)=555.555556, null(1)=0]
 ├── key: (1)
 ├── fd: (1)-->(2,3)
 └── scan tjson@b_idx
      ├── columns: a:1(int!null)
      ├── constraint: /2/1: [/'{"a": "b"}' - /'{"a": "b"}']
      ├── stats: [rows=555.555556, distinct(1)=555.555556, null(1)=0]
      └── key: (1)

# Should generate a zigzag join on the inverted index. Row count should be
# strictly lower than the above scan.
opt
SELECT * FROM tjson WHERE b @> '{"a":"b", "c":"d"}'
----
inner-join (lookup tjson)
 ├── columns: a:1(int!null) b:2(jsonb) c:3(jsonb)
 ├── key columns: [1] = [1]
 ├── stats: [rows=61.7283951, distinct(1)=61.7283951, null(1)=0]
 ├── key: (1)
 ├── fd: (1)-->(2,3)
 ├── inner-join (zigzag tjson@b_idx tjson@b_idx)
 │    ├── columns: a:1(int!null)
 │    ├── eq columns: [1] = [1]
 │    ├── left fixed columns: [2] = ['{"a": "b"}']
 │    ├── right fixed columns: [2] = ['{"c": "d"}']
 │    ├── stats: [rows=61.7283951, distinct(1)=61.7283951, null(1)=0]
 │    └── filters (true)
 └── filters
      └── b @> '{"a": "b", "c": "d"}' [type=bool, outer=(2)]

# Should generate a select on the table with a JSON filter, since c does not
# have an inverted index.
opt
SELECT * FROM tjson WHERE c @> '{"a":"b"}'
----
select
 ├── columns: a:1(int!null) b:2(jsonb) c:3(jsonb)
 ├── stats: [rows=555.555556, distinct(1)=555.555556, null(1)=0]
 ├── key: (1)
 ├── fd: (1)-->(2,3)
 ├── scan tjson
 │    ├── columns: a:1(int!null) b:2(jsonb) c:3(jsonb)
 │    ├── stats: [rows=5000, distinct(1)=5000, null(1)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2,3)
 └── filters
      └── c @> '{"a": "b"}' [type=bool, outer=(3)]

# Should have a lower row count than the above case, due to a containment query
# on 2 json paths.
opt
SELECT * FROM tjson WHERE c @> '{"a":"b", "c":"d"}'
----
select
 ├── columns: a:1(int!null) b:2(jsonb) c:3(jsonb)
 ├── stats: [rows=61.7283951, distinct(1)=61.7283951, null(1)=0]
 ├── key: (1)
 ├── fd: (1)-->(2,3)
 ├── scan tjson
 │    ├── columns: a:1(int!null) b:2(jsonb) c:3(jsonb)
 │    ├── stats: [rows=5000, distinct(1)=5000, null(1)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2,3)
 └── filters
      └── c @> '{"a": "b", "c": "d"}' [type=bool, outer=(3)]

# Bump up null counts.
exec-ddl
ALTER TABLE a INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 5000,
    "distinct_count": 5000
  },
  {
    "columns": ["y"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 4000,
    "distinct_count": 400,
    "null_count": 1000
  }
]'
----

exec-ddl
ALTER TABLE b INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 5000,
    "null_count": 2000
  },
  {
    "columns": ["z"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 100
  },
  {
    "columns": ["rowid"],
    "created_at": "2018-01-01 1:30:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 10000
  }
]'
----

exec-ddl
ALTER TABLE c INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 5000,
    "null_count": 5000
  },
  {
    "columns": ["z"],
    "created_at": "2018-01-01 2:00:00.00000+00:00",
    "row_count": 10000,
    "distinct_count": 10000
  }
]'
----

# Distinct values calculation with constraints.
norm
SELECT * FROM b WHERE x = 1 AND z = 2 AND rowid >= 5 AND rowid <= 8
----
project
 ├── columns: x:1(int!null) z:2(int!null)
 ├── stats: [rows=6.4e-06]
 ├── fd: ()-->(1,2)
 └── select
      ├── columns: x:1(int!null) z:2(int!null) rowid:3(int!null)
      ├── stats: [rows=6.4e-06, distinct(1)=6.4e-06, null(1)=0, distinct(2)=6.4e-06, null(2)=0, distinct(3)=6.4e-06, null(3)=0]
      ├── key: (3)
      ├── fd: ()-->(1,2)
      ├── scan b
      │    ├── columns: x:1(int) z:2(int!null) rowid:3(int!null)
      │    ├── stats: [rows=10000, distinct(1)=5000, null(1)=2000, distinct(2)=100, null(2)=0, distinct(3)=10000, null(3)=0]
      │    ├── key: (3)
      │    └── fd: (3)-->(1,2)
      └── filters
           ├── (rowid >= 5) AND (rowid <= 8) [type=bool, outer=(3), constraints=(/3: [/5 - /8]; tight)]
           ├── x = 1 [type=bool, outer=(1), constraints=(/1: [/1 - /1]; tight), fd=()-->(1)]
           └── z = 2 [type=bool, outer=(2), constraints=(/2: [/2 - /2]; tight), fd=()-->(2)]

# Can't determine stats from filter.
norm
SELECT * FROM a WHERE x + y < 10
----
select
 ├── columns: x:1(int!null) y:2(int)
 ├── stats: [rows=1666.66667, distinct(1)=1666.66667, null(1)=0]
 ├── key: (1)
 ├── fd: (1)-->(2)
 ├── scan a
 │    ├── columns: x:1(int!null) y:2(int)
 │    ├── stats: [rows=5000, distinct(1)=5000, null(1)=0]
 │    ├── key: (1)
 │    └── fd: (1)-->(2)
 └── filters
      └── (x + y) < 10 [type=bool, outer=(1,2)]

# Remaining filter.
norm
SELECT * FROM a WHERE y = 5 AND x + y < 10
----
select
 ├── columns: x:1(int!null) y:2(int!null)
 ├── stats: [rows=3.33333333, distinct(1)=3.33333333, null(1)=0, distinct(2)=1, null(2)=0]
 ├── key: (1)
 ├── fd: ()-->(2)
 ├── scan a
 │    ├── columns: x:1(int!null) y:2(int)
 │    ├── stats: [rows=5000, distinct(1)=5000, null(1)=0, distinct(2)=400, null(2)=1000]
 │    ├── key: (1)
 │    └── fd: (1)-->(2)
 └── filters
      ├── y = 5 [type=bool, outer=(2), constraints=(/2: [/5 - /5]; tight), fd=()-->(2)]
      └── (x + y) < 10 [type=bool, outer=(1,2)]

# Test that the null count for x is propagated correctly (since it's a weak
# key).
norm
SELECT * FROM c WHERE x >= 0 AND x < 100
----
select
 ├── columns: x:1(int!null) z:2(int!null)
 ├── stats: [rows=100, distinct(1)=100, null(1)=0, distinct(2)=100, null(2)=0]
 ├── key: (1)
 ├── fd: (1)-->(2)
 ├── scan c
 │    ├── columns: x:1(int) z:2(int!null)
 │    ├── stats: [rows=10000, distinct(1)=5000, null(1)=5000, distinct(2)=10000, null(2)=0]
 │    ├── lax-key: (1,2)
 │    └── fd: (1)~~>(2)
 └── filters
      └── (x >= 0) AND (x < 100) [type=bool, outer=(1), constraints=(/1: [/0 - /99]; tight)]

# Bump up null counts
exec-ddl
ALTER TABLE customers INJECT STATISTICS '[
{
  "columns": ["name"],
  "created_at": "2018-01-01 1:00:00.00000+00:00",
  "row_count": 10000,
  "distinct_count": 500,
  "null_count": 2000
},
{
  "columns": ["id"],
  "created_at": "2018-01-01 1:30:00.00000+00:00",
  "row_count": 10000,
  "distinct_count": 10000
}
]'
----

# This tests selectivityFromReducedCols
# The following two tests cases are paired together. The first has
# one constraint, one on single non-key column. The second  query has two
# constraints on columns which form a determinant, dependent FD pair.
# The dependent column in this FD pair is from the first test case.
# This series of tests demonstrates that the selectivity
# contribution for a pair of (determinant, dependent) FDs is the
# selectivity of the determinant.
# 1/2 join-subquery-selectivityFromReducedCols tests

build
SELECT * FROM (SELECT * FROM customers, order_history WHERE id = customer_id)
WHERE name='andy'
----
select
 ├── columns: id:1(int!null) name:2(string!null) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 ├── stats: [rows=1.85039927, distinct(1)=1.85039927, null(1)=0, distinct(2)=1, null(2)=0, distinct(6)=1.85039927, null(6)=0]
 ├── fd: ()-->(2), (1)-->(3), (1)==(6), (6)==(1)
 ├── project
 │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 │    ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(2)=432.339125, null(2)=200, distinct(6)=100, null(6)=0]
 │    ├── fd: (1)-->(2,3), (1)==(6), (6)==(1)
 │    └── select
 │         ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int) rowid:8(int!null)
 │         ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(2)=432.339125, null(2)=200, distinct(6)=100, null(6)=0, distinct(8)=632.138954, null(8)=0]
 │         ├── key: (8)
 │         ├── fd: (1)-->(2,3), (8)-->(4-7), (1)==(6), (6)==(1)
 │         ├── inner-join (hash)
 │         │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    ├── stats: [rows=10000000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=2000000, distinct(6)=100, null(6)=100000, distinct(8)=1000, null(8)=0]
 │         │    ├── key: (1,8)
 │         │    ├── fd: (1)-->(2,3), (8)-->(4-7)
 │         │    ├── scan customers
 │         │    │    ├── columns: id:1(int!null) name:2(string) state:3(string)
 │         │    │    ├── stats: [rows=10000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=2000]
 │         │    │    ├── key: (1)
 │         │    │    └── fd: (1)-->(2,3)
 │         │    ├── scan order_history
 │         │    │    ├── columns: order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    │    ├── stats: [rows=1000, distinct(6)=100, null(6)=10, distinct(8)=1000, null(8)=0]
 │         │    │    ├── key: (8)
 │         │    │    └── fd: (8)-->(4-7)
 │         │    └── filters (true)
 │         └── filters
 │              └── id = customer_id [type=bool, outer=(1,6), constraints=(/1: (/NULL - ]; /6: (/NULL - ]), fd=(1)==(6), (6)==(1)]
 └── filters
      └── name = 'andy' [type=bool, outer=(2), constraints=(/2: [/'andy' - /'andy']; tight), fd=()-->(2)]

# This tests selectivityFromReducedCols
# The previous tests case and the following are paired together. The first has
# one constraint, one on single non-key column. The second  query has two
# constraints on columns which form a determinant, dependent FD pair.
# The dependent column in this FD pair is from the first test case.
# This series of tests demonstrates that the selectivity
# contribution for a pair of (determinant, dependent) FDs is the
# selectivity of the determinant.
# 2/2 join-subquery-selectivityFromReducedCols tests

build
SELECT * FROM (SELECT * FROM customers, order_history WHERE id = customer_id)
WHERE id = 1 AND name='andy'
----
select
 ├── columns: id:1(int!null) name:2(string!null) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 ├── stats: [rows=8, distinct(1)=1, null(1)=0, distinct(2)=1, null(2)=0, distinct(6)=8, null(6)=0]
 ├── fd: ()-->(1-3,6), (1)==(6), (6)==(1)
 ├── project
 │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int)
 │    ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(2)=432.339125, null(2)=200, distinct(6)=100, null(6)=0]
 │    ├── fd: (1)-->(2,3), (1)==(6), (6)==(1)
 │    └── select
 │         ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int!null) year:7(int) rowid:8(int!null)
 │         ├── stats: [rows=1000, distinct(1)=100, null(1)=0, distinct(2)=432.339125, null(2)=200, distinct(6)=100, null(6)=0, distinct(8)=632.138954, null(8)=0]
 │         ├── key: (8)
 │         ├── fd: (1)-->(2,3), (8)-->(4-7), (1)==(6), (6)==(1)
 │         ├── inner-join (hash)
 │         │    ├── columns: id:1(int!null) name:2(string) state:3(string) order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    ├── stats: [rows=10000000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=2000000, distinct(6)=100, null(6)=100000, distinct(8)=1000, null(8)=0]
 │         │    ├── key: (1,8)
 │         │    ├── fd: (1)-->(2,3), (8)-->(4-7)
 │         │    ├── scan customers
 │         │    │    ├── columns: id:1(int!null) name:2(string) state:3(string)
 │         │    │    ├── stats: [rows=10000, distinct(1)=10000, null(1)=0, distinct(2)=500, null(2)=2000]
 │         │    │    ├── key: (1)
 │         │    │    └── fd: (1)-->(2,3)
 │         │    ├── scan order_history
 │         │    │    ├── columns: order_id:4(int) item_id:5(int) customer_id:6(int) year:7(int) rowid:8(int!null)
 │         │    │    ├── stats: [rows=1000, distinct(6)=100, null(6)=10, distinct(8)=1000, null(8)=0]
 │         │    │    ├── key: (8)
 │         │    │    └── fd: (8)-->(4-7)
 │         │    └── filters (true)
 │         └── filters
 │              └── id = customer_id [type=bool, outer=(1,6), constraints=(/1: (/NULL - ]; /6: (/NULL - ]), fd=(1)==(6), (6)==(1)]
 └── filters
      └── (id = 1) AND (name = 'andy') [type=bool, outer=(1,2), constraints=(/1: [/1 - /1]; /2: [/'andy' - /'andy']; tight), fd=()-->(1,2)]

exec-ddl
CREATE TABLE nulls (x INT, y INT);
----

exec-ddl
ALTER TABLE nulls INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 1000,
    "distinct_count": 0,
    "null_count": 1000
  }
]'
----

# Regression test for #34440. Ensure the null count for x is less than or equal
# to the row count.
build colstat=1
SELECT * FROM nulls WHERE y = 3
----
project
 ├── columns: x:1(int) y:2(int!null)
 ├── stats: [rows=9.9, distinct(1)=0.99995224, null(1)=9.9]
 ├── fd: ()-->(2)
 └── select
      ├── columns: x:1(int) y:2(int!null) rowid:3(int!null)
      ├── stats: [rows=9.9, distinct(1)=0.99995224, null(1)=9.9, distinct(2)=1, null(2)=0, distinct(3)=9.9, null(3)=0]
      ├── key: (3)
      ├── fd: ()-->(2), (3)-->(1)
      ├── scan nulls
      │    ├── columns: x:1(int) y:2(int) rowid:3(int!null)
      │    ├── stats: [rows=1000, distinct(1)=1, null(1)=1000, distinct(2)=100, null(2)=10, distinct(3)=1000, null(3)=0]
      │    ├── key: (3)
      │    └── fd: (3)-->(1,2)
      └── filters
           └── y = 3 [type=bool, outer=(2), constraints=(/2: [/3 - /3]; tight), fd=()-->(2)]

# Sample testcase using exprgen.
expr colstat=1 colstat=2
(Select 
  (FakeRel
    [
      (OutputCols [ (NewColumn "a" "int") (NewColumn "b" "int") (NewColumn "c" "int")] )
      (Cardinality "-")
      (Stats `[
        {
          "columns": ["a"],
          "distinct_count": 100,
          "null_count": 0, 
          "row_count": 100, 
          "created_at": "2018-01-01 1:00:00.00000+00:00"
        },
        {
          "columns": ["b"],
          "distinct_count": 20,
          "null_count": 5, 
          "row_count": 100, 
          "created_at": "2018-01-01 1:00:00.00000+00:00"
        }
      ]`)
    ]
  )
  [ (Eq (Var "b") (Const 1)) ]
)
----
select
 ├── columns: a:1(int) b:2(int!null) c:3(int)
 ├── stats: [rows=4.75, distinct(1)=4.75, null(1)=0, distinct(2)=1, null(2)=0]
 ├── fd: ()-->(2)
 ├── fake-rel
 │    ├── columns: a:1(int) b:2(int) c:3(int)
 │    └── stats: [rows=100, distinct(1)=100, null(1)=0, distinct(2)=20, null(2)=5]
 └── filters
      └── b = 1 [type=bool, outer=(2), constraints=(/2: [/1 - /1]; tight), fd=()-->(2)]

# Regression test for #37754.
norm
SELECT
    1
FROM
    (
        VALUES
            (true, NULL, B'001000101101110'),
            (true, e'19\x1e':::STRING, NULL)
    )
        AS t (_bool, _string, _bit)
GROUP BY
    _string, _bit
HAVING
    min(_bool)
----
project
 ├── columns: "?column?":5(int!null)
 ├── cardinality: [0 - 2]
 ├── stats: [rows=1e-10]
 ├── fd: ()-->(5)
 ├── select
 │    ├── columns: column2:2(string) column3:3(varbit) min:4(bool!null)
 │    ├── cardinality: [0 - 2]
 │    ├── stats: [rows=1e-10, distinct(4)=1e-10, null(4)=0]
 │    ├── key: (2,3)
 │    ├── fd: ()-->(4)
 │    ├── group-by
 │    │    ├── columns: column2:2(string) column3:3(varbit) min:4(bool)
 │    │    ├── grouping columns: column2:2(string) column3:3(varbit)
 │    │    ├── cardinality: [1 - 2]
 │    │    ├── stats: [rows=2, distinct(4)=2, null(4)=2, distinct(2,3)=2, null(2,3)=2]
 │    │    ├── key: (2,3)
 │    │    ├── fd: (2,3)-->(4)
 │    │    ├── values
 │    │    │    ├── columns: column1:1(bool!null) column2:2(string) column3:3(varbit)
 │    │    │    ├── cardinality: [2 - 2]
 │    │    │    ├── stats: [rows=2, distinct(2,3)=2, null(2,3)=2]
 │    │    │    ├── (true, NULL, B'001000101101110') [type=tuple{bool, string, varbit}]
 │    │    │    └── (true, e'19\x1e', NULL) [type=tuple{bool, string, varbit}]
 │    │    └── aggregations
 │    │         └── min [type=bool, outer=(1)]
 │    │              └── variable: column1 [type=bool]
 │    └── filters
 │         └── variable: min [type=bool, outer=(4), constraints=(/4: [/true - /true]; tight), fd=()-->(4)]
 └── projections
      └── const: 1 [type=int]

# Test that distinct count estimates are correct for date ranges.
exec-ddl
CREATE TABLE date_test (
    k INT PRIMARY KEY,
    d1 date NOT NULL,
    d2 date NOT NULL,
    d3 date NOT NULL,
    INDEX d1_idx (d1 ASC),
    INDEX d2_idx (d2 DESC)
)
----

opt
SELECT d1 FROM date_test WHERE d1 > DATE '1995-10-01' AND d1 < DATE '1995-11-01'
----
scan date_test@d1_idx
 ├── columns: d1:2(date!null)
 ├── constraint: /2/1: [/'1995-10-02' - /'1995-10-31']
 └── stats: [rows=300, distinct(2)=30, null(2)=0]

opt
SELECT d1 FROM date_test WHERE d1 >= DATE '1995-10-01' AND d1 <= DATE '1995-11-01'
----
scan date_test@d1_idx
 ├── columns: d1:2(date!null)
 ├── constraint: /2/1: [/'1995-10-01' - /'1995-11-01']
 └── stats: [rows=320, distinct(2)=32, null(2)=0]

opt
SELECT d2 FROM date_test WHERE d2 > DATE '1903-10-01' AND d2 <= DATE '1903-11-01'
----
scan date_test@d2_idx
 ├── columns: d2:3(date!null)
 ├── constraint: /-3/1: [/'1903-11-01' - /'1903-10-02']
 └── stats: [rows=310, distinct(3)=31, null(3)=0]

opt
SELECT d2 FROM date_test WHERE d2 >= DATE '2003-10-01' AND d2 < DATE '2003-11-01'
----
scan date_test@d2_idx
 ├── columns: d2:3(date!null)
 ├── constraint: /-3/1: [/'2003-10-31' - /'2003-10-01']
 └── stats: [rows=310, distinct(3)=31, null(3)=0]

opt
SELECT d3 FROM date_test WHERE d3 >= DATE '2003-10-01' AND d3 < DATE '2003-11-01'
----
select
 ├── columns: d3:4(date!null)
 ├── stats: [rows=310, distinct(4)=31, null(4)=0]
 ├── scan date_test
 │    ├── columns: d3:4(date!null)
 │    └── stats: [rows=1000, distinct(4)=100, null(4)=0]
 └── filters
      └── (d3 >= '2003-10-01') AND (d3 < '2003-11-01') [type=bool, outer=(4), constraints=(/4: [/'2003-10-01' - /'2003-10-31']; tight)]

opt
SELECT d3 FROM date_test WHERE d3 >= DATE '1903-10-01' AND d3 < DATE '2003-10-01'
----
select
 ├── columns: d3:4(date!null)
 ├── stats: [rows=1000, distinct(4)=100, null(4)=0]
 ├── scan date_test
 │    ├── columns: d3:4(date!null)
 │    └── stats: [rows=1000, distinct(4)=100, null(4)=0]
 └── filters
      └── (d3 >= '1903-10-01') AND (d3 < '2003-10-01') [type=bool, outer=(4), constraints=(/4: [/'1903-10-01' - /'2003-09-30']; tight)]

# Regression test for #38344. Avoid floating point precision errors.
exec-ddl
CREATE TABLE t38344 (x BOOL)
----

exec-ddl
ALTER TABLE t38344 INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 20000000000,
    "distinct_count": 1,
    "null_count": 20000000000
  }
]'
----

norm
WITH t(x) AS (
  SELECT (t1.x::int << 5533)::bool OR t2.x  AS x
  FROM t38344 AS t1 LEFT JOIN t38344 AS t2 ON true
)
SELECT x FROM t WHERE x
----
with &1 (t)
 ├── columns: x:6(bool!null)
 ├── stats: [rows=1, distinct(6)=1, null(6)=0]
 ├── fd: ()-->(6)
 ├── project
 │    ├── columns: x:5(bool)
 │    ├── stats: [rows=4e+20]
 │    ├── left-join (hash)
 │    │    ├── columns: t1.x:1(bool) t2.x:3(bool)
 │    │    ├── stats: [rows=4e+20]
 │    │    ├── scan t1
 │    │    │    ├── columns: t1.x:1(bool)
 │    │    │    └── stats: [rows=2e+10]
 │    │    ├── scan t2
 │    │    │    ├── columns: t2.x:3(bool)
 │    │    │    └── stats: [rows=2e+10]
 │    │    └── filters (true)
 │    └── projections
 │         └── (t1.x::INT8 << 5533)::BOOL OR t2.x [type=bool, outer=(1,3)]
 └── select
      ├── columns: x:6(bool!null)
      ├── stats: [rows=1, distinct(6)=1, null(6)=0]
      ├── fd: ()-->(6)
      ├── with-scan &1 (t)
      │    ├── columns: x:6(bool)
      │    ├── mapping:
      │    │    └──  x:5(bool) => x:6(bool)
      │    ├── stats: [rows=4e+20]
      │    └── fd: (5)-->(6)
      └── filters
           └── variable: x [type=bool, outer=(6), constraints=(/6: [/true - /true]; tight), fd=()-->(6)]

# Regression test for #38375. Avoid floating point precision errors.
exec-ddl
CREATE TABLE t38375 (x INT, y INT)
----

exec-ddl
ALTER TABLE t38375 INJECT STATISTICS '[
  {
    "columns": ["x"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 20000000000,
    "distinct_count": 20000000000,
    "null_count": 20000000000
  },
  {
    "columns": ["y"],
    "created_at": "2018-01-01 1:00:00.00000+00:00",
    "row_count": 20000000000,
    "distinct_count": 10,
    "null_count": 0
  }
]'
----

opt colstat=2
SELECT * FROM t38375 WHERE x = 1
----
select
 ├── columns: x:1(int!null) y:2(int)
 ├── stats: [rows=1e-10, distinct(1)=1e-10, null(1)=0, distinct(2)=1e-10, null(2)=0]
 ├── fd: ()-->(1)
 ├── scan t38375
 │    ├── columns: x:1(int) y:2(int)
 │    └── stats: [rows=2e+10, distinct(1)=2e+10, null(1)=2e+10, distinct(2)=10, null(2)=0]
 └── filters
      └── x = 1 [type=bool, outer=(1), constraints=(/1: [/1 - /1]; tight), fd=()-->(1)]
