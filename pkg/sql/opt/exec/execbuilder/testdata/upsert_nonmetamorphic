# LogicTest: local !metamorphic

# Regression test for #25726.
# UPSERT over tables with column families, on the fast path, use the
# INSERT logic. This has special casing for column families of 1
# column, and another special casing for column families of 2+
# columns. The special casing is only for families that do not include
# the primary key. So we need a table with 3 families: 1 for the PK, 1
# with just 1 col, and 1 with 2+ cols.
statement ok
CREATE TABLE tu (a INT PRIMARY KEY, b INT, c INT, d INT, FAMILY (a), FAMILY (b), FAMILY (c,d));
  INSERT INTO tu VALUES (1, 2, 3, 4)

statement ok
SET tracing = on,kv,results; UPSERT INTO tu VALUES (1, NULL, NULL, NULL); SET tracing = off

query T
SELECT message FROM [SHOW KV TRACE FOR SESSION]
 WHERE operation != 'dist sender send'
----
Put /Table/53/1/1/0 -> /TUPLE/
Del /Table/53/1/1/1/1
Del /Table/53/1/1/2/1
fast path completed
rows affected: 1

# KV operations.
statement ok
CREATE DATABASE t; CREATE TABLE t.kv(k INT PRIMARY KEY, v INT, FAMILY "primary" (k, v))

statement ok
CREATE UNIQUE INDEX woo ON t.kv(v)

statement ok
SET tracing = on,kv,results; UPSERT INTO t.kv(k, v) VALUES (2,3); SET tracing = off

query TT
SELECT operation, message FROM [SHOW KV TRACE FOR SESSION]
 WHERE operation != 'dist sender send' AND operation != 'kv.DistSender: sending partial batch'
----
colbatchscan            Scan /Table/55/1/2/0
batch flow coordinator  CPut /Table/55/1/2/0 -> /TUPLE/2:2:Int/3
batch flow coordinator  InitPut /Table/55/2/3/0 -> /BYTES/0x8a
batch flow coordinator  fast path completed
exec stmt               rows affected: 1

statement ok
SET tracing = on,kv,results; UPSERT INTO t.kv(k, v) VALUES (1,2); SET tracing = off

query TT
SELECT operation, message FROM [SHOW KV TRACE FOR SESSION]
 WHERE operation != 'dist sender send' AND operation != 'kv.DistSender: sending partial batch'
----
colbatchscan            Scan /Table/55/1/1/0
batch flow coordinator  CPut /Table/55/1/1/0 -> /TUPLE/2:2:Int/2
batch flow coordinator  InitPut /Table/55/2/2/0 -> /BYTES/0x89
batch flow coordinator  fast path completed
exec stmt               rows affected: 1

statement error duplicate key value
SET tracing = on,kv,results; UPSERT INTO t.kv(k, v) VALUES (2,2); SET tracing = off

query TT
set tracing=off;
SELECT operation, message FROM [SHOW KV TRACE FOR SESSION]
 WHERE operation != 'dist sender send' AND operation != 'kv.DistSender: sending partial batch'
----
colbatchscan            Scan /Table/55/1/2/0
colbatchscan            fetched: /kv/kv_pkey/2/v -> /3
batch flow coordinator  Put /Table/55/1/2/0 -> /TUPLE/2:2:Int/2
batch flow coordinator  Del /Table/55/2/3/0
batch flow coordinator  CPut /Table/55/2/2/0 -> /BYTES/0x8a (expecting does not exist)
exec stmt               execution failed after 0 rows: duplicate key value violates unique constraint "woo"


# ---------------------------------------------------------
# Partial Index With Delete Preserving Encoding
# ---------------------------------------------------------
statement ok
CREATE TABLE tpi (
    a INT PRIMARY KEY,
    b INT,
    c STRING,
    FAMILY (a, b, c),
    CHECK (b > 0),
    INDEX partial (c) WHERE a > b AND c IN ('foo', 'foobar')
);

let $t_id
select id from system.namespace where name='tpi'

let $updated_t_jsonb
WITH
  descs
    AS (
      SELECT
        id,
        crdb_internal.pb_to_json(
          'cockroach.sql.sqlbase.Descriptor',
          descriptor
        )
          AS descriptor
      FROM
        system.descriptor
    )
SELECT
  CAST (json_set(descriptor, ARRAY['table', 'indexes', '0', 'useDeletePreservingEncoding'], 'true') AS STRING)
FROM
  descs WHERE id = $t_id;

statement ok
SELECT * FROM crdb_internal.unsafe_upsert_descriptor($t_id, crdb_internal.json_to_pb('cockroach.sql.sqlbase.Descriptor',$$ $updated_t_jsonb $$), true)

statement ok
INSERT INTO tpi VALUES (1, 2, 'bar'), (2, 3, 'bar'), (3, 4, 'foo')

# Upsert a row that doesn't match the partial index.
query T kvtrace
UPSERT INTO tpi VALUES (1, 3, 'bar')
----
Scan /Table/56/1/1/0
Put /Table/56/1/1/0 -> /TUPLE/2:2:Int/3/1:3:Bytes/bar

# Upsert a row that didn't match the partial index before but matches after.
query T kvtrace
UPSERT INTO tpi VALUES (3, 2, 'foo')
----
Scan /Table/56/1/3/0
Put /Table/56/1/3/0 -> /TUPLE/2:2:Int/2/1:3:Bytes/foo
CPut /Table/56/2/"foo"/3/0 -> /BYTES/0x0a01031000 (expecting does not exist)

# Upsert a row that matches the partial index before and after, but the index
# entry doesn't change.
query T kvtrace
UPSERT INTO tpi VALUES (3, 1, 'foo')
----
Scan /Table/56/1/3/0
Put /Table/56/1/3/0 -> /TUPLE/2:2:Int/1/1:3:Bytes/foo

# Upsert a row that matches the partial index before and after, and the index
# entry changes.
query T kvtrace
UPSERT INTO tpi VALUES (3, 2, 'foobar')
----
Scan /Table/56/1/3/0
Put /Table/56/1/3/0 -> /TUPLE/2:2:Int/2/1:3:Bytes/foobar
Put (delete) /Table/56/2/"foo"/3/0
CPut /Table/56/2/"foobar"/3/0 -> /BYTES/0x0a01031000 (expecting does not exist)

# Upsert a row that matches the partial index before but not after.
query T kvtrace
UPSERT INTO tpi VALUES (3, 1, 'baz')
----
Scan /Table/56/1/3/0
Put /Table/56/1/3/0 -> /TUPLE/2:2:Int/1/1:3:Bytes/baz
Put (delete) /Table/56/2/"foobar"/3/0

# ---------------------------------------------------------
# Expression Index With Delete Preserving Encoding
# ---------------------------------------------------------
statement ok
CREATE TABLE tei (
  k INT PRIMARY KEY,
  a INT,
  b INT,
  FAMILY (k, a, b)
)

statement ok
CREATE INDEX t_a_plus_b_idx ON tei ((a + b))

let $t_id
select id from system.namespace where name='tei'

let $updated_t_jsonb
WITH
  descs
    AS (
      SELECT
        id,
        crdb_internal.pb_to_json(
          'cockroach.sql.sqlbase.Descriptor',
          descriptor
        )
          AS descriptor
      FROM
        system.descriptor
    )
SELECT
  CAST (
    json_set(
      json_set(descriptor, ARRAY['table', 'indexes', '0', 'useDeletePreservingEncoding'], 'true'),
      ARRAY['table', 'modificationTime'], json_build_object('wallTime', cluster_logical_timestamp()::INT8::STRING)
    ) AS STRING
  )
FROM
  descs WHERE id = $t_id;

statement ok
SELECT * FROM crdb_internal.unsafe_upsert_descriptor($t_id, crdb_internal.json_to_pb('cockroach.sql.sqlbase.Descriptor',$$ $updated_t_jsonb $$), true)

statement ok
INSERT INTO tei VALUES (1, 2, 100), (2, 3, 200), (3, 4, 300)

# Upsert a row which changes the index entry.
query T kvtrace
UPSERT INTO tei VALUES (1, 3, 500)
----
Scan /Table/57/1/1/0
Put /Table/57/1/1/0 -> /TUPLE/2:2:Int/3/1:3:Int/500
Put (delete) /Table/57/2/102/1/0
CPut /Table/57/2/503/1/0 -> /BYTES/0x0a01031000 (expecting does not exist)

# Upsert a row with different values without changing the index entry.
query T kvtrace
UPSERT INTO tei VALUES (1, 4, 499)
----
Scan /Table/57/1/1/0
Put /Table/57/1/1/0 -> /TUPLE/2:2:Int/4/1:3:Int/499

# Upsert a row with a different primary key with the same index entry.
query T kvtrace
UPSERT INTO tei VALUES (2, 4, 499)
----
Scan /Table/57/1/2/0
Put /Table/57/1/2/0 -> /TUPLE/2:2:Int/4/1:3:Int/499
Put (delete) /Table/57/2/203/2/0
CPut /Table/57/2/503/2/0 -> /BYTES/0x0a01031000 (expecting does not exist)

# ---------------------------------------------------------
# Inverted Index With Delete Preserving Encoding
# ---------------------------------------------------------

statement ok
CREATE TABLE tii (
  a INT PRIMARY KEY,
  b INT[],
  FAMILY (a,b),
  INVERTED INDEX(b)
)

let $t_id
select id from system.namespace where name='tii'

let $updated_t_jsonb
WITH
  descs
    AS (
      SELECT
        id,
        crdb_internal.pb_to_json(
          'cockroach.sql.sqlbase.Descriptor',
          descriptor
        )
          AS descriptor
      FROM
        system.descriptor
    )
SELECT
  CAST (
    json_set(
      json_set(descriptor, ARRAY['table', 'indexes', '0', 'useDeletePreservingEncoding'], 'true'),
      ARRAY['table', 'modificationTime'], json_build_object('wallTime', cluster_logical_timestamp()::INT8::STRING)
    ) AS STRING
  )
FROM
  descs WHERE id = $t_id;

statement ok
SELECT * FROM crdb_internal.unsafe_upsert_descriptor($t_id, crdb_internal.json_to_pb('cockroach.sql.sqlbase.Descriptor',$$ $updated_t_jsonb $$), true)

statement ok
INSERT INTO tii VALUES (1, ARRAY[1, 2, 3, 2, 2, NULL, 3])

# Upsert a row that has 1 new entry and 1 removed entry in the index.
query T kvtrace
UPSERT INTO tii VALUES (1, ARRAY[1, 2, 2, NULL, 4, 4])
----
Scan /Table/58/1/1/0
Put /Table/58/1/1/0 -> /TUPLE/
Put (delete) /Table/58/2/3/1/0
InitPut /Table/58/2/4/1/0 -> /BYTES/0x0a01031000

# ---------------------------------------------------------
# Multicolumn Inverted Index With Delete Preserving Encoding
# ---------------------------------------------------------

statement ok
CREATE TABLE tmi (
  a INT PRIMARY KEY,
  b INT,
  c JSON,
  FAMILY (a, b, c),
  INVERTED INDEX(b, c)
)

let $t_id
select id from system.namespace where name='tmi'

let $updated_t_jsonb
WITH
  descs
    AS (
      SELECT
        id,
        crdb_internal.pb_to_json(
          'cockroach.sql.sqlbase.Descriptor',
          descriptor
        )
          AS descriptor
      FROM
        system.descriptor
    )
SELECT
  CAST (
    json_set(
      json_set(descriptor, ARRAY['table', 'indexes', '0', 'useDeletePreservingEncoding'], 'true'),
      ARRAY['table', 'modificationTime'], json_build_object('wallTime', cluster_logical_timestamp()::INT8::STRING)
    ) AS STRING
  )
FROM
  descs WHERE id = $t_id;

statement ok
SELECT * FROM crdb_internal.unsafe_upsert_descriptor($t_id, crdb_internal.json_to_pb('cockroach.sql.sqlbase.Descriptor',$$ $updated_t_jsonb $$), true)

statement ok
INSERT INTO tmi VALUES (1, 2, '{"a": "foo", "b": "bar"}'::json)

query T kvtrace
UPSERT INTO tmi VALUES (1, 3, '{"a": "foobar", "c": "baz"}'::json)
----
Scan /Table/59/1/1/0
Put /Table/59/1/1/0 -> /TUPLE/2:2:Int/3/
Put (delete) /Table/59/2/2/"a"/"foo"/1/0
Put (delete) /Table/59/2/2/"b"/"bar"/1/0
InitPut /Table/59/2/3/"a"/"foobar"/1/0 -> /BYTES/0x0a01031000
InitPut /Table/59/2/3/"c"/"baz"/1/0 -> /BYTES/0x0a01031000
