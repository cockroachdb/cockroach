// Copyright 2019 The Cockroach Authors.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.
//
// Processor definitions for distributed SQL APIs. See
// docs/RFCS/distributed_sql.md.
// All the concepts here are "physical plan" concepts.

syntax = "proto2";
// Beware! This package name must not be changed, even though it doesn't match
// the Go package name, because it defines the Protobuf message names which
// can't be changed without breaking backward compatibility.
package cockroach.sql.distsqlrun;
option go_package = "execinfrapb";

import "jobs/jobspb/jobs.proto";
import "roachpb/io-formats.proto";
import "sql/catalog/catpb/catalog.proto";
import "sql/catalog/descpb/structured.proto";
import "sql/execinfrapb/processors_base.proto";
import "util/hlc/timestamp.proto";
import "gogoproto/gogo.proto";
import "roachpb/data.proto";
import "roachpb/api.proto";

// BackfillerSpec is the specification for a "schema change backfiller".
// The created backfill processor runs a backfill for the first mutations in
// the table descriptor mutation list with the same mutation id and type.
// A backfiller processor performs KV operations to retrieve rows for a
// table and backfills the new indexes/columns contained in the table
// descriptor. It checkpoints its progress by updating the table
// descriptor in the database, and doesn't emit any rows nor support
// any post-processing.
message BackfillerSpec {
  optional int64 job_id = 13 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];

  enum Type {
    Invalid = 0;
    Column = 1;
    Index = 2;
  }
  optional Type type = 1 [(gogoproto.nullable) = false];
  optional sqlbase.TableDescriptor table = 2 [(gogoproto.nullable) = false];

  // Sections of the table to be backfilled.
  reserved 3;
  repeated roachpb.Span spans = 10 [(gogoproto.nullable) = false];

  // Run the backfill for approximately this duration.
  // The backfill will always process at least one backfill chunk.
  optional int64 duration = 4 [(gogoproto.nullable) = false, (gogoproto.casttype) = "time.Duration"];

  // The backfill involves a complete table scan in chunks,
  // where each chunk is a transactional read of a set of rows
  // along with a backfill for the rows. This is the maximum number
  // of entries backfilled per chunk.
  optional int64 chunk_size = 5 [(gogoproto.nullable) = false];

  // WriteAsOf is the time that the backfill entries should be written.
  // Note: Older nodes may also use this as the read time instead of readAsOf.
  optional util.hlc.Timestamp writeAsOf = 7 [(gogoproto.nullable) = false];
  // The timestamp to perform index backfill historical scans at.
  optional util.hlc.Timestamp readAsOf = 9 [(gogoproto.nullable) = false];

  // IndexesToBackfill is the set of indexes to backfill. This is populated only
  // starting in 21.1, prior to that the implied index set are those containing
  // the mutation ID of the first mutation on the table descriptor.
  repeated uint32 indexes_to_backfill = 8 [(gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/sql/catalog/descpb.IndexID"];

  reserved 6;

  optional int32 initial_splits = 11 [(gogoproto.nullable) = false];

  // WriteAtBatchTimestamp will write the SST MVCC timestamps at the batch
  // timestamp, even if the request gets pushed server-side. This ensures the
  // writes comply with the timestamp cache and closed timestamp. See also
  // AddSSTableRequest.SSTTimestampToRequestTimestamp.
  //
  // Note that older nodes do not respect this flag so callers should
  // check MVCCAddSSTable before setting this option.
  optional bool write_at_batch_timestamp = 12 [(gogoproto.nullable) = false];

  // NEXTID: 14.
}

// JobProgress identifies the job to report progress on. This reporting
// happens outside this package.
message JobProgress {
  optional int64 job_id = 1 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID",
                            (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/jobs/jobspb.JobID"];
  // contribution is the percent of work of the total this processor will
  // process.
  optional float contribution = 2 [(gogoproto.nullable) = false];
  // slot is the index into the job details for this processor's completion.
  optional int32 slot = 3 [(gogoproto.nullable) = false];
}

message ReadImportDataSpec {
  optional int64 job_id = 19 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];
  reserved 1;
  optional roachpb.IOFileFormat format = 8 [(gogoproto.nullable) = false];
  // sample_size is the rate at which to output rows, based on an input row's size.
  optional int32 sample_size = 2 [(gogoproto.nullable) = false];
  reserved 3;

  message ImportTable {
    optional sqlbase.TableDescriptor desc = 1 [(gogoproto.nullable) = true];
    // targetCols is used to store the target columns for each existing table
    // being imported into. These are the columns for which the processor should
    // read and emit data (ignoring data for any other tables or columns outside
    // of the targetCols, that is present in the input).
    repeated string targetCols = 2 [(gogoproto.nullable) = true];
  }

  // tables supports input formats that can read multiple tables. If it is
  // non-empty, the keys specify the names of tables for which the processor
  // should read and emit data (ignoring data for any other tables that is
  // present in the input).
  //
  // TODO(dt): If a key has a nil value, the schema for that table should be
  // determined from the input on-the-fly (e.g. by parsing a CREATE TABLE in a
  // dump file) and the processor should emit a key/value for the generated
  // TableDescriptor with the corresponding descriptor ID key. If tables is
  // empty (and table_desc above is not specified), the processor should read
  // all tables in the input, determining their schemas on the fly.
  map<string, ImportTable> tables = 9 [(gogoproto.nullable) = true];

  // uri is a cloud.ExternalStorage URI pointing to the CSV files to be
  // read. The map key must be unique across the entire IMPORT job.
  map<int32, string> uri = 7;

  // resume_pos specifies a map from an input ID to an offset in that
  // input from which the processing should continue.
  // The meaning of offset is specific to each processor.
  map<int32, int64> resume_pos = 14;

  optional JobProgress progress = 6 [(gogoproto.nullable) = false];

  reserved 4;
  reserved 5;

  optional bool skip_missing_foreign_keys = 10 [(gogoproto.nullable) = false];

  // walltimeNanos is the MVCC time at which the created KVs will be written.
  optional int64 walltimeNanos = 11 [(gogoproto.nullable) = false];

  reserved 12;

  // If set, specifies reader parallelism; 0 implies "use default".
  optional int32 readerParallelism = 13 [(gogoproto.nullable) = false];

  // User who initiated the import. This is used to check access privileges
  // when using FileTable ExternalStorage.
  optional string user_proto = 15 [(gogoproto.nullable) = false, (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/security.SQLUsernameProto"];

  repeated sqlbase.TypeDescriptor types = 16;

  // If the database being imported into is a multi-region database, then this
  // field stores the databases' primary region.
  optional string database_primary_region = 17 [
    (gogoproto.nullable) = false,
    (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/sql/catalog/catpb.RegionName"
  ];

  optional int32 initial_splits = 18 [(gogoproto.nullable) = false];

  // NEXTID: 20.
}

message StreamIngestionDataSpec {
  reserved 1;

  // StreamID is the ID of the stream (which is shared across the producer and consumer).
  optional uint64 stream_id = 5 [(gogoproto.nullable) = false, (gogoproto.customname) = "StreamID"];

  // PartitionSpecs specify how to subscribe to the i'th partition.
  repeated string partition_ids = 6;
  // PartitionSpecs specify how to subscribe to the i'th partition.
  repeated string partition_specs = 7;
  // PartitionAddresses locate the partitions that produce events to be
  // ingested. We don't set the casttype to avoid depending on ccl packages.
  repeated string partition_addresses = 8;

  // The processor will ingest events from StartTime onwards.
  optional util.hlc.Timestamp start_time = 2 [(gogoproto.nullable) = false];
  // StreamAddress locate the stream so that a stream client can be initialized.
  optional string stream_address = 3 [(gogoproto.nullable) = false];
  // JobID is the job ID of the stream ingestion job.
  optional int64 job_id = 4 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];

}

message StreamIngestionFrontierSpec {
  // HighWaterAtStart is set by the ingestion job when initializing the frontier
  // processor. It is used as sanity check by the frontier processor to ensure
  // that it does not receive updates at a timestamp lower than this field. This
  // consequently prevents the job progress from regressing during ingestion.
  optional util.hlc.Timestamp high_water_at_start = 1 [(gogoproto.nullable) = false];
  // TrackedSpans is the entire span set being watched. The spans do not really
  // represent KV spans but uniquely identify the partitions in the ingestion
  // stream. Once all the partitions in the ingestion stream have been resolved
  // at a certain timestamp, then it's safe to resolve the ingestion at that
  // timestamp.
  repeated roachpb.Span tracked_spans = 2 [(gogoproto.nullable) = false];
  // JobID is the job ID of the stream ingestion job.
  optional int64 job_id = 3 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];

  // StreamID is the ID of the stream.
  optional uint64 stream_id = 4 [(gogoproto.nullable) = false, (gogoproto.customname) = "StreamID"];
  // StreamAddress locate the stream so that a stream client can be initialized.
  optional string stream_address = 5 [(gogoproto.nullable) = false];
}

message BackupDataSpec {
  optional int64 job_id = 11 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];
  repeated roachpb.Span spans = 1 [(gogoproto.nullable) = false];
  repeated roachpb.Span introduced_spans = 2 [(gogoproto.nullable) = false];
  optional string default_uri = 3 [(gogoproto.nullable) = false, (gogoproto.customname) = "DefaultURI"];
  map<string, string> uris_by_locality_kv = 4 [(gogoproto.customname) = "URIsByLocalityKV"];
  optional roachpb.MVCCFilter mvcc_filter = 5 [(gogoproto.nullable) = false, (gogoproto.customname) = "MVCCFilter"];
  optional roachpb.FileEncryptionOptions encryption = 6;
  optional util.hlc.Timestamp backup_start_time = 7 [(gogoproto.nullable) = false];
  optional util.hlc.Timestamp backup_end_time = 8 [(gogoproto.nullable) = false];

  // PKIDs is used to convert result from an ExportRequest into row count
  // information passed back to track progress in the backup job.
  map<uint64, bool> pk_ids = 9 [(gogoproto.customname) = "PKIDs"];

  // User who initiated the backup. This is used to check access privileges
  // when using FileTable ExternalStorage.
  optional string user_proto = 10 [(gogoproto.nullable) = false, (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/security.SQLUsernameProto"];

  // NEXTID: 12.
}

message RestoreFileSpec {
  optional roachpb.ExternalStorage dir = 1 [(gogoproto.nullable) = false];
  optional string path = 2 [(gogoproto.nullable) = false];
  reserved 3;
  reserved 4;
}

message TableRekey {
  // OldID is the previous ID of `new_desc`.
  optional uint32 old_id = 1 [(gogoproto.nullable) = false, (gogoproto.customname) = "OldID"];
  // NewDesc is an encoded Descriptor message.
  optional bytes new_desc = 2;
}

message TenantRekey {
  // OldID is a previous tenant ID.
  optional roachpb.TenantID old_id = 1 [(gogoproto.nullable) = false, (gogoproto.customname) = "OldID"];
  // NewID is the ID with which to replace OldID.
  optional roachpb.TenantID new_id = 2 [(gogoproto.nullable) = false, (gogoproto.customname) = "NewID"];
}

// RestoreDataEntry will be specified at planning time to the SplitAndScatter
// processors, then those processors will stream these, encoded as bytes in rows
// to the RestoreDataProcessors.
// This field has a subset of the importEntry struct defined in restore.
message RestoreSpanEntry {
  optional roachpb.Span span = 1 [(gogoproto.nullable) = false];
  repeated RestoreFileSpec files = 2 [(gogoproto.nullable) = false];
  optional int64 progressIdx = 3 [(gogoproto.nullable) = false];
}

message RestoreDataSpec {
  optional int64 job_id = 6 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];
  optional util.hlc.Timestamp restore_time = 1 [(gogoproto.nullable) = false];
  optional roachpb.FileEncryptionOptions encryption = 2;
  repeated TableRekey table_rekeys = 3 [(gogoproto.nullable) = false];
  repeated TenantRekey tenant_rekeys = 5[(gogoproto.nullable) = false];
  // PKIDs is used to convert result from an ExportRequest into row count
  // information passed back to track progress in the backup job.
  map<uint64, bool> pk_ids = 4 [(gogoproto.customname) = "PKIDs"];
  // NEXT ID: 7.
}

message SplitAndScatterSpec {
  optional int64 job_id = 4 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];
  message RestoreEntryChunk {
    repeated RestoreSpanEntry entries = 1 [(gogoproto.nullable) = false];
  }

  repeated RestoreEntryChunk chunks = 1 [(gogoproto.nullable) = false];
  repeated TableRekey table_rekeys = 2 [(gogoproto.nullable) = false];
  repeated TenantRekey tenant_rekeys = 3 [(gogoproto.nullable) = false];

  // NEXTID: 5.
}

// ExporterSpec is the specification for a processor that consumes rows and
// writes them to Parquet or CSV files at uri. It outputs a row per file written with
// the file name, row count and byte size.
message ExportSpec {
  // destination as a cloud.ExternalStorage URI pointing to an export store
  // location (directory).
  optional string destination = 1 [(gogoproto.nullable) = false];
  optional string name_pattern = 2 [(gogoproto.nullable) = false];
  optional roachpb.IOFileFormat format = 3 [(gogoproto.nullable) = false];

  // chunk_rows is num rows to write per file. 0 = no limit.
  optional int64 chunk_rows = 4 [(gogoproto.nullable) = false];
  // chunk_size is the target byte size per file.
  optional int64 chunk_size = 5 [(gogoproto.nullable) = false];

  // User who initiated the export. This is used to check access privileges
  // when using FileTable ExternalStorage.
  optional string user_proto = 6 [(gogoproto.nullable) = false, (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/security.SQLUsernameProto"];

  // col_names specifies the logical column names for the exported parquet file.
  repeated string col_names = 7 ;
}

// BulkRowWriterSpec is the specification for a processor that consumes rows and
// writes them to a target table using AddSSTable. It outputs a BulkOpSummary.
message BulkRowWriterSpec {
  optional sqlbase.TableDescriptor table = 1 [(gogoproto.nullable) = false];
}

message IndexBackfillMergerSpec {
  optional sqlbase.TableDescriptor table = 1 [(gogoproto.nullable) = false];

  repeated uint32 temporary_indexes = 2 [(gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/sql/catalog/descpb.IndexID"];
  repeated uint32 added_indexes = 3 [(gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/sql/catalog/descpb.IndexID"];

  repeated roachpb.Span spans = 4 [(gogoproto.nullable) = false];
  repeated int32 span_idx = 5;

  // NEXT ID: 8.
}
