// Copyright 2019 The Cockroach Authors.
//
// Use of this software is governed by the CockroachDB Software License
// included in the /LICENSE file.
//
// Processor definitions for distributed SQL APIs. See
// docs/RFCS/distributed_sql.md.
// All the concepts here are "physical plan" concepts.

syntax = "proto2";
// Beware! This package name must not be changed, even though it doesn't match
// the Go package name, because it defines the Protobuf message names which
// can't be changed without breaking backward compatibility.
package cockroach.sql.distsqlrun;
option go_package = "github.com/cockroachdb/cockroach/pkg/sql/execinfrapb";

import "jobs/jobspb/jobs.proto";
import "roachpb/io-formats.proto";
import "sql/catalog/descpb/structured.proto";
import "roachpb/metadata.proto";
import "util/hlc/timestamp.proto";
import "gogoproto/gogo.proto";
import "roachpb/data.proto";
import "kv/kvpb/api.proto";
import "cloud/cloudpb/external_storage.proto";

// BackfillerSpec is the specification for a "schema change backfiller".
// The created backfill processor runs a backfill for the first mutations in
// the table descriptor mutation list with the same mutation id and type.
// A backfiller processor performs KV operations to retrieve rows for a
// table and backfills the new indexes/columns contained in the table
// descriptor. It checkpoints its progress by updating the table
// descriptor in the database, and doesn't emit any rows nor support
// any post-processing.
message BackfillerSpec {
  reserved 13; // job_id
  enum Type {
    Invalid = 0;
    Column = 1;
    Index = 2;
  }
  optional Type type = 1 [(gogoproto.nullable) = false];
  optional sqlbase.TableDescriptor table = 2 [(gogoproto.nullable) = false];

  // Sections of the table to be backfilled.
  reserved 3;
  repeated roachpb.Span spans = 10 [(gogoproto.nullable) = false];

  // Run the backfill for approximately this duration.
  // The backfill will always process at least one backfill chunk.
  optional int64 duration = 4 [(gogoproto.nullable) = false, (gogoproto.casttype) = "time.Duration"];

  // The backfill involves a complete table scan in chunks,
  // where each chunk is a transactional read of a set of rows
  // along with a backfill for the rows. This is the maximum number
  // of entries backfilled per chunk.
  optional int64 chunk_size = 5 [(gogoproto.nullable) = false];

  // The column backfiller will run an update batch immediately
  // once its estimated byte size reaches UpdateChunkSizeThresholdBytes, if nonzero.
  optional uint64 update_chunk_size_threshold_bytes = 14 [(gogoproto.nullable) = false];

  // WriteAsOf is the time that the backfill entries should be written.
  // Note: Older nodes may also use this as the read time instead of readAsOf.
  optional util.hlc.Timestamp writeAsOf = 7 [(gogoproto.nullable) = false];
  // The timestamp to perform index backfill historical scans at.
  optional util.hlc.Timestamp readAsOf = 9 [(gogoproto.nullable) = false];

  // IndexesToBackfill is the set of indexes to backfill. This is populated only
  // starting in 21.1, prior to that the implied index set are those containing
  // the mutation ID of the first mutation on the table descriptor.
  repeated uint32 indexes_to_backfill = 8 [(gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/sql/catalog/descpb.IndexID"];

  reserved 6;

  optional int32 initial_splits = 11 [(gogoproto.nullable) = false];

  // WriteAtBatchTimestamp will write the SST MVCC timestamps at the batch
  // timestamp, even if the request gets pushed server-side. This ensures the
  // writes comply with the timestamp cache and closed timestamp. See also
  // AddSSTableRequest.SSTTimestampToRequestTimestamp.
  optional bool write_at_batch_timestamp = 12 [(gogoproto.nullable) = false];

  // NEXTID: 15.
}

// JobProgress identifies the job to report progress on. This reporting
// happens outside this package.
message JobProgress {
  optional int64 job_id = 1 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID",
                            (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/jobs/jobspb.JobID"];
  // contribution is the percent of work of the total this processor will
  // process.
  optional float contribution = 2 [(gogoproto.nullable) = false];
  // slot is the index into the job details for this processor's completion.
  optional int32 slot = 3 [(gogoproto.nullable) = false];
}

message ReadImportDataSpec {
  // TODO(lidor): job_id is not needed when interoperability with 22.2 is
  // dropped, the new way to send the job tag is using 'job_tag' in the
  // SetupFlowRequest message.
  optional int64 job_id = 19 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];
  reserved 1;
  optional roachpb.IOFileFormat format = 8 [(gogoproto.nullable) = false];
  // sample_size is the rate at which to output rows, based on an input row's size.
  optional int32 sample_size = 2 [(gogoproto.nullable) = false];
  reserved 3;

  message ImportTable {
    optional sqlbase.TableDescriptor desc = 1 [(gogoproto.nullable) = true];
    // targetCols is used to store the target columns for each existing table
    // being imported into. These are the columns for which the processor should
    // read and emit data (ignoring data for any other tables or columns outside
    // of the targetCols, that is present in the input).
    repeated string targetCols = 2 [(gogoproto.nullable) = true];
  }

  // tables supports input formats that can read multiple tables. If it is
  // non-empty, the keys specify the names of tables for which the processor
  // should read and emit data (ignoring data for any other tables that is
  // present in the input).
  //
  // TODO(dt): If a key has a nil value, the schema for that table should be
  // determined from the input on-the-fly (e.g. by parsing a CREATE TABLE in a
  // dump file) and the processor should emit a key/value for the generated
  // TableDescriptor with the corresponding descriptor ID key. If tables is
  // empty (and table_desc above is not specified), the processor should read
  // all tables in the input, determining their schemas on the fly.
  map<string, ImportTable> tables = 9 [(gogoproto.nullable) = true];

  // uri is a cloud.ExternalStorage URI pointing to the CSV files to be
  // read. The map key must be unique across the entire IMPORT job.
  map<int32, string> uri = 7;

  // resume_pos specifies a map from an input ID to an offset in that
  // input from which the processing should continue.
  // The meaning of offset is specific to each processor.
  map<int32, int64> resume_pos = 14;

  optional JobProgress progress = 6 [(gogoproto.nullable) = false];

  reserved 4;
  reserved 5;

  optional bool skip_missing_foreign_keys = 10 [(gogoproto.nullable) = false];

  // walltimeNanos is the MVCC time at which the created KVs will be written.
  optional int64 walltimeNanos = 11 [(gogoproto.nullable) = false];

  reserved 12;

  // If set, specifies reader parallelism; 0 implies "use default".
  optional int32 readerParallelism = 13 [(gogoproto.nullable) = false];

  // User who initiated the import. This is used to check access privileges
  // when using FileTable ExternalStorage.
  optional string user_proto = 15 [(gogoproto.nullable) = false, (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/security/username.SQLUsernameProto"];

  repeated sqlbase.TypeDescriptor types = 16;

  // If the database being imported into is a multi-region database, then this
  // field stores the databases' primary region.
  optional string database_primary_region = 17 [
    (gogoproto.nullable) = false,
    (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/sql/catalog/catpb.RegionName"
  ];

  optional int32 initial_splits = 18 [(gogoproto.nullable) = false];

  // NEXTID: 20.
}

message IngestStoppedSpec {
  optional int64 job_id = 1 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID",
  (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/jobs/jobspb.JobID"];
}

// StreamIngestionPartitionSpec contains information about a partition and how
// to connect to it.
message StreamIngestionPartitionSpec {
  optional string partition_id = 1 [(gogoproto.nullable) = false, (gogoproto.customname) = "PartitionID"];

  optional string subscription_token = 2 [(gogoproto.nullable) = false];

  optional string partition_conn_uri = 3 [(gogoproto.nullable) = false];

  repeated roachpb.Span spans = 4 [(gogoproto.nullable) = false];

  optional int32 src_instance_id = 5 [
    (gogoproto.nullable) = false,
    (gogoproto.customname) = "SrcInstanceID",
    (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/base.SQLInstanceID"];

  optional int32 dest_instance_id = 6 [
    (gogoproto.nullable) = false,
    (gogoproto.customname) = "DestInstanceID",
    (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/base.SQLInstanceID"];
}

// StreamIngestionPartitionSpecs contains all the partition specs that are part
// of the DistSQL plan.
message StreamIngestionPartitionSpecs {
  repeated StreamIngestionPartitionSpec specs = 3;
}

// FrontierEntries is a snapshot of a span frontier.
message FrontierEntries {
  repeated jobs.jobspb.ResolvedSpan resolved_spans = 1 [(gogoproto.nullable) = false];
}

message StreamIngestionDataSpec {
  reserved 1;

  // StreamID is the ID of the stream (which is shared across the producer and consumer).
  optional uint64 stream_id = 5 [(gogoproto.nullable) = false, (gogoproto.customname) = "StreamID"];

  // PartitionSpecs maps partition IDs to their specifications.
  map<string, StreamIngestionPartitionSpec> partition_specs = 6 [(gogoproto.nullable) = false];

  // PreviousReplicatedTimestamp specifies the timestamp from which spans will
  // start ingesting data in the replication job. This timestamp is empty unless
  // the replication job resumes after a progress checkpoint has been recorded.
  // While it is empty we use the InitialScanTimestamp described below.
  optional util.hlc.Timestamp previous_replicated_timestamp = 2 [(gogoproto.nullable) = false];

  // InitialScanTimestamp is the timestamp at which the partition will run the
  // initial rangefeed scan before replicating further changes to the target
  // spans. This timestamp is always non-empty, but a partition will only run an
  // initial scan if no progress has been recorded prior to the current
  // resumption of the replication job. Otherwise, all spans will start
  // ingesting data from the PreviousReplicatedTimestamp described above.
  optional util.hlc.Timestamp initial_scan_timestamp = 11 [(gogoproto.nullable) = false];

  reserved 3; // this was `string stream_address` in case we want to resurrect it.

  // JobID is the job ID of the stream ingestion job.
  optional int64 job_id = 4 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];

  // The processor will rekey the tenant's keyspace to a new tenant based on 'tenant_rekey'.
  optional TenantRekey tenant_rekey = 9 [(gogoproto.nullable) = false, (gogoproto.customname) = "TenantRekey"];

  // Checkpoint stores a set of resolved spans denoting completed progress.
  optional jobs.jobspb.StreamIngestionCheckpoint checkpoint = 10 [(gogoproto.nullable) = false];
}

message StreamIngestionFrontierSpec {
  // ReplicatedTimeAtStart is set by the ingestion job when initializing the frontier
  // processor. It is used as sanity check by the frontier processor to ensure
  // that it does not receive updates at a timestamp lower than this field. This
  // consequently prevents the job progress from regressing during ingestion.
  optional util.hlc.Timestamp replicated_time_at_start = 1 [(gogoproto.nullable) = false];
  // TrackedSpans is the entire span set being watched. The spans do not really
  // represent KV spans but uniquely identify the partitions in the ingestion
  // stream. Once all the partitions in the ingestion stream have been resolved
  // at a certain timestamp, then it's safe to resolve the ingestion at that
  // timestamp.
  repeated roachpb.Span tracked_spans = 2 [(gogoproto.nullable) = false];

  reserved 6;

  // JobID is the job ID of the stream ingestion job.
  optional int64 job_id = 3 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];

  // StreamID is the ID of the stream.
  optional uint64 stream_id = 4 [(gogoproto.nullable) = false, (gogoproto.customname) = "StreamID"];
  reserved 5;

  // Checkpoint stores a set of resolved spans denoting completed progress
  optional jobs.jobspb.StreamIngestionCheckpoint checkpoint = 7 [(gogoproto.nullable) = false];

	// ConnectionUris are the URIs that can be connected to to interact with the
	// source job
  repeated string connection_uris = 8;

  // PartitionSpecs contains the topology of the physical replication stream.
  optional StreamIngestionPartitionSpecs partition_specs = 9 [(gogoproto.nullable) = false];
}

enum ElidePrefix {
  None = 0;
  Tenant = 1;
  TenantAndTable = 2;
}

message BackupDataSpec {
  // TODO(lidor): job_id is not needed when interoperability with 22.2 is
  // dropped, the new way to send the job tag is using 'job_tag' in the
  // SetupFlowRequest message.
  optional int64 job_id = 11 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];
  repeated roachpb.Span spans = 1 [(gogoproto.nullable) = false];
  repeated roachpb.Span introduced_spans = 2 [(gogoproto.nullable) = false];
  optional string default_uri = 3 [(gogoproto.nullable) = false, (gogoproto.customname) = "DefaultURI"];
  map<string, string> uris_by_locality_kv = 4 [(gogoproto.customname) = "URIsByLocalityKV"];
  optional roachpb.MVCCFilter mvcc_filter = 5 [(gogoproto.nullable) = false, (gogoproto.customname) = "MVCCFilter"];
  optional roachpb.FileEncryptionOptions encryption = 6;
  optional util.hlc.Timestamp backup_start_time = 7 [(gogoproto.nullable) = false];
  optional util.hlc.Timestamp backup_end_time = 8 [(gogoproto.nullable) = false];

  // PKIDs is used to convert result from an ExportRequest into row count
  // information passed back to track progress in the backup job.
  map<uint64, bool> pk_ids = 9 [(gogoproto.customname) = "PKIDs"];

  // User who initiated the backup. This is used to check access privileges
  // when using FileTable ExternalStorage.
  optional string user_proto = 10 [(gogoproto.nullable) = false, (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/security/username.SQLUsernameProto"];

  optional ElidePrefix elide_prefix = 12 [(gogoproto.nullable) = false];

  // IncludeMVCCValueHeader indicates whether the backup should be
  // created with MVCCValueHeaders in the exported data. This should
  // only be set on backups starting on cluster version 24.1 or
  // greater.
  optional bool include_mvcc_value_header = 13 [(gogoproto.nullable) = false, (gogoproto.customname) = "IncludeMVCCValueHeader"];

  // NEXTID: 14.
}

message RestoreFileSpec {
  optional cloud.cloudpb.ExternalStorage dir = 1 [(gogoproto.nullable) = false];
  optional string path = 2 [(gogoproto.nullable) = false];
  reserved 3;
  reserved 4;
  optional roachpb.Span backup_file_entry_span = 5 [(gogoproto.nullable) = false];
  optional roachpb.RowCount backup_file_entry_counts = 6 [(gogoproto.nullable) = false];
  optional uint64 backing_file_size = 7 [(gogoproto.nullable) = false];
  optional uint64 approximate_physical_size = 8 [(gogoproto.nullable) = false];
  optional int32 layer = 9 [(gogoproto.nullable) = false];
  optional bool has_range_keys = 10 [(gogoproto.nullable) = false];
  // NEXT ID: 11.
}

message TableRekey {
  // OldID is the previous ID of `new_desc`.
  optional uint32 old_id = 1 [(gogoproto.nullable) = false, (gogoproto.customname) = "OldID"];
  // NewDesc is an encoded Descriptor message.
  optional bytes new_desc = 2;
}

message TenantRekey {
  // OldID is a previous tenant ID.
  optional roachpb.TenantID old_id = 1 [(gogoproto.nullable) = false, (gogoproto.customname) = "OldID"];
  // NewID is the ID with which to replace OldID.
  optional roachpb.TenantID new_id = 2 [(gogoproto.nullable) = false, (gogoproto.customname) = "NewID"];
}

// RestoreDataEntry will be specified at planning time to the SplitAndScatter
// processors, then those processors will stream these, encoded as bytes in rows
// to the RestoreDataProcessors.
// This field has a subset of the importEntry struct defined in restore.
message RestoreSpanEntry {
  optional roachpb.Span span = 1 [(gogoproto.nullable) = false];
  repeated RestoreFileSpec files = 2 [(gogoproto.nullable) = false];
  optional int64 progressIdx = 3 [(gogoproto.nullable) = false];
  optional ElidePrefix elided_prefix = 4 [(gogoproto.nullable) = false];
}

message RestoreDataSpec {
  // TODO(lidor): job_id is not needed when interoperability with 22.2 is
  // dropped, the new way to send the job tag is using 'job_tag' in the
  // SetupFlowRequest message.
  optional int64 job_id = 6 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];
  optional util.hlc.Timestamp restore_time = 1 [(gogoproto.nullable) = false];
  optional roachpb.FileEncryptionOptions encryption = 2;
  repeated TableRekey table_rekeys = 3 [(gogoproto.nullable) = false];
  repeated TenantRekey tenant_rekeys = 5[(gogoproto.nullable) = false];
  // PKIDs is used to convert result from an ExportRequest into row count
  // information passed back to track progress in the backup job.
  map<uint64, bool> pk_ids = 4 [(gogoproto.customname) = "PKIDs"];
  reserved 7;
  optional bool validate_only = 8 [(gogoproto.nullable) = false];
  reserved 9;

  // ResumeClusterVersion is the cluster version when the restore job resumed.
  optional roachpb.Version resume_cluster_version = 10 [(gogoproto.nullable) = false];
  // NEXT ID: 11.
}

// ExporterSpec is the specification for a processor that consumes rows and
// writes them to Parquet or CSV files at uri. It outputs a row per file written with
// the file name, row count and byte size.
message ExportSpec {
  // destination as a cloud.ExternalStorage URI pointing to an export store
  // location (directory).
  optional string destination = 1 [(gogoproto.nullable) = false];
  optional string name_pattern = 2 [(gogoproto.nullable) = false];
  optional roachpb.IOFileFormat format = 3 [(gogoproto.nullable) = false];

  // chunk_rows is num rows to write per file. 0 = no limit.
  optional int64 chunk_rows = 4 [(gogoproto.nullable) = false];
  // chunk_size is the target byte size per file.
  optional int64 chunk_size = 5 [(gogoproto.nullable) = false];

  // User who initiated the export. This is used to check access privileges
  // when using FileTable ExternalStorage.
  optional string user_proto = 6 [(gogoproto.nullable) = false, (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/security/username.SQLUsernameProto"];

  // col_names specifies the logical column names for the exported parquet file.
  repeated string col_names = 7 ;
}

// BulkRowWriterSpec is the specification for a processor that consumes rows and
// writes them to a target table using AddSSTable. It outputs a BulkOpSummary.
message BulkRowWriterSpec {
  optional sqlbase.TableDescriptor table = 1 [(gogoproto.nullable) = false];
}

message IndexBackfillMergerSpec {
  optional sqlbase.TableDescriptor table = 1 [(gogoproto.nullable) = false];

  repeated uint32 temporary_indexes = 2 [(gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/sql/catalog/descpb.IndexID"];
  repeated uint32 added_indexes = 3 [(gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/sql/catalog/descpb.IndexID"];

  repeated roachpb.Span spans = 4 [(gogoproto.nullable) = false];
  repeated int32 span_idx = 5;

  optional util.hlc.Timestamp mergeTimestamp = 8 [(gogoproto.nullable) = false];

  // NEXT ID: 9.
}

message GenerativeSplitAndScatterSpec {
  repeated TableRekey table_rekeys = 1 [(gogoproto.nullable) = false];
  repeated TenantRekey tenant_rekeys = 2 [(gogoproto.nullable) = false];
  optional bool validate_only = 3 [(gogoproto.nullable) = false];

  // URIs is the URIs of the backup manifests.
  repeated string uris = 4 [(gogoproto.customname) = "URIs"];
  optional jobs.jobspb.BackupEncryptionOptions encryption = 5;

  // EndTime is the time of the restore.
  optional util.hlc.Timestamp endTime = 9 [(gogoproto.nullable) = false];
  // Spans is the required spans in the restore.
  repeated roachpb.Span spans = 10 [(gogoproto.nullable) = false];
  repeated jobs.jobspb.RestoreDetails.BackupLocalityInfo backup_locality_info = 11 [(gogoproto.nullable) = false];
  reserved 12;
  // User who initiated the restore.
  optional string user_proto = 13 [(gogoproto.nullable) = false, (gogoproto.casttype) = "github.com/cockroachdb/cockroach/pkg/security/username.SQLUsernameProto"];
  // ChunkSize is the number of import spans per chunk.
  optional int64 chunk_size = 14[(gogoproto.nullable) = false];
  // TargetSize is the target size for each import span.
  optional int64 target_size = 15[(gogoproto.nullable) = false];
  // NumEntries is the total number of import spans in this restore.
  optional int64 num_entries = 16[(gogoproto.nullable) = false];
  // NumNodes is the number of nodes available for dist restore.
  optional int64 num_nodes = 17[(gogoproto.nullable) = false];
  optional int64 job_id = 18 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];
  reserved 20 ;
  repeated jobs.jobspb.RestoreProgress.FrontierEntry checkpointed_spans = 21 [(gogoproto.nullable) = false];
  // ExclusiveFileSpanComparison is true if the backup can safely use
  // exclusive file span comparison.
  optional bool exclusive_file_span_comparison = 22 [(gogoproto.nullable) = false];
  // MaxFileCount is the max number of files in an extending restore span entry.
  optional int64 max_file_count = 23[(gogoproto.nullable) = false];
  // SQLInstanceIDs is a slice of SQL instance IDs available for dist restore.
  repeated int32 sql_instance_ids = 24[(gogoproto.customname) = "SQLInstanceIDs"];
  reserved 19;
}



message CloudStorageTestSpec {
  optional string location = 1 [(gogoproto.nullable) = false];
  message Params {
    // TransferSize determines the size of the file that is written and then
    // read back in its entirety in each iteration of the test.
    optional int64 transfer_size = 1 [(gogoproto.nullable) = false];
    // MinDuration causes the test to be run repeatedly until the duration has
    // elapsed.
    optional int64 min_duration = 2 [(gogoproto.nullable) = false, (gogoproto.casttype) = "time.Duration"];
    // Concurrency causes multiple tests to be run in parallel.
    optional int64 concurrency = 3 [(gogoproto.nullable) = false];
  }
  optional Params params = 2 [(gogoproto.nullable) = false];
  // NEXT ID: 3;
}

message TableReplicationMetadata {
  optional cockroach.sql.sqlbase.TableDescriptor source_descriptor = 1 [(gogoproto.nullable) = false];
  optional string destination_parent_database_name = 2 [(gogoproto.nullable) = false];
  optional string destination_parent_schema_name = 3 [(gogoproto.nullable) = false];
  optional string destination_table_name = 4 [(gogoproto.nullable) = false];
  // DestinationFunctionOID, if non-zero, is the OID of the
  // user-defined function that should be used for the table.
  optional uint32 destination_function_oid = 5 [(gogoproto.nullable) = false, (gogoproto.customname) = "DestinationFunctionOID"];
}

message LogicalReplicationWriterSpec {
    // JobID of the job that ran the replicationWriterProcessor.
    optional int64 job_id = 1 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];

    // StreamID is the ID of the stream (which is shared across the producer and consumer).
    optional uint64 stream_id = 2 [(gogoproto.nullable) = false, (gogoproto.customname) = "StreamID"];

    // PartitionSpec is the source partition this processor will create a
    // subscription for.
    optional StreamIngestionPartitionSpec partition_spec = 3 [(gogoproto.nullable) = false];

    // PreviousReplicatedTimestamp specifies the timestamp from which spans will
    // start ingesting data in the replication job. This timestamp is empty unless
    // the replication job resumes after a progress checkpoint has been recorded.
    // While it is empty we use the InitialScanTimestamp described below.
    optional util.hlc.Timestamp previous_replicated_timestamp = 4 [(gogoproto.nullable) = false];

    // InitialScanTimestamp is the timestamp at which the partition will run the
    // initial rangefeed scan before replicating further changes to the target
    // spans. This timestamp is always non-empty, but a partition will only run an
    // initial scan if no progress has been recorded prior to the current
    // resumption of the replication job. Otherwise, all spans will start
    // ingesting data from the PreviousReplicatedTimestamp described above.
    optional util.hlc.Timestamp initial_scan_timestamp = 5 [(gogoproto.nullable) = false];

		// PartitionConnUri locate the stream so that a stream client can be
		// initialized.
    optional string partition_conn_uri = 6 [(gogoproto.nullable) = false];

    // Checkpoint stores a set of resolved spans denoting completed progress.
    optional jobs.jobspb.StreamIngestionCheckpoint checkpoint = 7 [(gogoproto.nullable) = false];

    // TableMetadataByDestID is a map from destination table IDs to metadata
    // containing source table descriptors and fully qualified destination table
    // names.
    map<int32, TableReplicationMetadata> table_metadata_by_dest_id = 8 [(gogoproto.nullable) = false, (gogoproto.customname) = "TableMetadataByDestID"];

    reserved 9;

    // Discard is an option on whether to filter some events.
    optional jobs.jobspb.LogicalReplicationDetails.Discard discard  = 12 [(gogoproto.nullable) = false];

    optional jobs.jobspb.LogicalReplicationDetails.ApplyMode mode = 10 [(gogoproto.nullable) = false];

    optional string metrics_label = 11 [(gogoproto.nullable) = false];

    // Next ID: 13.
}

message LogicalReplicationOfflineScanSpec {
    // JobID of the job that ran the replicationWriterProcessor.
    optional int64 job_id = 1 [(gogoproto.nullable) = false, (gogoproto.customname) = "JobID"];

    // StreamID is the ID of the stream (which is shared across the producer and consumer).
    optional uint64 stream_id = 2 [(gogoproto.nullable) = false, (gogoproto.customname) = "StreamID"];

    // PartitionSpec is the source partition this processor will create a
    // subscription for.
    optional StreamIngestionPartitionSpec partition_spec = 3 [(gogoproto.nullable) = false];

    // InitialScanTimestamp is the timestamp at which the partition will run the
    // initial rangefeed scan before replicating further changes to the target
    // spans. This timestamp is always non-empty, but a partition will only run an
    // initial scan if no progress has been recorded prior to the current
    // resumption of the replication job. Otherwise, all spans will start
    // ingesting data from the PreviousReplicatedTimestamp described above.
    optional util.hlc.Timestamp initial_scan_timestamp = 4 [(gogoproto.nullable) = false];

    // StreamAddress locate the stream so that a stream client can be initialized.
    optional string stream_address = 5 [(gogoproto.nullable) = false];

    // Checkpoint stores a set of resolved spans denoting completed progress.
    optional jobs.jobspb.StreamIngestionCheckpoint checkpoint = 6 [(gogoproto.nullable) = false];

    repeated TableRekey rekey = 7 [(gogoproto.nullable) = false];

    optional string metrics_label = 8 [(gogoproto.nullable) = false];

}
