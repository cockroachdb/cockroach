# LogicTest: local

# logicTest.newCluster uses a 192MB limit. We can insert multiple large rows
# individually, but reading them out in a single scan caused by an index join
# trips the memory limit in KV during the scan.
#
# Once we improve the handling for a single query as discussed in
# https://github.com/cockroachdb/cockroach/issues/54680 we will need to
# rewrite this test to issue multiple concurrent queries in a loop until we
# trigger a condition where the aggregate memory across the concurrently
# executing queries exceeds the limit, and one of them returns an error.

statement ok
CREATE TABLE foo (id INT PRIMARY KEY, attribute INT, blob TEXT, INDEX(attribute))

statement ok
INSERT INTO foo SELECT 1, 10, repeat('a', 60000000)

statement ok
INSERT INTO foo SELECT 2, 10, repeat('a', 60000000)

statement ok
INSERT INTO foo SELECT 3, 10, repeat('a', 60000000)

statement ok
INSERT INTO foo SELECT 4, 10, repeat('a', 60000000)

query T
EXPLAIN SELECT * FROM foo@foo_attribute_idx WHERE attribute=10 AND blob LIKE 'blah%'
----
distribution: local
vectorized: true
·
• filter
│ filter: blob LIKE 'blah%'
│
└── • index join
    │ table: foo@primary
    │
    └── • scan
          missing stats
          table: foo@foo_attribute_idx
          spans: [/10 - /10]

query error scan with start key .* memory budget exceeded
SELECT * FROM foo@foo_attribute_idx WHERE attribute=10 AND blob LIKE 'blah%'
