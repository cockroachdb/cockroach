# cluster-opt: disable-mvcc-range-tombstones-for-point-deletes

statement ok
SET kv_transaction_buffered_writes_enabled=true

statement ok
CREATE TABLE t1 (pk int primary key, v int, FAMILY (pk, v))

subtest point_delete

statement ok
INSERT INTO t1 VALUES (1,1)

statement ok
BEGIN

statement count 1
DELETE FROM t1 WHERE pk = 1

statement count 0
DELETE FROM t1 WHERE pk = 3

statement ok
COMMIT

subtest repeated_point_delete

statement ok
INSERT INTO t1 VALUES (1,1)

statement ok
BEGIN

statement count 1
DELETE FROM t1 WHERE pk = 1

# The second delete should be served from the write buffer and observe
# the buffered tombstone.
statement count 0
DELETE FROM t1 WHERE pk = 1

statement ok
COMMIT

subtest point_delete_after_write

statement ok
BEGIN

statement ok
INSERT INTO t1 VALUES (1,1)

statement count 1
DELETE FROM t1 WHERE pk = 1

# The second delete should be served from the write buffer and observe
# the buffered tombstone.
statement count 0
DELETE FROM t1 WHERE pk = 1

statement ok
COMMIT

subtest delete_then_insert

statement ok
INSERT INTO t1 VALUES (1,1)

statement ok
BEGIN

statement count 1
DELETE FROM t1 WHERE pk = 1

statement ok
INSERT INTO t1 VALUES (1,1)

statement ok
COMMIT

query II rowsort
SELECT * FROM t1
----
1 1

statement ok
CREATE TABLE t2 (k INT PRIMARY KEY);

statement ok
BEGIN;

statement error pgcode 23505 duplicate key value violates unique constraint "t2_pkey"
INSERT INTO t2 VALUES (1), (1);

statement ok
ROLLBACK;

statement ok
BEGIN;

statement ok
INSERT INTO t2 VALUES (1);

statement error pgcode 23505 duplicate key value violates unique constraint "t2_pkey"
INSERT INTO t2 VALUES (1);

statement ok
ROLLBACK;

statement ok
BEGIN;

statement ok
INSERT INTO t2 VALUES (1);

statement ok
DELETE FROM t2 WHERE k = 1;

statement ok
INSERT INTO t2 VALUES (1);

statement ok
COMMIT;

query I rowsort
SELECT * FROM t2
----
1

# Ensure that DeleteRange requests work correctly with buffered writes. In
# particular, a DeleteRange request results in a buffer flush.

statement ok
CREATE TABLE t3 (k INT PRIMARY KEY)

statement ok
INSERT INTO t3 VALUES (1)

statement ok
BEGIN

statement ok
INSERT INTO t3 VALUES (2)

statement count 0
DELETE FROM t3 WHERE k = 3

statement count 2
DELETE FROM t3 WHERE k < 10 AND k > 0

statement ok
COMMIT

query I rowsort
SELECT count(*) from t3
----
0

# Test savepoints, and in particular savepoint rollbacks, with buffered writes. 
# We test both intermediate selects after rollbacks and the final state
# the transaction has been committed.
subtest savepoint_rollbacks

# First, create a new table with a secondary index on it. That way, the DELETE
# statements below will not use DeleteRange requets which cause the buffer to
# be flushed.
statement ok
CREATE TABLE t4 (k INT PRIMARY KEY, v INT)

statement ok
CREATE INDEX idx ON t4 (v)

statement ok
BEGIN;
INSERT INTO t4 VALUES(1, 100), (2, 200), (3, 300);
SAVEPOINT s1;
INSERT INTO t4 VALUES(4, 400), (5, 500), (6, 600)

query II rowsort
SELECT * FROM t4
----
1  100
2  200
3  300
4  400
5  500
6  600

statement ok
SAVEPOINT s2; 
INSERT INTO t4 VALUES(7, 700), (8, 800), (9, 900)

query II rowsort
SELECT * FROM t4
----
1  100
2  200
3  300
4  400
5  500
6  600
7  700
8  800
9  900

# Throw in some Deletes.
statement ok
DELETE FROM t4 WHERE k = 1;
DELETE FROM t4 WHERE k = 2;
DELETE FROM t4 WHERE k = 3;

query II rowsort
SELECT * FROM t4
----
4  400
5  500
6  600
7  700
8  800
9  900

statement ok
ROLLBACK TO SAVEPOINT s2

query II rowsort
SELECT * FROM t4
----
1  100
2  200
3  300
4  400
5  500
6  600

statement ok
ROLLBACK TO SAVEPOINT s1;

query II rowsort
SELECT * FROM t4
----
1  100
2  200
3  300

statement ok
COMMIT

query II rowsort
SELECT * FROM t4
----
1  100
2  200
3  300

subtest regression

# Regression test for #144274.
statement ok
EXPLAIN CREATE DATABASE foo

statement ok
EXPLAIN ANALYZE CREATE DATABASE foo

# Regression test for #144273.
statement ok
CREATE TABLE t144273 (
  k INT PRIMARY KEY,
  a INT,
  b INT,
  INDEX (a),
  INDEX (b),
  FAMILY (k, a, b)
)

statement ok
PREPARE p AS UPDATE t144273 t1 SET a = t2.a + 1, b = t2.b + 1 FROM t144273 t2 WHERE t1.k = t2.a AND t1.k = $1

statement ok
SET vectorize = on

statement ok
INSERT INTO t144273 VALUES (1, 1, 1);

# The table IDs in the kv trace below are different with the legacy schema
# changer, so disable that configuration.
skipif config local-legacy-schema-changer
query T kvtrace
EXECUTE p(1)
----
Scan /Table/114/2/{1-2}
Scan /Table/114/1/1/0
Scan /Table/114/1/1/0
Put (locking) /Table/114/1/1/0 -> /TUPLE/2:2:Int/2/1:3:Int/2
Del /Table/114/2/1/1/0
Put /Table/114/2/2/1/0 -> /BYTES/
Del /Table/114/3/1/1/0
Put /Table/114/3/2/1/0 -> /BYTES/

statement ok
SET vectorize = off

statement ok
INSERT INTO t144273 VALUES (2, 2, 2);

# The table IDs in the kv trace below are different with the legacy schema
# changer, so disable that configuration.
skipif config local-legacy-schema-changer
query T kvtrace
EXECUTE p(2)
----
Scan /Table/114/2/{2-3}
Scan /Table/114/1/1/0, /Table/114/1/2/0
Scan /Table/114/1/2/0
Put (locking) /Table/114/1/2/0 -> /TUPLE/2:2:Int/3/1:3:Int/3
Del /Table/114/2/2/2/0
Put /Table/114/2/3/2/0 -> /BYTES/
Del /Table/114/3/2/2/0
Put /Table/114/3/3/2/0 -> /BYTES/

statement ok
RESET vectorize

# Create some large blobs that will require the scans to be paginated (in 10MiB
# chunks).
statement ok
CREATE TABLE large (k PRIMARY KEY, blob) AS SELECT i * 100, repeat('a', 11 * 1024 * 1024) FROM generate_series(1, 4) AS g(i);

statement ok
BEGIN;

statement ok
INSERT INTO large SELECT 11 + i * 100, 'b' FROM generate_series(1, 4) AS g(i);

# Forward scan.
query I
SELECT k FROM large ORDER BY k;
----
100
111
200
211
300
311
400
411

# Reverse scan.
query I
SELECT k FROM large ORDER BY k DESC;
----
411
400
311
300
211
200
111
100

statement ok
COMMIT;
