// Copyright 2016 The Cockroach Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
// implied. See the License for the specific language governing
// permissions and limitations under the License.
//
// Author: Radu Berinde (radu@cockroachlabs.com)
// Author: Irfan Sharif (irfansharif@cockroachlabs.com)
//
// Processor definitions for distributed SQL APIs. See
// docs/RFCS/distributed_sql.md.
// All the concepts here are "physical plan" concepts.

syntax = "proto2";
package cockroach.sql.distsql;
option go_package = "distsql";

import "cockroach/pkg/roachpb/data.proto";
import "cockroach/pkg/roachpb/errors.proto";
import "cockroach/pkg/sql/sqlbase/structured.proto";
import "cockroach/pkg/sql/sqlbase/encoded_datum.proto";
import "cockroach/pkg/sql/distsql/data.proto";
import "gogoproto/gogo.proto";

// NoopCoreSpec indicates a "no-op" processor core. This is used when only a
// synchronizer is required, e.g. at the final endpoint.
message NoopCoreSpec {
}

message TableReaderSpan {
  // TODO(radu): the dist_sql APIs should be agnostic to how we map tables to
  // KVs. The span should be described as starting and ending lists of values
  // for a prefix of the index columns, along with inclusive/exclusive flags.
  optional roachpb.Span span = 1 [(gogoproto.nullable) = false];
}

// TableReaderSpec is the specification for a "table reader". A table reader
// performs KV operations to retrieve rows for a table and outputs the desired
// columns of the rows that pass a filter expression.
message TableReaderSpec {
  optional sqlbase.TableDescriptor table = 1 [(gogoproto.nullable) = false];
  // If 0, we use the primary index. If non-zero, we use the index_idx-th index,
  // i.e. table.indexes[index_idx-1]
  optional uint32 index_idx = 2 [(gogoproto.nullable) = false];
  optional bool reverse = 3 [(gogoproto.nullable) = false];
  repeated TableReaderSpan spans = 4 [(gogoproto.nullable) = false];

  // The filter expression references the columns in the table (table.columns)
  // via $0, $1, etc. If a secondary index is used, the columns that are not
  // available as part of the index cannot be referenced.
  optional Expression filter = 5 [(gogoproto.nullable) = false];

  // The table reader will only produce values for these columns, referenced by
  // their indices in table.columns.
  repeated uint32 output_columns = 6 [packed = true];

  // If nonzero, the table reader only needs to return this many rows.
  optional int64 hard_limit = 8 [(gogoproto.nullable) = false];
  // The soft limit is a hint for how many rows the consumer of the table reader
  // output might need. If both the hard limit and the soft limit are set, the
  // soft limit must be lower than the hard limit.
  optional int64 soft_limit = 7 [(gogoproto.nullable) = false];
}

// JoinReaderSpec is the specification for a "join reader". A join reader
// performs KV operations to retrieve specific rows that correspond to the
// values in the input stream (join by lookup).
message JoinReaderSpec {
  optional sqlbase.TableDescriptor table = 1 [(gogoproto.nullable) = false];

  // If 0, we use the primary index; each row in the input stream has a value
  // for each primary key.
  // TODO(radu): figure out the correct semantics when joining with an index.
  optional uint32 index_idx = 2 [(gogoproto.nullable) = false];

  // The filter expression references the columns in the table (table.columns)
  // via $0, $1, etc. If a secondary index is used, the columns that are not
  // available as part of the index cannot be referenced.
  optional Expression filter = 3 [(gogoproto.nullable) = false];

  // The table reader will only produce values for these columns, referenced by
  // their indices in table.columns.
  repeated uint32 output_columns = 4 [packed = true];

  // TODO(radu): add field to describe the input columns and allow plumbing
  // through values that aren't used for the lookup.
}

// SorterSpec is the specification for a "sorting aggregator". A sorting
// aggregator sorts elements in the input stream providing a certain output
// order guarantee regardless of the input ordering. The output ordering is
// according to a configurable set of columns.
message SorterSpec {
  optional Ordering output_ordering = 1 [(gogoproto.nullable) = false];

  // Ordering match length, specifying that the input is already sorted by the
  // first 'n' output ordering columns, can be optionally specified for
  // possible speed-ups taking advantage of the partial orderings.
  optional uint32 ordering_match_len = 2 [(gogoproto.nullable) = false];

  // Limits can be optionally specified to allow for further optimizations
  // taking advantage of the fact that only the top 'k' results are needed.
  optional int64 limit = 3 [(gogoproto.nullable) = false];
}

// EvaluatorSpec is the specification for an "evaluator", a fully
// programmable no-grouping aggregator. It runs a 'program' on each individual
// row and is restricted to operating on one row of data at a time.
// The 'program' is a set of expressions evaluated in order, the output
// schema therefore consists of the results of evaluating each of these
// expressions on the input row.
//
// TODO(irfansharif): Add support for an optional output filter expression.
// The filter expression would reference the columns in the row via $0, $1,
// etc., possibly optimizing if filtering on expressions common to the
// 'program'.
message EvaluatorSpec {
  repeated sqlbase.ColumnType.Kind types = 1;

  repeated Expression exprs = 2 [(gogoproto.nullable) = false];
}

message DistinctSpec {
  // The ordering of the input stream can be optionally specified for
  // possible optimizations.
  optional Ordering ordering = 1 [(gogoproto.nullable) = false];
}

enum JoinType {
  INNER = 0;
  LEFT_OUTER = 1;
  RIGHT_OUTER = 2;
  FULL_OUTER = 3;
}

// MergeJoinerSpec is the specification for a merge join processor. The processor
// has two inputs and one output. The inputs must have the same ordering on the
// columns that have equality constraints. For example:
//   SELECT * FROM T1 INNER JOIN T2 ON T1.C1 = T2.C5 AND T1.C2 = T2.C4
//
// To perform a merge join, the streams corresponding to T1 and T2 must have the
// same ordering on columns C1, C2 and C5, C4 respectively. For example: C1+,C2-
// and C5+,C4-.
//
// It is guaranteed that the results preserve this ordering.
message MergeJoinerSpec {
  // The streams must be ordered according to the columns that have equality
  // constraints. The first column of the left ordering is constrained to be
  // equal to the first column in the right ordering and so on. The ordering
  // lengths and directions must match.
  // In the example above, left_ordering describes C1+,C2- and right_ordering
  // describes C5+,C4-.
  optional Ordering left_ordering = 1 [(gogoproto.nullable) = false];
  optional Ordering right_ordering = 2 [(gogoproto.nullable) = false];

  // "ON" expression (in addition to the equality constraints captured by the
  // orderings). Assuming that the left stream has N columns and the right
  // stream has M columns, in this expression variables $0 to $(N-1) refer to
  // columns of the left stream and variables $N to $(N+M-1) refer to columns in
  // the right stream.
  optional Expression expr = 3 [(gogoproto.nullable) = false];

  optional JoinType type = 4 [(gogoproto.nullable) = false];

  // Columns for the output stream. Assuming that the left stream has N columns
  // and the right stream has M columns, column indices 0 to (N-1) refer to left
  // stream columns and indices N to (N+M-1) refer to right stream columns.
  repeated uint32 output_columns = 5 [packed = true];
}

// HashJoinerSpec is the specification for a hash join processor. The processor
// has two inputs and one output.
//
// The processor works by reading the entire left input and putting it in a hash
// table. Thus, there is no guarantee on the ordering of results that stem only
// from the left input (in the case of LEFT_OUTER, FULL_OUTER). However, it is
// guaranteed that results that involve the right stream preserve the ordering;
// i.e. all results that stem from right row (i) precede results that stem from
// right row (i+1).
message HashJoinerSpec {
  // The join constraints certain columns from the left stream to equal
  // corresponding columns on the right stream. These must have the same length.
  repeated uint32 left_eq_columns = 1 [packed = true];
  repeated uint32 right_eq_columns = 2 [packed = true];

  // "ON" expression (in addition to the equality constraints captured by the
  // orderings). Assuming that the left stream has N columns and the right
  // stream has M columns, in this expression variables $0 to $(N-1) refer to
  // columns of the left stream and variables $N to $(N+M-1) refer to columns in
  // the right stream.
  optional Expression expr = 3 [(gogoproto.nullable) = false];

  optional JoinType type = 4 [(gogoproto.nullable) = false];

  // Columns for the output stream. Assuming that the left stream has N columns
  // and the right stream has M columns, column indices 0 to (N-1) refer to left
  // stream columns and indices N to (N+M-1) refer to right stream columns.
  repeated uint32 output_columns = 5 [packed = true];
}

// AggregatorSpec is the specification for an "aggregator" (processor core
// type, not the logical plan computation stage). An aggregator performs
// 'aggregation' in the SQL sense in that it groups rows and computes an aggregate
// for each group. The group is configured using the group key. The aggregator
// can be configured with one or more of the following aggregation functions:
//    SUM
//    COUNT
//    MIN
//    MAX
//    AVG
//    DISTINCT
//    COUNT DISTINCT
// The aggregator's output schema consists of the group key, plus a
// configurable subset of the generated aggregated values.
message AggregatorSpec {
  // These mirror the aggregate functions supported by sql/parser. See
  // sql/parser/aggregate_builtins.go.
  enum Func {
    // The identity function is set to be the default zero-value function,
    // returning the last value added.
    IDENT = 0;

    AVG = 1;
    BOOL_AND = 2;
    BOOL_OR = 3;
    CONCAT_AGG = 4;
    COUNT = 5;
    MAX = 7;
    MIN = 8;
    STDDEV = 9;
    SUM = 10;
    VARIANCE = 11;
  }

  message Expr {
    optional Func func = 1 [(gogoproto.nullable) = false];

    // Aggregation functions with distinct = true functions like you would
    // expect '<FUNC> DISTINCT' to operate, the default behaviour would be
    // the '<FUNC> ALL' operation.
    optional bool distinct = 2 [(gogoproto.nullable) = false];

    // The column index specifies the argument to the aggregator function.
    optional uint32 col_idx = 3 [(gogoproto.nullable) = false];
  }

  repeated sqlbase.ColumnType.Kind types = 1;

  // The group key is a subset of the columns in the input stream schema on the
  // basis of which we define our groups.
  repeated uint32 group_cols = 2;

  // Exprs represents the SELECT expressions.
  repeated Expr exprs = 3 [(gogoproto.nullable) = false];
}

message ProcessorCoreUnion {
  option (gogoproto.onlyone) = true;

  optional NoopCoreSpec noop = 1;
  optional TableReaderSpec tableReader = 2;
  optional JoinReaderSpec joinReader = 3;
  optional SorterSpec sorter = 4;
  optional AggregatorSpec aggregator = 5;
  optional EvaluatorSpec evaluator = 6;
  optional DistinctSpec distinct = 7;
  optional MergeJoinerSpec mergeJoiner = 8;
  optional HashJoinerSpec hashJoiner = 9;
  // TODO(radu): other "processor core" types will go here.
  // TODO(irfansharif): add aggregation, join, set operations, etc. from #7587
}

message ProcessorSpec {
  // In most cases, there is one input.
  repeated InputSyncSpec input = 1 [(gogoproto.nullable) = false];

  optional ProcessorCoreUnion core = 2 [(gogoproto.nullable) = false];

  // In most cases, there is one output.
  repeated OutputRouterSpec output = 3 [(gogoproto.nullable) = false];
}

// FlowSpec describes a "flow" which is a subgraph of a distributed SQL
// computation consisting of processors and streams.
message FlowSpec {
  optional bytes flow_id = 1 [(gogoproto.nullable) = false,
                              (gogoproto.customname) = "FlowID",
                              (gogoproto.customtype) = "FlowID"];

  repeated ProcessorSpec processors = 2 [(gogoproto.nullable) = false];
}
