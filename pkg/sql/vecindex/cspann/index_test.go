// Copyright 2024 The Cockroach Authors.
//
// Use of this software is governed by the CockroachDB Software License
// included in the /LICENSE file.

package cspann_test

import (
	"bytes"
	"cmp"
	"context"
	"fmt"
	"math"
	"math/rand"
	"regexp"
	"runtime"
	"sort"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann"
	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann/commontest"
	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann/memstore"
	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann/quantize"
	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann/testutils"
	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann/utils"
	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann/workspace"
	"github.com/cockroachdb/cockroach/pkg/util/leaktest"
	"github.com/cockroachdb/cockroach/pkg/util/log"
	"github.com/cockroachdb/cockroach/pkg/util/num32"
	"github.com/cockroachdb/cockroach/pkg/util/stop"
	"github.com/cockroachdb/cockroach/pkg/util/vector"
	"github.com/cockroachdb/datadriven"
	"github.com/cockroachdb/errors"
	"github.com/stretchr/testify/require"
)

func TestIndex(t *testing.T) {
	defer leaktest.AfterTest(t)()
	defer log.Scope(t).Close(t)

	ctx := context.Background()
	state := testState{T: t, Ctx: ctx, Stopper: stop.NewStopper()}
	defer state.Stopper.Stop(ctx)

	datadriven.Walk(t, "testdata", func(t *testing.T, path string) {
		if regexp.MustCompile("/.+/").MatchString(path) {
			// Skip files that are in subdirs.
			return
		}
		if !strings.HasSuffix(path, ".ddt") {
			// Skip files that are not data-driven tests.
			return
		}
		state.Reset()

		datadriven.RunTest(t, path, func(t *testing.T, d *datadriven.TestData) string {
			// Process args supported by every command.
			state.TreeKey = nil
			state.DiscardFixups = false
			state.SkipFixups = false
			for _, arg := range d.CmdArgs {
				switch arg.Key {
				case "tree":
					// K-means tree that the command operates on.
					state.TreeKey = state.parseTreeID(arg)

				case "discard-fixups":
					// Discard any fixups generated by the command.
					state.DiscardFixups = state.parseFlag(arg)

				case "skip-fixups":
					// Skip processing of any fixups generated by the command.
					state.SkipFixups = state.parseFlag(arg)
				}
			}

			var result string
			switch d.Cmd {
			case "new-index":
				result = state.NewIndex(d)

			case "load-index":
				result = state.LoadIndex(d)

			case "format-tree":
				result = state.FormatTree(d)

			case "search":
				result = state.Search(d)

			case "search-for-insert":
				result = state.SearchForInsert(d)

			case "search-for-delete":
				result = state.SearchForDelete(d)

			case "insert":
				result = state.Insert(d)

			case "delete":
				result = state.Delete(d)

			case "force-split", "force-merge":
				result = state.ForceSplitOrMerge(d)

			case "recall":
				result = state.Recall(d)

			case "validate-tree":
				result = state.ValidateTree(d)

			case "metrics":
				result = state.ShowMetrics(d)

			default:
				t.Fatalf("unknown cmd: %s", d.Cmd)
			}

			// Process or discard fixups generated by the command.
			state.processFixups()

			return result
		})
	})
}

type testState struct {
	T         *testing.T
	Ctx       context.Context
	Workspace workspace.T
	Stopper   *stop.Stopper
	Quantizer quantize.Quantizer
	MemStore  *memstore.Store
	Index     *cspann.Index
	Options   cspann.IndexOptions
	Features  vector.Set

	// Parse args.
	TreeKey       cspann.TreeKey
	DiscardFixups bool
	SkipFixups    bool

	// Metrics
	SuccessfulSplits    int
	PendingSplitsMerges int
}

func (s *testState) NewIndex(d *datadriven.TestData) string {
	s.makeNewIndex(d)

	// Insert initial vectors.
	return s.Insert(d)
}

func (s *testState) LoadIndex(d *datadriven.TestData) string {
	s.makeNewIndex(d)

	lines := strings.Split(d.Input, "\n")
	s.loadIndexFromFormat(s.TreeKey, lines, 0)

	return fmt.Sprintf("Loaded %d vectors.\n", len(s.MemStore.GetAllVectors()))
}

func (s *testState) Reset() {
	// Reset state between tests.
	s.SuccessfulSplits = 0
	s.PendingSplitsMerges = 0
}

func (s *testState) FormatTree(d *datadriven.TestData) string {
	var tree string
	commontest.RunTransaction(s.Ctx, s.T, s.MemStore, func(txn cspann.Txn) {
		rootPartitionKey := cspann.RootKey
		for _, arg := range d.CmdArgs {
			switch arg.Key {
			case "root":
				rootPartitionKey = cspann.PartitionKey(s.parseInt(arg))
			}
		}

		var err error
		options := cspann.FormatOptions{PrimaryKeyStrings: true, RootPartitionKey: rootPartitionKey}
		tree, err = s.Index.Format(s.Ctx, s.TreeKey, options)
		require.NoError(s.T, err)
	})
	return tree
}

func (s *testState) Search(d *datadriven.TestData) string {
	var vec vector.T
	searchSet := cspann.SearchSet{MaxResults: 1}
	options := cspann.SearchOptions{}

	for _, arg := range d.CmdArgs {
		switch arg.Key {
		case "use-feature":
			vec = s.parseUseFeature(arg)

		case "max-results":
			searchSet.MaxResults = s.parseInt(arg)

		case "beam-size":
			options.BaseBeamSize = s.parseInt(arg)

		case "skip-rerank":
			options.SkipRerank = s.parseFlag(arg)
		}
	}

	// If re-ranking results, make sure there are enough extra results to do that
	// effectively.
	if !options.SkipRerank {
		searchSet.MaxExtraResults = searchSet.MaxResults * cspann.RerankMultiplier
	}

	if vec == nil {
		// Parse input as the vector to search for.
		vec = s.parseVector(d.Input)
	}

	// Search the index within a transaction.
	commontest.RunTransaction(s.Ctx, s.T, s.MemStore, func(txn cspann.Txn) {
		var idxCtx cspann.Context
		idxCtx.Init(txn)
		err := s.Index.Search(s.Ctx, &idxCtx, s.TreeKey, vec, &searchSet, options)
		require.NoError(s.T, err)
	})

	var buf bytes.Buffer
	results := searchSet.PopResults()
	for i := range results {
		result := &results[i]
		var errorBound string
		if result.ErrorBound != 0 {
			errorBound = fmt.Sprintf("±%s ", utils.FormatFloat(result.ErrorBound, 2))
		}
		fmt.Fprintf(&buf, "%s: %s %s(centroid=%s)\n",
			string(result.ChildKey.KeyBytes), utils.FormatFloat(result.QuerySquaredDistance, 4),
			errorBound, utils.FormatFloat(result.CentroidDistance, 2))
	}

	buf.WriteString(fmt.Sprintf("%d leaf vectors, ", searchSet.Stats.QuantizedLeafVectorCount))
	buf.WriteString(fmt.Sprintf("%d vectors, ", searchSet.Stats.QuantizedVectorCount))
	buf.WriteString(fmt.Sprintf("%d full vectors, ", searchSet.Stats.FullVectorCount))
	buf.WriteString(fmt.Sprintf("%d partitions", searchSet.Stats.PartitionCount))

	return buf.String()
}

func (s *testState) SearchForInsert(d *datadriven.TestData) string {
	var vec vector.T

	for _, arg := range d.CmdArgs {
		switch arg.Key {
		case "use-feature":
			vec = s.parseUseFeature(arg)
		}
	}

	if vec == nil {
		// Parse input as the vector to search for.
		vec = s.parseVector(d.Input)
	}

	// Search the index within a transaction.
	var result *cspann.SearchResult
	commontest.RunTransaction(s.Ctx, s.T, s.MemStore, func(txn cspann.Txn) {
		var idxCtx cspann.Context
		idxCtx.Init(txn)
		var err error
		result, err = s.Index.SearchForInsert(s.Ctx, &idxCtx, s.TreeKey, vec)
		require.NoError(s.T, err)
	})

	var buf bytes.Buffer
	fmt.Fprintf(&buf, "partition %d, centroid=", result.ChildKey.PartitionKey)

	// Un-randomize the centroid and write it to buffer.
	original := make(vector.T, len(result.Vector))
	s.Index.UnRandomizeVector(result.Vector, original)
	utils.WriteVector(&buf, original, 4)

	fmt.Fprintf(&buf, ", sqdist=%s", utils.FormatFloat(result.QuerySquaredDistance, 4))
	if result.ErrorBound != 0 {
		fmt.Fprintf(&buf, "±%s", utils.FormatFloat(result.ErrorBound, 2))
	}
	buf.WriteByte('\n')

	return buf.String()
}

func (s *testState) SearchForDelete(d *datadriven.TestData) string {
	var buf bytes.Buffer

	var idxCtx cspann.Context
	for _, line := range strings.Split(d.Input, "\n") {
		line = strings.TrimSpace(line)
		if len(line) == 0 {
			continue
		}

		key, vec := s.parseKeyAndVector(line)

		// Search within a transaction.
		commontest.RunTransaction(s.Ctx, s.T, s.MemStore, func(txn cspann.Txn) {
			idxCtx.Init(txn)
			result, err := s.Index.SearchForDelete(s.Ctx, &idxCtx, s.TreeKey, vec, key)
			require.NoError(s.T, err)

			if result == nil {
				fmt.Fprintf(&buf, "%s: vector not found\n", string(key))
			} else {
				fmt.Fprintf(&buf, "%s: partition %d\n", string(key), result.ParentPartitionKey)
			}
		})
	}

	return buf.String()
}

func (s *testState) Insert(d *datadriven.TestData) string {
	hideTree := false
	count := 0
	for _, arg := range d.CmdArgs {
		switch arg.Key {
		case "load-features":
			count = s.parseInt(arg)

		case "hide-tree":
			hideTree = s.parseFlag(arg)
		}
	}

	vectors := vector.MakeSet(s.Quantizer.GetDims())
	childKeys := make([]cspann.ChildKey, 0, count)
	if count != 0 {
		// Load features.
		s.Features = testutils.LoadFeatures(s.T, 10000)
		vectors = s.Features
		vectors.SplitAt(count)
		for i := range count {
			key := cspann.KeyBytes(fmt.Sprintf("vec%d", i))
			childKeys = append(childKeys, cspann.ChildKey{KeyBytes: key})
		}
	} else {
		// Parse vectors.
		for _, line := range strings.Split(d.Input, "\n") {
			line = strings.TrimSpace(line)
			if len(line) == 0 {
				continue
			}
			parts := strings.Split(line, ":")
			require.Len(s.T, parts, 2)

			vectors.Add(s.parseVector(parts[1]))
			key := cspann.KeyBytes(parts[0])
			childKeys = append(childKeys, cspann.ChildKey{KeyBytes: key})
		}
	}

	var idxCtx cspann.Context
	var wait sync.WaitGroup
	step := (s.Options.MinPartitionSize + s.Options.MaxPartitionSize) / 2
	for i := range vectors.Count {
		// Insert within the scope of a transaction.
		commontest.RunTransaction(s.Ctx, s.T, s.MemStore, func(txn cspann.Txn) {
			idxCtx.Init(txn)
			s.MemStore.InsertVector(childKeys[i].KeyBytes, vectors.At(i))
			err := s.Index.Insert(s.Ctx, &idxCtx, s.TreeKey, vectors.At(i), childKeys[i].KeyBytes)
			require.NoError(s.T, err)
		})

		if (i+1)%step == 0 {
			// Run synchronous fixups so that test results are deterministic.
			s.processFixups()
		}
	}
	wait.Wait()

	// Handle any remaining fixups.
	s.processFixups()

	if hideTree {
		str := fmt.Sprintf("Created index with %d vectors with %d dimensions.\n",
			vectors.Count, vectors.Dims)
		return str + s.Index.FormatStats()
	}

	return s.FormatTree(d)
}

func (s *testState) Delete(d *datadriven.TestData) string {
	notFound := false
	for _, arg := range d.CmdArgs {
		switch arg.Key {
		case "not-found":
			notFound = s.parseFlag(arg)
		}
	}

	var idxCtx cspann.Context
	for i, line := range strings.Split(d.Input, "\n") {
		line = strings.TrimSpace(line)
		if len(line) == 0 {
			continue
		}

		key, vec := s.parseKeyAndVector(line)

		// Delete within the scope of a transaction.
		commontest.RunTransaction(s.Ctx, s.T, s.MemStore, func(txn cspann.Txn) {
			// If notFound=true, then simulate case where the vector is deleted in
			// the primary index, but it cannot be found in the secondary index.
			if !notFound {
				idxCtx.Init(txn)
				err := s.Index.Delete(s.Ctx, &idxCtx, s.TreeKey, vec, key)
				require.NoError(s.T, err)
			}
			s.MemStore.DeleteVector(key)
		})

		if (i+1)%s.Options.MaxPartitionSize == 0 {
			// Run synchronous fixups so that test results are deterministic.
			s.processFixups()
		}
	}

	// Handle any remaining fixups.
	s.processFixups()

	return s.FormatTree(d)
}

func (s *testState) ForceSplitOrMerge(d *datadriven.TestData) string {
	var parentPartitionKey, partitionKey cspann.PartitionKey
	var steps int
	for _, arg := range d.CmdArgs {
		switch arg.Key {
		case "parent-partition-key":
			parentPartitionKey = cspann.PartitionKey(s.parseInt(arg))

		case "partition-key":
			partitionKey = cspann.PartitionKey(s.parseInt(arg))

		case "steps":
			steps = s.parseInt(arg)
		}
	}

	// Perform N steps in a split or merge operation.
	for range max(steps, 1) {
		if d.Cmd == "force-split" {
			s.Index.ForceSplit(s.Ctx, s.TreeKey, parentPartitionKey, partitionKey, steps > 0)
		} else {
			s.Index.ForceMerge(s.Ctx, s.TreeKey, parentPartitionKey, partitionKey, steps > 0)
		}

		// Ensure the fixup runs.
		s.processFixups()
	}

	return s.FormatTree(d)
}

func (s *testState) Recall(d *datadriven.TestData) string {
	searchSet := cspann.SearchSet{MaxResults: 1}
	options := cspann.SearchOptions{}
	numSamples := 50
	var samples []int
	seed := 42
	var err error
	for _, arg := range d.CmdArgs {
		switch arg.Key {
		case "use-feature":
			// Use single designated sample.
			offset := s.parseInt(arg)
			numSamples = 1
			samples = []int{offset}

		case "samples":
			numSamples = s.parseInt(arg)

		case "seed":
			seed = s.parseInt(arg)

		case "topk":
			searchSet.MaxResults = s.parseInt(arg)

		case "beam-size":
			options.BaseBeamSize = s.parseInt(arg)
		}
	}

	data := s.MemStore.GetAllVectors()

	// Construct list of feature offsets.
	if samples == nil {
		// Shuffle the remaining features.
		rng := rand.New(rand.NewSource(int64(seed)))
		remaining := make([]int, s.Features.Count-len(data))
		for i := range remaining {
			remaining[i] = i
		}
		rng.Shuffle(len(remaining), func(i, j int) {
			remaining[i], remaining[j] = remaining[j], remaining[i]
		})

		// Pick numSamples randomly from the remaining set
		samples = make([]int, numSamples)
		copy(samples, remaining[:numSamples])
	}

	// calcTruth calculates the true nearest neighbors for the query vector.
	calcTruth := func(queryVector vector.T, data []cspann.VectorWithKey) []cspann.KeyBytes {
		distances := make([]float32, len(data))
		offsets := make([]int, len(data))
		for i := range len(data) {
			distances[i] = num32.L2SquaredDistance(queryVector, data[i].Vector)
			offsets[i] = i
		}
		sort.SliceStable(offsets, func(i int, j int) bool {
			res := cmp.Compare(distances[offsets[i]], distances[offsets[j]])
			if res != 0 {
				return res < 0
			}
			return data[offsets[i]].Key.Compare(data[offsets[j]].Key) < 0
		})

		truth := make([]cspann.KeyBytes, searchSet.MaxResults)
		for i := range len(truth) {
			truth[i] = data[offsets[i]].Key.KeyBytes
		}
		return truth
	}

	// Search for sampled features within a transaction.
	var sumMAP float64
	commontest.RunTransaction(s.Ctx, s.T, s.MemStore, func(txn cspann.Txn) {
		var idxCtx cspann.Context
		idxCtx.Init(txn)
		for i := range samples {
			// Calculate truth set for the vector.
			queryVector := s.Features.At(samples[i])
			truth := calcTruth(queryVector, data)

			// Calculate prediction set for the vector.
			err = s.Index.Search(s.Ctx, &idxCtx, s.TreeKey, queryVector, &searchSet, options)
			require.NoError(s.T, err)
			results := searchSet.PopResults()

			prediction := make([]cspann.KeyBytes, searchSet.MaxResults)
			for res := 0; res < len(results); res++ {
				prediction[res] = results[res].ChildKey.KeyBytes
			}

			sumMAP += findMAP(prediction, truth)
		}
	})

	recall := sumMAP / float64(numSamples) * 100
	quantizedLeafVectors := float64(searchSet.Stats.QuantizedLeafVectorCount) / float64(numSamples)
	quantizedVectors := float64(searchSet.Stats.QuantizedVectorCount) / float64(numSamples)
	fullVectors := float64(searchSet.Stats.FullVectorCount) / float64(numSamples)
	partitions := float64(searchSet.Stats.PartitionCount) / float64(numSamples)

	var buf bytes.Buffer
	buf.WriteString(fmt.Sprintf("%.2f%% recall@%d\n", recall, searchSet.MaxResults))
	buf.WriteString(fmt.Sprintf("%.2f leaf vectors, ", quantizedLeafVectors))
	buf.WriteString(fmt.Sprintf("%.2f vectors, ", quantizedVectors))
	buf.WriteString(fmt.Sprintf("%.2f full vectors, ", fullVectors))
	buf.WriteString(fmt.Sprintf("%.2f partitions", partitions))
	return buf.String()
}

func (s *testState) ValidateTree(d *datadriven.TestData) string {
	vectorCount := 0
	partitionKeys := []cspann.PartitionKey{cspann.RootKey}
	level := cspann.InvalidLevel
	for level != cspann.LeafLevel {
		// Get all child keys for next level.
		var childKeys []cspann.ChildKey
		for _, key := range partitionKeys {
			partition, err := s.MemStore.TryGetPartition(s.Ctx, s.TreeKey, key)
			require.NoError(s.T, err)
			level = partition.Level()
			childKeys = append(childKeys, partition.ChildKeys()...)
		}

		if len(childKeys) == 0 {
			break
		}

		// Verify full vectors exist for the level.
		commontest.RunTransaction(s.Ctx, s.T, s.MemStore, func(txn cspann.Txn) {
			refs := make([]cspann.VectorWithKey, len(childKeys))
			for i := range childKeys {
				refs[i].Key = childKeys[i]
			}
			err := txn.GetFullVectors(s.Ctx, s.TreeKey, refs)
			require.NoError(s.T, err)
			for i := range refs {
				require.NotNil(s.T, refs[i].Vector)
			}

			// If this is not the leaf level, then process the next level.
			if childKeys[0].KeyBytes == nil {
				partitionKeys = make([]cspann.PartitionKey, len(childKeys))
				for i := range childKeys {
					partitionKeys[i] = childKeys[i].PartitionKey
				}
			} else {
				// This is the leaf level, so count vectors and end.
				vectorCount += len(childKeys)
			}
		})
	}

	return fmt.Sprintf("Validated index with %d vectors.\n", vectorCount)
}

func (s *testState) ShowMetrics(d *datadriven.TestData) string {
	var buf bytes.Buffer
	buf.WriteString(fmt.Sprintf("%d successful splits\n", s.SuccessfulSplits))
	buf.WriteString(fmt.Sprintf("%d pending splits/merges\n", s.PendingSplitsMerges))
	return buf.String()
}

func (s *testState) makeNewIndex(d *datadriven.TestData) {
	var err error
	dims := 2
	s.Options = cspann.IndexOptions{
		IsDeterministic: true,
		// Disable stalled op timeout, since it can interfere with stepping tests.
		StalledOpTimeout: func() time.Duration { return 0 },
	}
	for _, arg := range d.CmdArgs {
		switch arg.Key {
		case "min-partition-size":
			s.Options.MinPartitionSize = s.parseInt(arg)

		case "max-partition-size":
			s.Options.MaxPartitionSize = s.parseInt(arg)

		case "quality-samples":
			s.Options.QualitySamples = s.parseInt(arg)

		case "dims":
			dims = s.parseInt(arg)

		case "beam-size":
			s.Options.BaseBeamSize = s.parseInt(arg)
		}
	}

	const seed = 42
	s.Quantizer = quantize.NewRaBitQuantizer(dims, seed)
	s.MemStore = memstore.New(s.Quantizer, seed)
	s.Index, err = cspann.NewIndex(s.Ctx, s.MemStore, s.Quantizer, seed, &s.Options, s.Stopper)
	require.NoError(s.T, err)

	s.Index.Fixups().OnSuccessfulSplit(func() { s.SuccessfulSplits++ })
	s.Index.Fixups().OnPendingSplitsMerges(func(count int) { s.PendingSplitsMerges = count })

	// Suspend background fixups until ProcessFixups is explicitly called, so
	// that vector index operations can be deterministic.
	s.Index.SuspendFixups()
}

func (s *testState) processFixups() {
	if s.Index != nil {
		if s.DiscardFixups {
			s.Index.DiscardFixups()
		} else if !s.SkipFixups {
			s.Index.ProcessFixups()
		}
	}
}

func (s *testState) parseInt(arg datadriven.CmdArg) int {
	require.Len(s.T, arg.Vals, 1)
	val, err := strconv.Atoi(arg.Vals[0])
	require.NoError(s.T, err)
	return val
}

func (s *testState) parseFlag(arg datadriven.CmdArg) bool {
	require.Len(s.T, arg.Vals, 0)
	return true
}

func (s *testState) parseTreeID(arg datadriven.CmdArg) cspann.TreeKey {
	return memstore.ToTreeKey(memstore.TreeID(s.parseInt(arg)))
}

func (s *testState) parseUseFeature(arg datadriven.CmdArg) vector.T {
	return s.Features.At(s.parseInt(arg))
}

// parseVector parses a vector string in this form: (1.5, 6, -4).
func (s *testState) parseVector(str string) vector.T {
	// Remove parentheses and split by commas.
	str = strings.TrimSpace(str)
	str = strings.TrimPrefix(str, "(")
	str = strings.TrimSuffix(str, ")")
	elems := strings.Split(str, ",")

	// Construct the vector.
	vector := make(vector.T, len(elems))
	for i, elem := range elems {
		elem = strings.TrimSpace(elem)
		value, err := strconv.ParseFloat(elem, 32)
		require.NoError(s.T, err)
		vector[i] = float32(value)
	}

	return vector
}

// parseKeyAndVector parses a line that may contain a key and vector separated
// by a colon. If there's no colon, it treats the line as just a key and gets
// the vector from the store.
func (s *testState) parseKeyAndVector(line string) (cspann.KeyBytes, vector.T) {
	parts := strings.Split(line, ":")
	if len(parts) == 1 {
		// Get the value from the store.
		key := cspann.KeyBytes(line)
		return key, s.MemStore.GetVector(key)
	}

	// Parse the value after the colon.
	require.Len(s.T, parts, 2)
	key := cspann.KeyBytes(parts[0])
	return key, s.parseVector(parts[1])
}

func (s *testState) loadIndexFromFormat(
	treeKey cspann.TreeKey, lines []string, indent int,
) (remaining []string, level cspann.Level, centroid vector.T, childKey cspann.ChildKey) {
	// Ensure line contains "• ", note that the '•' rune is 3 UTF-8 bytes.
	idx := strings.Index(lines[0], "• ")
	require.NotEqual(s.T, -1, idx)
	line := lines[0][idx+4:]
	idx = strings.Index(line, " ")

	if line[0] < '0' || line[0] > '9' {
		// This is a leaf vector.
		keyBytes := []byte(strings.TrimSpace(line[:idx]))
		line = strings.TrimSpace(line[idx:])
		vec := s.parseVector(line)
		s.MemStore.InsertVector(keyBytes, vec)
		randomized := s.Index.RandomizeVector(vec, make(vector.T, len(vec)))
		return lines[1:], cspann.LeafLevel, randomized, cspann.ChildKey{KeyBytes: keyBytes}
	}

	// This is an interior partition.
	val, err := strconv.Atoi(line[:idx])
	require.NoError(s.T, err)
	partitionKey := cspann.PartitionKey(val)
	s.MemStore.EnsureUniquePartitionKey(treeKey, partitionKey)
	line = line[idx:]

	// Parse centroid and state.
	details := cspann.MakeReadyDetails()
	idx = strings.Index(line, "[")
	if idx != -1 {
		require.True(s.T, strings.HasSuffix(line, "]"))
		details = parsePartitionStateDetails(line[idx+1 : len(line)-1])
		line = line[:idx-1]
	}
	centroid = s.parseVector(line)
	centroid = s.Index.RandomizeVector(centroid, make(vector.T, len(centroid)))

	lines = lines[1:]

	childLevel := cspann.LeafLevel
	childVectors := vector.MakeSet(len(centroid))
	childKeys := []cspann.ChildKey(nil)
	childIndent := 0

	// Loop over children.
	for len(lines) > 0 && len(lines[0]) > indent {
		if strings.HasSuffix(lines[0], "│") {
			childIndent = len(lines[0])
			lines = lines[1:]
		}
		require.Greater(s.T, childIndent, 0)

		var childVector vector.T
		lines, childLevel, childVector, childKey = s.loadIndexFromFormat(treeKey, lines, childIndent)
		childVectors.Add(childVector)
		childKeys = append(childKeys, childKey)
	}

	metadata := cspann.PartitionMetadata{
		Level:        childLevel,
		Centroid:     centroid,
		StateDetails: details,
	}
	err = s.MemStore.TryCreateEmptyPartition(s.Ctx, treeKey, partitionKey, metadata)
	require.NoError(s.T, err)

	if len(childKeys) > 0 {
		valueBytes := make([]cspann.ValueBytes, len(childKeys))
		added, err := s.MemStore.TryAddToPartition(
			s.Ctx, treeKey, partitionKey, childVectors, childKeys, valueBytes, metadata)
		require.NoError(s.T, err)
		require.True(s.T, added)
	}

	return lines, childLevel + 1, centroid, cspann.ChildKey{PartitionKey: partitionKey}
}

// parsePartitionStateDetails parses a partition state details string in this
// format:
//
//	Ready
//	DrainingForSplit:1,2
//	Merging:1
func parsePartitionStateDetails(s string) cspann.PartitionStateDetails {
	var details cspann.PartitionStateDetails
	idx := strings.Index(s, ":")
	if idx == -1 {
		details.State = cspann.ParsePartitionState(s)
		return details
	}
	details.State = cspann.ParsePartitionState(s[:idx])

	// Parse the partition keys after the colon.
	remaining := s[idx+1:]
	if comma := strings.Index(remaining, ","); comma != -1 {
		// Two keys means we're parsing targets.
		if num, err := strconv.Atoi(remaining[:comma]); err == nil {
			details.Target1 = cspann.PartitionKey(num)
		}
		if num, err := strconv.Atoi(remaining[comma+1:]); err == nil {
			details.Target2 = cspann.PartitionKey(num)
		}
	} else if num, err := strconv.Atoi(remaining); err == nil {
		// Has one parameter - set as Source for merge operations, Target1
		// otherwise.
		switch details.State {
		case cspann.MergingState, cspann.DrainingForMergeState, cspann.RemovingLevelState:
			details.Source = cspann.PartitionKey(num)
		default:
			details.Target1 = cspann.PartitionKey(num)
		}
	}

	return details
}

// findMAP returns mean average precision, which compares a set of predicted
// results with the true set of results. Both sets are expected to be of equal
// length. It returns the percentage overlap of the predicted set with the truth
// set.
func findMAP(prediction, truth []cspann.KeyBytes) float64 {
	if len(prediction) != len(truth) {
		panic(errors.AssertionFailedf("prediction and truth sets are not same length"))
	}

	predictionMap := make(map[string]bool, len(prediction))
	for _, p := range prediction {
		predictionMap[string(p)] = true
	}

	var intersect float64
	for _, t := range truth {
		_, ok := predictionMap[string(t)]
		if ok {
			intersect++
		}
	}
	return intersect / float64(len(truth))
}

func TestRandomizeVector(t *testing.T) {
	defer leaktest.AfterTest(t)()
	defer log.Scope(t).Close(t)

	// Create index.
	var workspace workspace.T
	ctx := context.Background()
	stopper := stop.NewStopper()
	defer stopper.Stop(ctx)

	const dims = 97
	const count = 5
	quantizer := quantize.NewRaBitQuantizer(dims, 46)
	inMemStore := memstore.New(quantizer, 42)
	index, err := cspann.NewIndex(ctx, inMemStore, quantizer, 42, &cspann.IndexOptions{}, stopper)
	require.NoError(t, err)

	// Generate random vectors with exponentially increasing norms, in order
	// make distances more distinct.
	rng := rand.New(rand.NewSource(42))
	data := make([]float32, dims*count)
	for i := range data {
		vecIdx := float64(i / dims)
		data[i] = float32(rng.NormFloat64() * math.Pow(1.5, vecIdx))
	}

	original := vector.MakeSetFromRawData(data, dims)
	randomized := vector.MakeSet(dims)
	randomized.AddUndefined(count)
	for i := range original.Count {
		index.RandomizeVector(original.At(i), randomized.At(i))

		// Ensure that calling unRandomizeVector recovers original vector.
		randomizedInv := make([]float32, dims)
		index.UnRandomizeVector(randomized.At(i), randomizedInv)
		for j, val := range original.At(i) {
			require.InDelta(t, val, randomizedInv[j], 0.00001)
		}
	}

	// Ensure that distances are similar, whether using the original vectors or
	// the randomized vectors.
	originalSet := quantizer.Quantize(&workspace, original).(*quantize.RaBitQuantizedVectorSet)
	randomizedSet := quantizer.Quantize(&workspace, randomized).(*quantize.RaBitQuantizedVectorSet)

	distances := make([]float32, count)
	errorBounds := make([]float32, count)
	quantizer.EstimateSquaredDistances(&workspace, originalSet, original.At(0), distances, errorBounds)
	require.Equal(t, []float32{0, 272.75, 550.86, 950.93, 2421.41}, testutils.RoundFloats(distances, 2))
	require.Equal(t, []float32{37.58, 46.08, 57.55, 69.46, 110.57}, testutils.RoundFloats(errorBounds, 2))

	quantizer.EstimateSquaredDistances(&workspace, randomizedSet, randomized.At(0), distances, errorBounds)
	require.Equal(t, []float32{5.1, 292.72, 454.95, 1011.85, 2475.87}, testutils.RoundFloats(distances, 2))
	require.Equal(t, []float32{37.58, 46.08, 57.55, 69.46, 110.57}, testutils.RoundFloats(errorBounds, 2))
}

// TestIndexConcurrency builds an index on multiple goroutines, with background
// splits and merges enabled.
func TestIndexConcurrency(t *testing.T) {
	defer leaktest.AfterTest(t)()
	defer log.Scope(t).Close(t)

	// Create index.
	ctx := context.Background()
	stopper := stop.NewStopper()
	defer stopper.Stop(ctx)

	// Load features.
	features := testutils.LoadFeatures(t, 1000)

	// Trim feature dimensions from 512 to 4, in order to make the test run
	// faster and hit more interesting concurrency combinations.
	const dims = 4
	vectors := vector.MakeSet(dims)

	primaryKeys := make([]cspann.KeyBytes, features.Count)
	for i := range features.Count {
		primaryKeys[i] = cspann.KeyBytes(fmt.Sprintf("vec%d", i))
		vectors.Add(features.At(i)[:dims])
	}

	for i := range 10 {
		log.Infof(ctx, "iteration %d", i)

		options := cspann.IndexOptions{
			MinPartitionSize: 2,
			MaxPartitionSize: 8,
			BaseBeamSize:     2,
			QualitySamples:   4,
		}
		seed := int64(i)
		quantizer := quantize.NewRaBitQuantizer(vectors.Dims, seed)
		store := memstore.New(quantizer, seed)
		index, err := cspann.NewIndex(ctx, store, quantizer, seed, &options, stopper)
		require.NoError(t, err)

		buildIndex(ctx, t, store, index, vectors, primaryKeys)

		vectorCount := validateIndex(ctx, t, store)
		require.Equal(t, vectors.Count, vectorCount)

		index.Close()
	}
}

func buildIndex(
	ctx context.Context,
	t *testing.T,
	store *memstore.Store,
	index *cspann.Index,
	vectors vector.Set,
	primaryKeys []cspann.KeyBytes,
) {
	var insertCount atomic.Uint64

	// Insert block of vectors within the scope of a transaction.
	insertBlock := func(idxCtx *cspann.Context, start, end int) {
		for i := start; i < end; i++ {
			commontest.RunTransaction(ctx, t, store, func(txn cspann.Txn) {
				idxCtx.Init(txn)
				store.InsertVector(primaryKeys[i], vectors.At(i))
				require.NoError(t,
					index.Insert(ctx, idxCtx, nil /* treeKey */, vectors.At(i), primaryKeys[i]))
			})
			insertCount.Add(1)
		}
	}

	// Insert vectors into the store on multiple goroutines.
	var wait sync.WaitGroup
	procs := runtime.GOMAXPROCS(-1)
	countPerProc := (vectors.Count + procs) / procs
	blockSize := index.Options().MinPartitionSize
	for i := 0; i < vectors.Count; i += countPerProc {
		end := min(i+countPerProc, vectors.Count)
		wait.Add(1)
		go func(start, end int) {
			defer wait.Done()

			// Break vector group into individual transactions that each insert a
			// block of vectors. Run any pending fixups after each block.
			var idxCtx cspann.Context
			for j := start; j < end; j += blockSize {
				insertBlock(&idxCtx, j, min(j+blockSize, end))
				index.ProcessFixups()
			}
		}(i, end)
	}

	info := log.Every(time.Second)
	for int(insertCount.Load()) < vectors.Count {
		time.Sleep(10 * time.Millisecond)

		if info.ShouldLog() {
			log.Infof(ctx, "%d vectors inserted", insertCount.Load())
		}

		// Fail on foreground goroutine if any background goroutines failed.
		if t.Failed() {
			t.FailNow()
		}
	}

	wait.Wait()

	// Process any remaining fixups.
	index.ProcessFixups()

	log.Infof(ctx, "%d vectors inserted", vectors.Count)
}

func validateIndex(ctx context.Context, t *testing.T, store *memstore.Store) int {
	treeKey := memstore.ToTreeKey(memstore.TreeID(0))

	// Duplicate vectors are possible in the index, so the total count of vectors
	// is not deterministc. Instead, return the number of unique vectors, which
	// should be stable.
	var deDup cspann.ChildKeyDeDup
	deDup.Init(1000)

	partitionKeys := []cspann.PartitionKey{cspann.RootKey}
	for {
		// Get all child keys for next level.
		var childKeys []cspann.ChildKey
		for _, key := range partitionKeys {
			partition, err := store.TryGetPartition(ctx, treeKey, key)
			if !errors.Is(err, cspann.ErrPartitionNotFound) {
				// Ignore ErrPartitionNotFound, as splits can cause dangling
				// partition keys.
				require.NoError(t, err)
				childKeys = append(childKeys, partition.ChildKeys()...)
			}
		}

		if len(childKeys) == 0 {
			break
		}

		if childKeys[0].KeyBytes != nil {
			// This is the leaf level, so verify that full vectors exist. Count
			// the number of unique vectors.
			commontest.RunTransaction(ctx, t, store, func(txn cspann.Txn) {
				refs := make([]cspann.VectorWithKey, len(childKeys))
				for i := range childKeys {
					refs[i].Key = childKeys[i]
					deDup.TryAdd(childKeys[i])
				}
				err := txn.GetFullVectors(ctx, treeKey, refs)
				require.NoError(t, err)
				for i := range refs {
					if refs[i].Vector == nil {
						panic("vector is nil")
					}
					require.NotNil(t, refs[i].Vector)
				}
			})

			// No more levels to process.
			break
		}

		// This is not the leaf level, so process the next level.
		partitionKeys = make([]cspann.PartitionKey, len(childKeys))
		for i := range childKeys {
			partitionKeys[i] = childKeys[i].PartitionKey
		}
	}

	return deDup.Count()
}
