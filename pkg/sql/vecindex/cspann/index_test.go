// Copyright 2024 The Cockroach Authors.
//
// Use of this software is governed by the CockroachDB Software License
// included in the /LICENSE file.

package cspann_test

import (
	"bytes"
	"context"
	"fmt"
	"math"
	"math/rand"
	"regexp"
	"strconv"
	"strings"
	"sync"
	"testing"
	"time"
	"unicode/utf8"

	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann"
	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann/commontest"
	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann/memstore"
	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann/quantize"
	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann/testutils"
	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann/utils"
	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/cspann/workspace"
	"github.com/cockroachdb/cockroach/pkg/sql/vecindex/vecpb"
	"github.com/cockroachdb/cockroach/pkg/util/leaktest"
	"github.com/cockroachdb/cockroach/pkg/util/log"
	"github.com/cockroachdb/cockroach/pkg/util/stop"
	"github.com/cockroachdb/cockroach/pkg/util/syncutil"
	"github.com/cockroachdb/cockroach/pkg/util/vector"
	"github.com/cockroachdb/datadriven"
	"github.com/cockroachdb/errors"
	"github.com/stretchr/testify/require"
	"golang.org/x/exp/slices"
)

func TestDatadrivenIndex(t *testing.T) {
	defer leaktest.AfterTest(t)()
	defer log.Scope(t).Close(t)

	ctx := context.Background()
	ti := &testIndex{ctx: ctx}
	ti.IndexTestState = commontest.NewIndexTestState(t, ti)
	ti.stopper = stop.NewStopper()
	defer ti.stopper.Stop(ctx)

	datadriven.Walk(t, "testdata", func(t *testing.T, path string) {
		if regexp.MustCompile("/.+/").MatchString(path) {
			// Skip files that are in subdirs.
			return
		}
		if !strings.HasSuffix(path, ".ddt") {
			// Skip files that are not data-driven tests.
			return
		}
		ti.reset()

		datadriven.RunTest(t, path, func(t *testing.T, d *datadriven.TestData) string {
			// Process args supported by every command.
			ti.treeKey = nil
			ti.discardFixups = false
			ti.skipFixups = false
			for _, arg := range d.CmdArgs {
				switch arg.Key {
				case "tree":
					// K-means tree that the command operates on.
					ti.treeKey = ti.parseTreeID(arg)

				case "discard-fixups":
					// Discard any fixups generated by the command.
					ti.discardFixups = testutils.ParseDataDrivenFlag(ti.T, arg)

				case "skip-fixups":
					// Skip processing of any fixups generated by the command.
					ti.skipFixups = testutils.ParseDataDrivenFlag(ti.T, arg)
				}
			}

			var result string
			switch d.Cmd {
			case "new-index":
				return ti.NewIndex(d)

			case "load-index":
				result = ti.LoadIndex(d)

			case "format-tree":
				return ti.FormatTree(ctx, d, ti.treeKey)

			case "search":
				result = ti.Search(d)

			case "search-for-insert":
				result = ti.SearchForInsert(d)

			case "search-for-delete":
				result = ti.SearchForDelete(d)

			case "insert":
				return ti.Insert(d)

			case "delete":
				result = ti.Delete(d)

			case "force-split", "force-merge":
				result = ti.ForceSplitOrMerge(d)

			case "recall":
				result = ti.Recall(d)

			case "best-centroids":
				result = ti.BestCentroids(d)

			case "validate-tree":
				result = ti.ValidateTree(d)

			case "metrics":
				result = ti.ShowMetrics(d)

			default:
				t.Fatalf("unknown cmd: %s", d.Cmd)
			}

			// Process or discard fixups generated by the command.
			ti.processFixups()

			return result
		})
	})
}

// testIndex implements the commontest.TestIndex interface.
type testIndex struct {
	*commontest.IndexTestState

	ctx         context.Context
	stopper     *stop.Stopper
	options     *cspann.IndexOptions
	quantizer   quantize.Quantizer
	memStore    *memstore.Store
	searchStats cspann.SearchStats

	// Parse args.
	treeKey       cspann.TreeKey
	discardFixups bool
	skipFixups    bool

	// Metrics
	successfulSplits    int
	pendingSplitsMerges int
}

// MakeNewIndex implements the commontest.IndexTestState interface.
func (ti *testIndex) MakeNewIndex(
	ctx context.Context, dims int, metric vecpb.DistanceMetric, options *cspann.IndexOptions,
) *cspann.Index {
	const seed = 42
	ti.options = options
	ti.quantizer = quantize.NewRaBitQuantizer(dims, seed, metric)
	ti.memStore = memstore.New(ti.quantizer, seed)
	index, err := cspann.NewIndex(ctx, ti.memStore, ti.quantizer, seed, options, ti.stopper)
	require.NoError(ti.T, err)

	index.Fixups().OnSuccessfulSplit(func() { ti.successfulSplits++ })
	index.Fixups().OnPendingSplitsMerges(func(count int) { ti.pendingSplitsMerges = count })

	return index
}

// InsertVectors implements the commontest.IndexTestState interface.
func (ti *testIndex) InsertVectors(
	ctx context.Context, treeKey cspann.TreeKey, keys []string, vectors vector.Set,
) {
	var idxCtx cspann.Context
	var wait sync.WaitGroup
	step := (ti.options.MinPartitionSize + ti.options.MaxPartitionSize) / 2
	for i := range vectors.Count {
		// Insert within the scope of a transaction.
		commontest.RunTransaction(ctx, ti.T, ti.memStore, func(txn cspann.Txn) {
			idxCtx.Init(txn)
			keyBytes := []byte(keys[i])
			ti.memStore.InsertVector(keyBytes, vectors.At(i))
			err := ti.Index.Insert(ctx, &idxCtx, treeKey, vectors.At(i), keyBytes)
			require.NoError(ti.T, err)
		})

		if (i+1)%step == 0 {
			// Run synchronous fixups so that test results are deterministic.
			ti.processFixups()
		}
	}
	wait.Wait()

	// Handle any remaining fixups.
	ti.processFixups()
}

// SearchVectors implements the commontest.IndexTestState interface.
func (ti *testIndex) SearchVectors(
	ctx context.Context,
	treeKey cspann.TreeKey,
	queryVector vector.T,
	beamSize, topK, rerankMultiplier int,
) []string {
	prediction := make([]string, topK)
	commontest.RunTransaction(ctx, ti.T, ti.Index.Store(), func(txn cspann.Txn) {
		var idxCtx cspann.Context
		idxCtx.Init(txn)

		searchSet := cspann.SearchSet{MaxResults: topK}
		if rerankMultiplier == -1 {
			// Use same default as the vector_search_rerank_multiplier setting.
			rerankMultiplier = 50
		}
		searchSet.MaxResults, searchSet.MaxExtraResults =
			cspann.IncreaseRerankResults(beamSize, searchSet.MaxResults, rerankMultiplier)

		options := cspann.SearchOptions{BaseBeamSize: beamSize}
		err := ti.Index.Search(ctx, &idxCtx, treeKey, queryVector, &searchSet, options)
		require.NoError(ti.T, err)
		results := searchSet.PopResults()

		if len(results) > topK {
			// It's possible to have extra search results even when re-ranking,
			// when the last search result(s) have the same distance.
			results = results[:topK]
		}

		for res := range len(results) {
			prediction[res] = string(results[res].ChildKey.KeyBytes)
		}

		// Update stats.
		ti.searchStats.Add(&searchSet.Stats)
	})

	return prediction
}

func (ti *testIndex) NewIndex(d *datadriven.TestData) string {
	count := ti.IndexTestState.NewIndex(ti.ctx, d, ti.treeKey)
	if count != 0 {
		// Loaded a dataset.
		str := fmt.Sprintf("Created index with %d vectors with %d dimensions.\n",
			count, ti.quantizer.GetDims())
		return str + ti.Index.FormatStats()
	}

	// Insert initial vectors.
	return ti.Insert(d)
}

func (ti *testIndex) LoadIndex(d *datadriven.TestData) string {
	count := ti.IndexTestState.NewIndex(ti.ctx, d, ti.treeKey)
	require.Zero(ti.T, count, "LoadIndex should not be used with the dataset option")

	lines := strings.Split(d.Input, "\n")

	// Determine the root level by determining max indent.
	maxIndent := 0
	maxPartitionKey := cspann.RootKey
	for _, line := range lines {
		// Lines ending with the "│" rune don't increase the level
		if strings.HasSuffix(line, "│") {
			continue
		}

		// Leaf vector lines don't increase the level.
		idx := strings.Index(line, "• ")
		if idx != -1 {
			// Note that the '•' rune is 3 UTF-8 bytes.
			start := idx + 4
			firstChar := line[start]
			if firstChar < '0' || firstChar > '9' {
				// This is a leaf vector.
				continue
			}

			// This is an interior partition, so keep track of the max partition key.
			end := start + strings.Index(line[start:], " ")
			val, err := strconv.Atoi(line[start:end])
			require.NoError(ti.T, err)
			if cspann.PartitionKey(val) > maxPartitionKey {
				maxPartitionKey = cspann.PartitionKey(val)
			}
		}

		maxIndent = max(maxIndent, getIndexFormatIndent(line))
	}

	// Ensure that index will not reuse any loaded partition key.
	ti.memStore.SetMinPartitionKey(maxPartitionKey + 1)

	// Load all the interior and leaf vectors.
	ti.loadIndexFromFormat(ti.treeKey, lines, cspann.Level(maxIndent+1))

	return fmt.Sprintf("Loaded %d vectors.\n", len(ti.memStore.GetAllVectors()))
}

func (ti *testIndex) Search(d *datadriven.TestData) string {
	var vec vector.T
	searchSet := cspann.SearchSet{MaxResults: 1}
	options := cspann.SearchOptions{}
	rerankMultiplier := -1

	for _, arg := range d.CmdArgs {
		switch arg.Key {
		case "use-dataset":
			vec = ti.parseUseDataset(arg)

		case "max-results":
			searchSet.MaxResults = testutils.ParseDataDrivenInt(ti.T, arg)

		case "beam-size":
			options.BaseBeamSize = testutils.ParseDataDrivenInt(ti.T, arg)

		case "skip-rerank":
			options.SkipRerank = testutils.ParseDataDrivenFlag(ti.T, arg)

		case "rerank-multiplier":
			rerankMultiplier = testutils.ParseDataDrivenInt(ti.T, arg)
		}
	}

	// If re-ranking results, make sure there are enough extra results to do that
	// effectively.
	if !options.SkipRerank {
		if rerankMultiplier >= 0 {
			searchSet.MaxResults, searchSet.MaxExtraResults =
				cspann.IncreaseRerankResults(options.BaseBeamSize, searchSet.MaxResults, rerankMultiplier)
		} else {
			searchSet.MaxExtraResults = searchSet.MaxResults * 10
		}
	}

	if vec == nil {
		// Parse input as the vector to search for.
		vec = ti.parseVector(d.Input)
	}

	// Search the index within a transaction.
	commontest.RunTransaction(ti.ctx, ti.T, ti.memStore, func(txn cspann.Txn) {
		var idxCtx cspann.Context
		idxCtx.Init(txn)
		err := ti.Index.Search(ti.ctx, &idxCtx, ti.treeKey, vec, &searchSet, options)
		require.NoError(ti.T, err)
	})

	var buf bytes.Buffer
	results := searchSet.PopResults()
	for i := range results {
		result := &results[i]
		var errorBound string
		if result.ErrorBound != 0 {
			errorBound = fmt.Sprintf(" ± %s", utils.FormatFloat(result.ErrorBound, 2))
		}
		fmt.Fprintf(&buf, "%s: %s%s\n",
			string(result.ChildKey.KeyBytes), utils.FormatFloat(result.QueryDistance, 4), errorBound)
	}

	buf.WriteString(fmt.Sprintf("%d leaf vectors, ", searchSet.Stats.QuantizedLeafVectorCount))
	buf.WriteString(fmt.Sprintf("%d vectors, ", searchSet.Stats.QuantizedVectorCount))
	buf.WriteString(fmt.Sprintf("%d full vectors, ", searchSet.Stats.FullVectorCount))
	buf.WriteString(fmt.Sprintf("%d partitions", searchSet.Stats.PartitionCount))

	return buf.String()
}

func (ti *testIndex) SearchForInsert(d *datadriven.TestData) string {
	var vec vector.T

	for _, arg := range d.CmdArgs {
		switch arg.Key {
		case "use-dataset":
			vec = ti.parseUseDataset(arg)
		}
	}

	if vec == nil {
		// Parse input as the vector to search for.
		vec = ti.parseVector(d.Input)
	}

	// Search the index within a transaction.
	var result *cspann.SearchResult
	commontest.RunTransaction(ti.ctx, ti.T, ti.memStore, func(txn cspann.Txn) {
		var idxCtx cspann.Context
		idxCtx.Init(txn)
		var err error
		result, err = ti.Index.SearchForInsert(ti.ctx, &idxCtx, ti.treeKey, vec)
		require.NoError(ti.T, err)
	})

	var buf bytes.Buffer
	fmt.Fprintf(&buf, "partition %d, centroid=", result.ChildKey.PartitionKey)

	// Un-randomize the centroid and write it to buffer.
	original := make(vector.T, len(result.Vector))
	ti.Index.UnRandomizeVector(result.Vector, original)
	utils.WriteVector(&buf, original, 4)

	fmt.Fprintf(&buf, ", dist=%s", utils.FormatFloat(result.QueryDistance, 4))
	if result.ErrorBound != 0 {
		fmt.Fprintf(&buf, "±%s", utils.FormatFloat(result.ErrorBound, 2))
	}
	buf.WriteByte('\n')

	return buf.String()
}

func (ti *testIndex) SearchForDelete(d *datadriven.TestData) string {
	var buf bytes.Buffer

	var idxCtx cspann.Context
	for _, line := range strings.Split(d.Input, "\n") {
		line = strings.TrimSpace(line)
		if len(line) == 0 {
			continue
		}

		key, vec := ti.parseKeyAndVector(line)

		// Search within a transaction.
		commontest.RunTransaction(ti.ctx, ti.T, ti.memStore, func(txn cspann.Txn) {
			idxCtx.Init(txn)
			result, err := ti.Index.SearchForDelete(ti.ctx, &idxCtx, ti.treeKey, vec, key)
			require.NoError(ti.T, err)

			if result == nil {
				fmt.Fprintf(&buf, "%s: vector not found\n", string(key))
			} else {
				fmt.Fprintf(&buf, "%s: partition %d\n", string(key), result.ParentPartitionKey)
			}
		})
	}

	return buf.String()
}

func (ti *testIndex) Insert(d *datadriven.TestData) string {
	ti.IndexTestState.Insert(ti.ctx, d, ti.treeKey)
	return ti.IndexTestState.FormatTree(ti.ctx, d, ti.treeKey)
}

func (ti *testIndex) Delete(d *datadriven.TestData) string {
	notFound := false
	for _, arg := range d.CmdArgs {
		switch arg.Key {
		case "not-found":
			notFound = testutils.ParseDataDrivenFlag(ti.T, arg)
		}
	}

	var idxCtx cspann.Context
	for i, line := range strings.Split(d.Input, "\n") {
		line = strings.TrimSpace(line)
		if len(line) == 0 {
			continue
		}

		key, vec := ti.parseKeyAndVector(line)
		require.NotNil(ti.T, vec)

		// Delete within the scope of a transaction.
		commontest.RunTransaction(ti.ctx, ti.T, ti.memStore, func(txn cspann.Txn) {
			// If notFound=true, then simulate case where the vector is deleted in
			// the primary index, but it cannot be found in the secondary index.
			if !notFound {
				idxCtx.Init(txn)
				_, err := ti.Index.Delete(ti.ctx, &idxCtx, ti.treeKey, vec, key)
				require.NoError(ti.T, err)
			}
			ti.memStore.DeleteVector(key)
		})

		if (i+1)%ti.options.MaxPartitionSize == 0 {
			// Run synchronous fixups so that test results are deterministic.
			ti.processFixups()
		}
	}

	// Handle any remaining fixups.
	ti.processFixups()

	return ti.FormatTree(ti.ctx, d, ti.treeKey)
}

func (ti *testIndex) ForceSplitOrMerge(d *datadriven.TestData) string {
	var parentPartitionKey, partitionKey cspann.PartitionKey
	var steps int
	for _, arg := range d.CmdArgs {
		switch arg.Key {
		case "parent-partition-key":
			val := testutils.ParseDataDrivenInt(ti.T, arg)
			parentPartitionKey = cspann.PartitionKey(val)

		case "partition-key":
			val := testutils.ParseDataDrivenInt(ti.T, arg)
			partitionKey = cspann.PartitionKey(val)

		case "steps":
			steps = testutils.ParseDataDrivenInt(ti.T, arg)
		}
	}

	// Perform N steps in a split or merge operation.
	for range max(steps, 1) {
		if d.Cmd == "force-split" {
			ti.Index.ForceSplit(ti.ctx, ti.treeKey, parentPartitionKey, partitionKey, steps > 0)
		} else {
			ti.Index.ForceMerge(ti.ctx, ti.treeKey, parentPartitionKey, partitionKey, steps > 0)
		}

		// Ensure the fixup runs.
		ti.processFixups()
	}

	return ti.FormatTree(ti.ctx, d, ti.treeKey)
}

func (ti *testIndex) Recall(d *datadriven.TestData) string {
	// Reset the search stats.
	ti.searchStats = cspann.SearchStats{}

	// Now run the recall test, which will update the search set via calls to
	// SearchVectors.
	topK, numSamples, recall := ti.IndexTestState.Recall(ti.ctx, d, ti.treeKey)

	// Compute stats.
	quantizedLeafVectors := float64(ti.searchStats.QuantizedLeafVectorCount) / float64(numSamples)
	quantizedVectors := float64(ti.searchStats.QuantizedVectorCount) / float64(numSamples)
	fullVectors := float64(ti.searchStats.FullVectorCount) / float64(numSamples)
	partitions := float64(ti.searchStats.PartitionCount) / float64(numSamples)

	var buf bytes.Buffer
	buf.WriteString(fmt.Sprintf("%.2f%% recall@%d\n", recall, topK))
	buf.WriteString(fmt.Sprintf("%.0f leaf vectors, ", quantizedLeafVectors))
	buf.WriteString(fmt.Sprintf("%.0f vectors, ", quantizedVectors))
	buf.WriteString(fmt.Sprintf("%.0f full vectors, ", fullVectors))
	buf.WriteString(fmt.Sprintf("%.0f partitions", partitions))
	return buf.String()
}

func (ti *testIndex) BestCentroids(d *datadriven.TestData) string {
	randomized := make(vector.T, ti.Dataset.Dims)
	topk := 10
	for _, arg := range d.CmdArgs {
		switch arg.Key {
		case "use-dataset":
			original := ti.parseUseDataset(arg)
			ti.Index.TransformVector(original, randomized)

		case "topk":
			topk = testutils.ParseDataDrivenInt(ti.T, arg)
		}
	}

	var w workspace.T
	var distances, errorBounds []float32
	var partitionKeys []cspann.PartitionKey

	var findCentroids func(partitionKey cspann.PartitionKey)
	findCentroids = func(partitionKey cspann.PartitionKey) {
		partition, err := ti.memStore.TryGetPartition(ti.ctx, ti.treeKey, partitionKey)
		require.NoError(ti.T, err)
		count := partition.Count()

		switch partition.Level() {
		case cspann.LeafLevel:
			// Nothing to do.

		case cspann.SecondLevel:
			distances = slices.Grow(distances, count)
			distances = distances[:len(distances)+count]
			errorBounds = slices.Grow(errorBounds, count)
			errorBounds = errorBounds[:len(errorBounds)+count]

			partition.Quantizer().EstimateDistances(&w, partition.QuantizedSet(), randomized,
				distances[len(distances)-count:],
				errorBounds[len(errorBounds)-count:])

			for _, key := range partition.ChildKeys() {
				partitionKeys = append(partitionKeys, key.PartitionKey)
			}

		default:
			// Descend to next level.
			for _, key := range partition.ChildKeys() {
				findCentroids(key.PartitionKey)
			}
		}
	}

	findCentroids(cspann.RootKey)

	// Create offsets for argsort.
	offsets := make([]int, len(partitionKeys))
	for i := range offsets {
		offsets[i] = i
	}

	// Sort indices by distance (argsort).
	slices.SortFunc(offsets, func(a, b int) int {
		if distances[a] < distances[b] {
			return -1
		} else if distances[a] > distances[b] {
			return 1
		}
		return 0
	})

	// Print top results.
	var buf strings.Builder
	for i := range min(topk, len(offsets)) {
		offset := offsets[i]

		partition, err := ti.memStore.TryGetPartition(ti.ctx, ti.treeKey, partitionKeys[offset])
		require.NoError(ti.T, err)
		exact := vecpb.MeasureDistance(vecpb.L2SquaredDistance, randomized, partition.Centroid())

		fmt.Fprintf(&buf, "%d: %.4f ± %.4f (exact=%.4f)\n",
			partitionKeys[offset], distances[offset], errorBounds[offset], exact)
	}

	return buf.String()
}

func (ti *testIndex) ValidateTree(d *datadriven.TestData) string {
	vectorCount := 0
	partitionKeys := []cspann.PartitionKey{cspann.RootKey}
	level := cspann.InvalidLevel
	for level != cspann.LeafLevel {
		// Get all child keys for next level.
		var childKeys []cspann.ChildKey
		for _, key := range partitionKeys {
			partition, err := ti.memStore.TryGetPartition(ti.ctx, ti.treeKey, key)
			require.NoError(ti.T, err)
			level = partition.Level()
			childKeys = append(childKeys, partition.ChildKeys()...)
		}

		if len(childKeys) == 0 {
			break
		}

		// Verify full vectors exist for the level.
		commontest.RunTransaction(ti.ctx, ti.T, ti.memStore, func(txn cspann.Txn) {
			refs := make([]cspann.VectorWithKey, len(childKeys))
			for i := range childKeys {
				refs[i].Key = childKeys[i]
			}
			err := txn.GetFullVectors(ti.ctx, ti.treeKey, refs)
			require.NoError(ti.T, err)
			for i := range refs {
				require.NotNil(ti.T, refs[i].Vector)
			}

			// If this is not the leaf level, then process the next level.
			if childKeys[0].KeyBytes == nil {
				partitionKeys = make([]cspann.PartitionKey, len(childKeys))
				for i := range childKeys {
					partitionKeys[i] = childKeys[i].PartitionKey
				}
			} else {
				// This is the leaf level, so count vectors and end.
				vectorCount += len(childKeys)
			}
		})
	}

	return fmt.Sprintf("Validated index with %d vectors.\n", vectorCount)
}

func (s *testIndex) ShowMetrics(d *datadriven.TestData) string {
	var buf bytes.Buffer
	buf.WriteString(fmt.Sprintf("%d successful splits\n", s.successfulSplits))
	buf.WriteString(fmt.Sprintf("%d pending splits/merges\n", s.pendingSplitsMerges))
	return buf.String()
}

func (ti *testIndex) loadIndexFromFormat(
	treeKey cspann.TreeKey, lines []string, level cspann.Level,
) (remaining []string, centroid vector.T, childKey cspann.ChildKey) {
	// Ensure line contains "• ", note that the '•' rune is 3 UTF-8 bytes.
	idx := strings.Index(lines[0], "• ")
	require.NotEqual(ti.T, -1, idx)
	line := lines[0][idx+4:]
	idx = strings.Index(line, " ")

	if line[0] < '0' || line[0] > '9' {
		// This is a leaf vector.
		keyBytes := []byte(strings.TrimSpace(line[:idx]))
		line = strings.TrimSpace(line[idx:])
		vec := ti.parseVector(line)
		ti.memStore.InsertVector(keyBytes, vec)
		randomized := ti.Index.TransformVector(vec, make(vector.T, len(vec)))
		return lines[1:], randomized, cspann.ChildKey{KeyBytes: keyBytes}
	}

	// This is an interior partition.
	val, err := strconv.Atoi(line[:idx])
	require.NoError(ti.T, err)
	partitionKey := cspann.PartitionKey(val)
	//s.MemStore.EnsureUniquePartitionKey(treeKey, partitionKey)
	line = line[idx:]

	// Parse centroid and state.
	var details cspann.PartitionStateDetails
	details.MakeReady()
	idx = strings.Index(line, "(")
	if idx != -1 {
		require.True(ti.T, strings.HasSuffix(line, ")"))
		details = parsePartitionStateDetails(line[idx+1 : len(line)-1])
		line = line[:idx-1]
	}
	centroid = ti.parseVector(line)
	centroid = ti.Index.TransformVector(centroid, make(vector.T, len(centroid)))

	// Parse any children.
	parentIndent := getIndexFormatIndent(lines[0])
	lines = lines[1:]

	childVectors := vector.MakeSet(len(centroid))
	childKeys := []cspann.ChildKey(nil)

	for len(lines) > 0 {
		childIndent := getIndexFormatIndent(lines[0])
		if childIndent <= parentIndent {
			break
		}
		if strings.HasSuffix(lines[0], "│") {
			// Skip line.
			lines = lines[1:]
			continue
		} else {
			var childVector vector.T
			lines, childVector, childKey = ti.loadIndexFromFormat(treeKey, lines, level-1)
			childVectors.Add(childVector)
			childKeys = append(childKeys, childKey)
		}
	}

	// Always create partition in Ready state so that adds are allowed.
	metadata := cspann.PartitionMetadata{
		Level:    level,
		Centroid: centroid,
	}
	metadata.StateDetails.MakeReady()
	err = ti.memStore.TryCreateEmptyPartition(ti.ctx, treeKey, partitionKey, metadata)
	require.NoError(ti.T, err)

	if len(childKeys) > 0 {
		valueBytes := make([]cspann.ValueBytes, len(childKeys))
		added, err := ti.memStore.TryAddToPartition(
			ti.ctx, treeKey, partitionKey, childVectors, childKeys, valueBytes, metadata)
		require.NoError(ti.T, err)
		require.True(ti.T, added)
	}

	if details.State != cspann.ReadyState {
		// Update the partition's state
		expected := metadata
		metadata.StateDetails = details
		err = ti.memStore.TryUpdatePartitionMetadata(ti.ctx, treeKey, partitionKey, metadata, expected)
		require.NoError(ti.T, err)
	}

	return lines, centroid, cspann.ChildKey{PartitionKey: partitionKey}
}

// parsePartitionStateDetails parses a partition state details string in this
// format:
//
//	Ready
//	DrainingForSplit:1,2
//	Merging:1
func parsePartitionStateDetails(s string) cspann.PartitionStateDetails {
	var details cspann.PartitionStateDetails
	idx := strings.Index(s, ":")
	if idx == -1 {
		details.State = cspann.ParsePartitionState(s)
		return details
	}
	details.State = cspann.ParsePartitionState(s[:idx])

	// Parse the partition keys after the colon.
	remaining := s[idx+1:]
	if comma := strings.Index(remaining, ","); comma != -1 {
		// Two keys means we're parsing targets.
		if num, err := strconv.Atoi(remaining[:comma]); err == nil {
			details.Target1 = cspann.PartitionKey(num)
		}
		if num, err := strconv.Atoi(remaining[comma+1:]); err == nil {
			details.Target2 = cspann.PartitionKey(num)
		}
	} else if num, err := strconv.Atoi(remaining); err == nil {
		// Has one parameter - set as Source for merge operations, Target1
		// otherwise.
		switch details.State {
		case cspann.MergingState, cspann.RemovingLevelState:
			details.Source = cspann.PartitionKey(num)
		default:
			details.Target1 = cspann.PartitionKey(num)
		}
	}

	return details
}

func (ti *testIndex) reset() {
	// Reset state between tests.
	ti.successfulSplits = 0
	ti.pendingSplitsMerges = 0
}

func (ti *testIndex) processFixups() {
	if ti.Index != nil {
		if ti.discardFixups {
			ti.Index.DiscardFixups(ti.ctx)
		} else if !ti.skipFixups {
			require.NoError(ti.T, ti.Index.ProcessFixups(ti.ctx))
		}
	}
}

func (ti *testIndex) parseUseDataset(arg datadriven.CmdArg) vector.T {
	val := testutils.ParseDataDrivenInt(ti.T, arg)
	return ti.Dataset.At(val)
}

func (ti *testIndex) parseTreeID(arg datadriven.CmdArg) cspann.TreeKey {
	val := testutils.ParseDataDrivenInt(ti.T, arg)
	return memstore.ToTreeKey(memstore.TreeID(val))
}

// parseVector parses a vector string in this form: [1.5, 6, -4].
func (ti *testIndex) parseVector(str string) vector.T {
	vec, err := vector.ParseVector(str)
	require.NoError(ti.T, err)
	return vec
}

// parseKeyAndVector parses a line that may contain a key and vector separated
// by a colon. If there's no colon, it treats the line as just a key and gets
// the vector from the store.
func (ti *testIndex) parseKeyAndVector(line string) (cspann.KeyBytes, vector.T) {
	parts := strings.Split(line, ":")
	if len(parts) == 1 {
		// Get the value from the store.
		key := cspann.KeyBytes(line)
		return key, ti.memStore.GetVector(key)
	}

	// Parse the value after the colon.
	require.Len(ti.T, parts, 2)
	key := cspann.KeyBytes(parts[0])
	return key, ti.parseVector(parts[1])
}

func TestTransformVector(t *testing.T) {
	defer leaktest.AfterTest(t)()
	defer log.Scope(t).Close(t)

	// Create index.
	var workspace workspace.T
	ctx := context.Background()
	stopper := stop.NewStopper()
	defer stopper.Stop(ctx)

	// Use the rotMatrix algorithm for this test; other algorithms are tested by
	// TestRandomOrthoTransformer.
	const dims = 97
	const count = 5
	quantizer := quantize.NewRaBitQuantizer(dims, 46, vecpb.L2SquaredDistance)
	inMemStore := memstore.New(quantizer, 42)
	index, err := cspann.NewIndex(ctx, inMemStore, quantizer, 42, &cspann.IndexOptions{}, stopper)
	require.NoError(t, err)

	// Generate random vectors with exponentially increasing norms, in order
	// make distances more distinct.
	rng := rand.New(rand.NewSource(42))
	data := make([]float32, dims*count)
	for i := range data {
		vecIdx := float64(i / dims)
		data[i] = float32(rng.NormFloat64() * math.Pow(1.5, vecIdx))
	}

	original := vector.MakeSetFromRawData(data, dims)
	randomized := vector.MakeSet(dims)
	randomized.AddUndefined(count)
	for i := range original.Count {
		index.TransformVector(original.At(i), randomized.At(i))

		// Ensure that calling UnRandomizeVector recovers the normalized vector
		// (or original vector if not using Cosine distance).
		randomizedInv := make([]float32, dims)
		index.UnRandomizeVector(randomized.At(i), randomizedInv)
		for j, val := range original.At(i) {
			require.InDelta(t, val, randomizedInv[j], 0.00001)
		}
	}

	// Ensure that distances are similar, whether using the original vectors or
	// the randomized vectors.
	originalSet := quantizer.Quantize(&workspace, original).(*quantize.RaBitQuantizedVectorSet)
	randomizedSet := quantizer.Quantize(&workspace, randomized).(*quantize.RaBitQuantizedVectorSet)

	distances := make([]float32, count)
	errorBounds := make([]float32, count)
	quantizer.EstimateDistances(&workspace, originalSet, original.At(0), distances, errorBounds)
	require.Equal(t, []float32{0, 272.75, 550.86, 950.93, 2421.41}, testutils.RoundFloats(distances, 2))
	require.Equal(t, []float32{27.87, 46.08, 57.55, 69.46, 110.57}, testutils.RoundFloats(errorBounds, 2))

	quantizer.EstimateDistances(&workspace, randomizedSet, randomized.At(0), distances, errorBounds)
	require.Equal(t, []float32{5.1, 292.72, 454.95, 1011.85, 2475.87}, testutils.RoundFloats(distances, 2))
	require.Equal(t, []float32{37.58, 46.08, 57.55, 69.46, 110.57}, testutils.RoundFloats(errorBounds, 2))
}

// TestIndexConcurrency builds an index on multiple goroutines, with background
// splits and merges enabled.
func TestIndexConcurrency(t *testing.T) {
	defer leaktest.AfterTest(t)()
	defer log.Scope(t).Close(t)

	// Create index.
	ctx := context.Background()
	stopper := stop.NewStopper()
	defer stopper.Stop(ctx)

	// Load dataset.
	dataset := testutils.LoadDataset(t, testutils.ImagesDataset)

	// Trim dataset count from 10k to 256 and dataset dimensions from 512 to 32,
	// in order to make the test run faster and hit more interesting concurrency
	// combinations.
	const vectorCount = 256
	const dims = 32
	vectors := vector.MakeSet(dims)

	primaryKeys := make([]cspann.KeyBytes, vectorCount)
	for i := range vectorCount {
		primaryKeys[i] = cspann.KeyBytes(fmt.Sprintf("vec%d", i))
		vectors.Add(dataset.At(i)[:dims])
	}

	for i := range 10 {
		log.Infof(ctx, "iteration %d", i)

		// Construct store. Multiple index instances running on different goroutines
		// will use this store.
		const seed = 42
		quantizer := quantize.NewRaBitQuantizer(vectors.Dims, seed, vecpb.L2SquaredDistance)
		store := memstore.New(quantizer, seed)

		// Create 8 instances of the index, all using the same shared Store.
		const instances = 8
		var expectedKeys syncutil.Set[string]

		options := cspann.IndexOptions{
			RotAlgorithm:     vecpb.RotGivens,
			MinPartitionSize: 2,
			MaxPartitionSize: 4,
			BaseBeamSize:     2,
			QualitySamples:   4,
			// Eliminate the delay for one instance assisting another, in order
			// to maximize the possibility of race conditions.
			StalledOpTimeout: func() time.Duration { return 0 },
			// Set MaxInsertAttempts really high so that test is very unlikely
			// to fail because it can't find a partition that allows inserts.
			MaxInsertAttempts: 100,
		}

		var wait sync.WaitGroup
		vecsPerInstance := vectors.Count / instances
		for i := 0; i < vectors.Count; i += vecsPerInstance {
			wait.Add(1)
			go func(start, end int) {
				defer wait.Done()

				// Set small partition size and beam size to trigger frequent splits and
				// merges.
				index, err := cspann.NewIndex(ctx, store, quantizer, seed, &options, stopper)
				require.NoError(t, err)

				vectorSubset := vectors.Slice(start, end-start)
				keySubset := primaryKeys[start:end]
				deletedKeySubset := buildIndex(ctx, t, store, index, vectorSubset, keySubset)

				// Add any keys that were not deleted into the shared map.
				for i := range keySubset {
					key := string(keySubset[i])
					if !deletedKeySubset.Contains(key) {
						expectedKeys.Add(key)
					}
				}

				// Process any remaining fixups and close the index.
				require.NoError(t, index.ProcessFixups(ctx))
				index.Close()
			}(i, min(i+vecsPerInstance, vectors.Count))
		}

		wait.Wait()

		validateIndex(ctx, t, store, &expectedKeys)
	}
}

// buildIndex inserts the vectors in batches, and tries to delete one vector in
// each batch. It returns the subset of keys it successfully deleted (as
// strings).
func buildIndex(
	ctx context.Context,
	t *testing.T,
	store *memstore.Store,
	index *cspann.Index,
	vectors vector.Set,
	primaryKeys []cspann.KeyBytes,
) (deletedKeys *syncutil.Set[string]) {
	deletedKeys = new(syncutil.Set[string])

	// Insert block of vectors within the scope of a transaction.
	insertVectors := func(idxCtx *cspann.Context, start, end int) {
		for i := start; i < end; i++ {
			commontest.RunTransaction(ctx, t, store, func(txn cspann.Txn) {
				idxCtx.Init(txn)
				store.InsertVector(primaryKeys[i], vectors.At(i))
				require.NoError(t,
					index.Insert(ctx, idxCtx, nil /* treeKey */, vectors.At(i), primaryKeys[i]))
			})
		}
	}

	// Delete a random subset of vectors.
	deleteRandomVectors := func(idxCtx *cspann.Context, start, end int) {
		for i := start; i < end; i++ {
			// 50% chance of deleting a vector.
			if rand.Int()%2 == 0 {
				continue
			}

			vec := vectors.At(i)
			key := primaryKeys[i]
			commontest.RunTransaction(ctx, t, store, func(txn cspann.Txn) {
				idxCtx.Init(txn)
				deleted, err := index.Delete(ctx, idxCtx, nil /* treeKey */, vec, key)
				require.NoError(t, err)
				if deleted {
					store.DeleteVector(key)
					deletedKeys.Add(string(key))
				}
			})
		}
	}

	// Insert vectors into the store on multiple goroutines. Delete the first
	// vector in each block.
	var wait sync.WaitGroup
	const procs = 4
	countPerProc := (vectors.Count + procs) / procs
	blockSize := index.Options().MinPartitionSize
	for i := 0; i < vectors.Count; i += countPerProc {
		end := min(i+countPerProc, vectors.Count)
		wait.Add(1)
		go func(start, end int) {
			defer wait.Done()

			// Break vector group into individual transactions that each insert a
			// block of vectors. Run any pending fixups after each block.
			var idxCtx cspann.Context
			for j := start; j < end; j += blockSize {
				jEnd := min(j+blockSize, end)
				insertVectors(&idxCtx, j, jEnd)
				deleteRandomVectors(&idxCtx, j, jEnd)
				require.NoError(t, index.ProcessFixups(ctx))
			}
		}(i, end)
	}
	wait.Wait()

	return deletedKeys
}

// validateIndex tests that the store contains exactly the set of expected keys.
// Note that dangling vectors are treated as if they're missing from the index,
// for validation purposes.
func validateIndex(
	ctx context.Context, t *testing.T, store *memstore.Store, expectedKeys *syncutil.Set[string],
) {
	treeKey := memstore.ToTreeKey(memstore.TreeID(0))

	partitionKeys := []cspann.PartitionKey{cspann.RootKey}
	for {
		// Get all child keys for next level.
		var childKeys []cspann.ChildKey
		for _, key := range partitionKeys {
			partition, err := store.TryGetPartition(ctx, treeKey, key)
			if errors.Is(err, cspann.ErrPartitionNotFound) {
				// Ignore ErrPartitionNotFound, as splits can cause dangling
				// partition keys.
				continue
			}

			require.NoError(t, err)
			childKeys = append(childKeys, partition.ChildKeys()...)

			// Append target partitions of the root, if they exist. These may not
			// yet have been added to the root partition, and so may not be
			// accessible in any other way.
			if key == cspann.RootKey {
				state := partition.Metadata().StateDetails
				if state.Target1 != cspann.InvalidKey {
					childKeys = append(childKeys, cspann.ChildKey{PartitionKey: state.Target1})
				}
				if state.Target2 != cspann.InvalidKey {
					childKeys = append(childKeys, cspann.ChildKey{PartitionKey: state.Target2})
				}
			}
		}

		if len(childKeys) == 0 {
			break
		}

		if childKeys[0].KeyBytes != nil {
			// This partition contains leaf-level vectors.
			commontest.RunTransaction(ctx, t, store, func(txn cspann.Txn) {
				refs := make([]cspann.VectorWithKey, len(childKeys))
				for i := range childKeys {
					refs[i].Key = childKeys[i]
				}
				err := txn.GetFullVectors(ctx, treeKey, refs)
				require.NoError(t, err)
				for i := range refs {
					if refs[i].Vector != nil {
						// Vector is in the tree, so remove it from the expected map.
						expectedKeys.Remove(string(refs[i].Key.KeyBytes))
					}
				}
			})

			// No more levels to process.
			break
		}

		// This is not the leaf level, so process the next level.
		partitionKeys = make([]cspann.PartitionKey, len(childKeys))
		for i := range childKeys {
			partitionKeys[i] = childKeys[i].PartitionKey
		}
	}

	// Validate that there are no remaining keys that were expected to be present
	// in the index.
	var missingKeys []string
	expectedKeys.Range(func(value string) bool {
		missingKeys = append(missingKeys, value)
		return true
	})
	require.Empty(t, missingKeys)
}

// getIndexFormatIndent returns the number of indentation levels in the given
// formatted line. For example, for this snippet:
//
// • 1 (0, 0)
// │
// ├───• 2 (-9, 18)
//
// Line 1 would have indent of 0 and lines 2 and 3 would have indent of 1.
func getIndexFormatIndent(line string) int {
	// Compute level for line containing the "•" rune.
	idx := strings.Index(line, "• ")
	if idx != -1 {
		return utf8.RuneCountInString(line[:idx]) / 4
	}

	// Otherwise, this must be a line ending with the "│" rune.
	return utf8.RuneCountInString(line)/4 + 1
}
