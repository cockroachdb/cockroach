# ----------------------------------------------------------------------
# Load 1000 512-dimension image embeddings generated with a multi-modal
# model and search them using text embeddings generated with the same
# model. This will result in low accuracy due to the modality gap.
# ----------------------------------------------------------------------
new-index dataset=clip-512d-1050.gob train-count=1000 min-partition-size=4 max-partition-size=16 quality-samples=8 beam-size=4 distance-metric=Euclidean
----
Created index with 1000 vectors with 512 dimensions.

recall topk=10 beam-size=1 samples=20
----
1.00% recall@10

recall topk=10 beam-size=2 samples=20
----
3.50% recall@10

recall topk=10 beam-size=4 samples=20
----
3.00% recall@10

recall topk=10 beam-size=8 samples=20
----
9.00% recall@10

recall topk=10 beam-size=16 samples=20
----
32.00% recall@10

recall topk=10 beam-size=32 samples=20
----
58.50% recall@10

recall topk=10 beam-size=64 samples=20
----
84.50% recall@10

# Accuracy tops out here because of quantization error due to an incomplete
# RaBitQ error bounds implementation.
recall topk=10 beam-size=128 samples=20
----
94.50% recall@10
