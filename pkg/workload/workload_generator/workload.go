// Copyright 2025 The Cockroach Authors.
//
// Use of this software is governed by the CockroachDB Software License
// included in the /LICENSE file.

package workload_generator

import (
	"context"
	gosql "database/sql"
	"fmt"
	"math/rand/v2"
	"os"
	"path/filepath"
	"strconv"
	"strings"

	"github.com/cockroachdb/cockroach/pkg/col/coldata"
	"github.com/cockroachdb/cockroach/pkg/sql/types"
	"github.com/cockroachdb/cockroach/pkg/util/bufalloc"
	"github.com/cockroachdb/cockroach/pkg/workload"
	"github.com/cockroachdb/cockroach/pkg/workload/histogram"
	"github.com/cockroachdb/errors"
	"github.com/spf13/pflag"
	"gopkg.in/yaml.v2"
)

const (
	// fieldLength is a constant for the placeholder ops part of the code.
	fieldLength = 100
	// baseBatchSize is used during data generation. Batch sizes vary based on number of rows.
	baseBatchSize = 100
)

func init() {
	workload.Register(dbworkloadMeta)
}

var dbworkloadMeta = workload.Meta{
	Name:        "workload_generator",
	Description: "workloadGeneratorStruct tries to generate workloads from debug zip",
	Version:     "1.0.0",
	New: func() workload.Generator {
		g := &workloadGeneratorStruct{}
		g.flags.FlagSet = pflag.NewFlagSet("db_workload", pflag.ContinueOnError)
		g.flags.StringVar(&g.debugLogsLocation, "debug-logs", "",
			"Path to unzipped debug logs directory.")
		g.flags.IntVar(&g.rowCount, "rows", 1000,
			"Base row count for tables without foreign keys; other tables scale by foreign key depth/fanout.")
		g.flags.StringVar(&g.inputYAML, "input-yaml", "",
			"Path to an existing schema YAML file. When it is set, this file is used for data generation and auto-generation of the schema is skipped.")
		g.flags.StringVar(&g.outputDir, "output-dir", ".",
			"Directory in which to write schema YAML and SQL files (default: current directory)")
		g.flags.BoolVar(&g.schemaOnly, "schema-only", false,
			"When set, only teh schema YAML and SQL workload are generated. The tables initialisation is skipped.")
		g.flags.IntVar(&g.readPct, "read-pct", 50,
			"Percentage of SQL workload operations executed as reads (0–100; default 50).")
		g.flags.Meta = map[string]workload.FlagMeta{}
		g.connFlags = workload.NewConnFlags(&g.flags)
		return g
	},
}

type workloadGeneratorStruct struct {
	flags             workload.Flags
	connFlags         *workload.ConnFlags
	debugLogsLocation string // path to the unzipped debug zip file
	dbName            string // database name to use for the workload
	rowCount          int    // base number of rows per table before FK‐depth scaling

	allSchema      map[string]*TableSchema // the table-schema map generated by parsing ddl statements
	createStmts    map[string]string       // table name → DDL string
	workloadSchema Schema                  // table name → []TableBlock : the yaml marshal-able schema

	inputYAML  string // path to “user-provided” schema YAML file
	outputDir  string // directory to write generated schema & SQL
	schemaOnly bool   // if true, only dump schema and exit
	readPct    int    // percent reads vs writes
}

// Meta implements the Generator interface.
func (*workloadGeneratorStruct) Meta() workload.Meta { return dbworkloadMeta }

// Flags implements the Flagser interface.
func (w *workloadGeneratorStruct) Flags() workload.Flags {
	return workload.Flags{
		FlagSet: w.flags.FlagSet,
	}
}

// ConnFlags implements the ConnFlagser interface.
func (w *workloadGeneratorStruct) ConnFlags() *workload.ConnFlags {
	return w.connFlags
}

func (w *workloadGeneratorStruct) Hooks() workload.Hooks {
	return workload.Hooks{
		// Before data generation begins, we have to parse the DDLs and generate the schema required for further steps.
		PreCreate: func(db *gosql.DB) error {
			// 1) Determine the database name (use --db override if set).
			dbName := w.Meta().Name
			if w.connFlags.DBOverride != "" {
				dbName = w.connFlags.DBOverride
			}
			w.dbName = dbName

			// 2) Parse DDLs out of the debug logs.
			schemas, stmts, err := GenerateDDLs(w.debugLogsLocation, w.dbName, false)
			if err != nil {
				return errors.Wrap(err, "failed to generate DDLs from debug logs")
			}
			w.allSchema, w.createStmts = schemas, stmts

			// 3a) Auto-generate (or) load schema YAML
			if w.inputYAML == "" {
				w.workloadSchema = buildWorkloadSchema(w.allSchema, w.dbName, w.rowCount)
				data, err := yaml.Marshal(w.workloadSchema)
				if err != nil {
					return errors.Wrap(err, "failed to marshal workload schema to YAML")
				}
				yamlPath := filepath.Join(w.outputDir, fmt.Sprintf("%s_schema.yaml", w.dbName))
				if err := os.WriteFile(yamlPath, data, 0644); err != nil {
					return errors.Wrapf(err, "failed to write schema YAML to %s", yamlPath)
				}
			} else {
				raw, err := os.ReadFile(w.inputYAML)
				if err != nil {
					return errors.Wrapf(err, "failed to read schema YAML from %s", w.inputYAML)
				}
				if err := yaml.UnmarshalStrict(raw, &w.workloadSchema); err != nil {
					return errors.Wrapf(err, "failed to unmarshal schema YAML from %s", w.inputYAML)
				}
			}

			// 3b) TODO: Always generate the SQL file (even in schema-only mode).
			// Will wire in GenerateWorkload(w.debugLogsLocation, w.allSchema, w.dbName, sqlPath)

			return nil
		},
	}
}

// Tables implements workload.Generator.Tables
func (w *workloadGeneratorStruct) Tables() []workload.Table {
	// If we are in yaml output mode, emit no tables.
	if w.schemaOnly {
		return []workload.Table{}
	}

	// tableOrder is the order in which the DDLs of different tables should be executed.
	// It currently assumes that the order in which the DDLs show up in the debug folder is the correct sequence.
	tableOrder := getTableOrder(w.workloadSchema)
	// Finding the number of batches to generate for each table.
	// We use the maximum number of rows in any table to determine the number of batches.
	// Currently, we maintain the same number of batches across all tables, vary the batchSize if the rowCount varies.
	// Since our generators seeds are made using batch numbers, this helps to ensure foreign key constraints are held.
	maxRows := 0
	for _, tblBlocks := range w.workloadSchema {
		if tblBlocks[0].Count > maxRows {
			maxRows = tblBlocks[0].Count
		}
	}
	globalNumBatches := (maxRows + baseBatchSize - 1) / baseBatchSize

	var tables []workload.Table
	for _, tableName := range tableOrder {
		blocks := w.workloadSchema[tableName]
		block := blocks[0]
		total := block.Count
		batchSizeI := total / globalNumBatches

		tables = append(tables, workload.Table{
			Name:   tableName,
			Schema: generateSchema(w.createStmts[tableName]), // This is the short DDL string , i.e. , without the preceding CREATE TABLE ... and proceeding RETURNING ...
			InitialRows: workload.BatchedTuples{
				NumBatches: globalNumBatches,
				FillBatch:  generateBatch(tableName, block, w.workloadSchema, batchSizeI, w),
			},
		})
	}
	return tables
}

// getTableOrder returns the order of tables based on their TableNumber in the schema.
// The table number was assigned based on position in the debug file.
func getTableOrder(rawSchema Schema) []string {
	tableOrder := make([]string, len(rawSchema))
	for tableName, blocks := range rawSchema {
		block := blocks[0]
		tableOrder[block.TableNumber] = tableName
	}
	return tableOrder
}

// generateSchema extracts the schema part from a CREATE TABLE statement.
// It strips the CREATE TABLE part and returns the column definitions and table constraints.
func generateSchema(createStmt string) string {
	// Find the first '('
	start := strings.Index(createStmt, "(")
	if start < 0 {
		return "" // or panic/empty, as you prefer
	}
	// Walk forward, counting nested parens until we close the top‐level one.
	depth := 0
	var end int
	for i := start; i < len(createStmt); i++ {
		switch createStmt[i] {
		case '(':
			depth++
		case ')':
			depth--
			if depth == 0 {
				end = i
				// Once we've closed the top‐level '(', we're done.
				return createStmt[start : end+1]
			}
		}
	}
	// If we get here, the DDL was malformed (unbalanced parens).
	return createStmt[start:]
}

// generateBatch returns the FillBatch func for one table.
// It generates a batch of data for the given table block.
// INPUTS-
//   - tableName: the name of the table
//   - block: the TableBlock containing metadata about the table
//   - fullSchema: the full schema of the workload (the full amp of all TableBlocks)
//   - batchSize: the size of each batch to generate, since different tables have different batchSizes
//   - d: the dbworkload instance, which contains the generators and caches
func generateBatch(
	tableName string, block TableBlock, fullSchema Schema, batchSize int, w *workloadGeneratorStruct,
) func(batchIdx int, cb coldata.Batch, _ *bufalloc.ByteAllocator) {
	// Determine the Cockroach columnar types and a stable column order.
	colOrder := block.ColumnOrder
	// Build Cockroach types in that order:
	cdTypes := make([]*types.T, len(colOrder))
	for i, colName := range colOrder {
		meta := block.Columns[colName]
		switch meta.Type {
		case "integer", "sequence":
			cdTypes[i] = types.Int
		case "float":
			cdTypes[i] = types.Float
		default:
			cdTypes[i] = types.Bytes
		}
	}

	return func(batchIdx int, cb coldata.Batch, _ *bufalloc.ByteAllocator) {
		total := block.Count
		start := batchIdx * batchSize
		end := start + batchSize
		if end > total {
			end = total
		}
		n := end - start

		// 1) Reset the vector; n rows, cdTypes[i] per column.
		cb.Reset(cdTypes, n, coldata.StandardColumnFactory)

		// 2) Instantiate one Generator per column, seeded by batchIdx.
		gens := make([]Generator, len(colOrder))
		for i, colName := range colOrder {
			meta := block.Columns[colName]
			gens[i] = buildGenerator(meta, batchIdx, baseBatchSize, fullSchema)
		}

		// 3) Fill row-by-row.
		for row := 0; row < n; row++ {
			for i, cdType := range cdTypes {
				raw := gens[i].Next()

				vec := cb.ColVec(i)
				nulls := vec.Nulls()
				if raw == "" {
					nulls.SetNull(row)
					continue
				}
				switch cdType.Family() {
				case types.IntFamily:
					// For sequences & integers.
					iv, err := strconv.ParseInt(raw, 10, 64)
					if err != nil {
						panic(fmt.Sprintf("parse int: %v", err))
					}
					vec.Int64()[row] = iv

				case types.FloatFamily:
					// For floats, decimals, numerics, doubles.
					fv, err := strconv.ParseFloat(raw, 64)
					if err != nil {
						panic(fmt.Sprintf("parse float: %v", err))
					}
					vec.Float64()[row] = fv

				default:
					// For strings, json, uuid, timestamp → bytes.
					bytesVec := vec.Bytes()
					bytesVec.Set(row, []byte(raw))
				}
			}
		}
	}
}

// Ops function is a placeholder for now.
func (w *workloadGeneratorStruct) Ops(
	ctx context.Context, urls []string, reg *histogram.Registry,
) (workload.QueryLoad, error) {
	sqlDatabase, err := gosql.Open("postgres", urls[0])
	if err != nil {
		return workload.QueryLoad{}, err
	}

	db := sqlDatabase

	readStmt, err := db.Prepare("SELECT field1, field2, field3 FROM simple WHERE id = $1")
	if err != nil {
		return workload.QueryLoad{}, err
	}

	updateStmt, err := db.Prepare("UPDATE simple SET field1 = $1, field2 = $2, field3 = $3 WHERE id = $4")
	if err != nil {
		return workload.QueryLoad{}, err
	}

	ql := workload.QueryLoad{
		WorkerFns: make([]func(context.Context) error, 8),
	}
	initialRowCount := int64(10)
	for i := range ql.WorkerFns {
		workerID := i
		rng := rand.New(rand.NewPCG(uint64(workerID), 0))
		ql.WorkerFns[i] = func(ctx context.Context) error {
			// 50% reads, 50% updates
			if rng.IntN(2) == 0 {
				// Read operation
				id := rng.Int64N(initialRowCount)
				var field1, field2, field3 string
				if err := readStmt.QueryRowContext(ctx, id).Scan(&field1, &field2, &field3); err != nil {
					return err
				}
			} else {
				// Update operation
				id := rng.Int64N(initialRowCount)
				field1 := randString(rng, fieldLength)
				field2 := randString(rng, fieldLength)
				field3 := randString(rng, fieldLength)
				if _, err := updateStmt.ExecContext(ctx, field1, field2, field3, id); err != nil {
					return err
				}
			}
			return nil
		}
	}

	// Close statements and database when the workload finishes
	ql.Close = func(context.Context) error {
		err := readStmt.Close()
		if err != nil {
			return err
		}
		err = updateStmt.Close()
		if err != nil {
			return err
		}
		return sqlDatabase.Close()
	}

	return ql, nil
}

func randString(rng *rand.Rand, length int) []byte {
	const letters = "abcdefghijklmnopqrstuvwxyz"
	result := make([]byte, length)
	for i := range result {
		result[i] = letters[rng.IntN(len(letters))]
	}
	return result
}
