// Copyright 2025 The Cockroach Authors.
//
// Use of this software is governed by the CockroachDB Software License
// included in the /LICENSE file.

package workload_generator

import (
	"context"
	gosql "database/sql"
	"fmt"
	"math/rand/v2"
	"os"
	"path/filepath"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/cockroachdb/cockroach-go/v2/crdb"
	"github.com/cockroachdb/cockroach/pkg/col/coldata"
	"github.com/cockroachdb/cockroach/pkg/sql/types"
	"github.com/cockroachdb/cockroach/pkg/util/bufalloc"
	"github.com/cockroachdb/cockroach/pkg/util/timeutil"
	"github.com/cockroachdb/cockroach/pkg/workload"
	"github.com/cockroachdb/cockroach/pkg/workload/histogram"
	"github.com/cockroachdb/errors"
	"github.com/spf13/pflag"
	"gopkg.in/yaml.v2"
)

// FKRef is a struct that holds the foreign key reference information for the Placeholder struct.
type FKRef struct {
	Table  string
	Column string
}

// Placeholder is sued to properly store all the metadata from the SQL query.
// Its values help dictate the runtime data generation during the execution of the SQL queries.
type Placeholder struct {
	Name         string
	ColType      string
	IsNullable   bool
	IsPrimaryKey bool
	Default      *string
	IsUnique     bool
	FKReference  *FKRef
	InlineCheck  *string
	Clause       string // e.g. "WHERE", "INSERT", "UPDATE"
	Position     int    // $1, $2, ...
	TableName    string
}

// SQLQuery has a SQL string and a slice of Placeholders.
// Sql is the raw string with $1, $2,... $x instead of real values that is executed
// The Placeholders are used to generate the data for the $1, $2,... $x in the SQL query at runtime.
type SQLQuery struct {
	SQL          string
	Placeholders []Placeholder
}

// Transaction represents a single transaction with a type and a list of SQL queries.
type Transaction struct {
	typ     string // e.g. "read", "write"
	Queries []SQLQuery
}

// runtimeColumn holds a generator and a cache of prior values fro a particular column.
type runtimeColumn struct {
	gen        Generator
	mu         sync.Mutex // protects gen and cache
	cache      []string   // ring buffer of recent values
	columnMeta ColumnMeta // metadata for this column
}

// txnWorker consists of all the data that is needed for the runtime routines.
type txnWorker struct {
	db                *gosql.DB //db is the connection to the running cluster.
	readTransactions  []Transaction
	writeTransactions []Transaction
	rng               *rand.Rand
	hists             *histogram.Histograms
	w                 *workloadGeneratorStruct // reference to the workloadGeneratorStruct for generators
}

const (
	// baseBatchSize is used during data generation. Batch sizes vary based on number of rows.
	baseBatchSize = 100
	// maxCacheSize is the maximum size of the cache for runtimeColumn.
	maxCacheSize = 100_000
)

func init() {
	workload.Register(dbworkloadMeta)
}

var dbworkloadMeta = workload.Meta{
	Name:        "workload_generator",
	Description: "workloadGeneratorStruct tries to generate workloads from debug zip",
	Version:     "1.0.0",
	New: func() workload.Generator {
		g := &workloadGeneratorStruct{}
		g.flags.FlagSet = pflag.NewFlagSet("db_workload", pflag.ContinueOnError)
		g.flags.StringVar(&g.debugLogsLocation, "debug-logs", "",
			"Path to unzipped debug logs directory.")
		g.flags.IntVar(&g.rowCount, "rows", 1000,
			"Base row count for tables without foreign keys; other tables scale by foreign key depth/fanout.")
		g.flags.StringVar(&g.inputYAML, "input-yaml", "",
			"Path to an existing schema YAML file. When it is set, this file is used for data generation and auto-generation of the schema is skipped.")
		g.flags.StringVar(&g.outputDir, "output-dir", ".",
			"Directory in which to write schema YAML and SQL files (default: current directory)")
		g.flags.BoolVar(&g.schemaOnly, "schema-only", false,
			"When set, only teh schema YAML and SQL workload are generated. The tables initialisation is skipped.")
		g.flags.IntVar(&g.readPct, "read-pct", 50,
			"Percentage of SQL workload operations executed as reads (0–100; default 50).")
		g.flags.Meta = map[string]workload.FlagMeta{}
		g.connFlags = workload.NewConnFlags(&g.flags)
		return g
	},
}

type workloadGeneratorStruct struct {
	flags             workload.Flags
	connFlags         *workload.ConnFlags
	debugLogsLocation string // path to the unzipped debug zip file
	dbName            string // database name to use for the workload
	rowCount          int    // base number of rows per table before FK‐depth scaling

	allSchema      map[string]*TableSchema // the table-schema map generated by parsing ddl statements
	createStmts    map[string]string       // table name → DDL string
	workloadSchema Schema                  // table name → []TableBlock : the yaml marshal-able schema

	inputYAML  string                    // path to “user-provided” schema YAML file
	outputDir  string                    // directory to write generated schema & SQL
	schemaOnly bool                      // if true, only dump schema and exit
	readPct    int                       // percent reads vs writes
	columnGens map[string]*runtimeColumn // table.col → runtimeColumn
}

// Meta implements the Generator interface.
func (*workloadGeneratorStruct) Meta() workload.Meta { return dbworkloadMeta }

// Flags implements the Flagser interface.
func (w *workloadGeneratorStruct) Flags() workload.Flags {
	return workload.Flags{
		FlagSet: w.flags.FlagSet,
	}
}

// ConnFlags implements the ConnFlagser interface.
func (w *workloadGeneratorStruct) ConnFlags() *workload.ConnFlags {
	return w.connFlags
}

func (w *workloadGeneratorStruct) Hooks() workload.Hooks {
	return workload.Hooks{
		// Before data generation begins, we have to parse the DDLs and generate the schema required for further steps.
		PreCreate: func(db *gosql.DB) error {
			// 1) Determining the database name (use --db override if set).
			dbName := w.Meta().Name
			if w.connFlags.DBOverride != "" {
				dbName = w.connFlags.DBOverride
			}
			w.dbName = dbName

			// 2) Parsing DDLs out of the debug logs.
			schemas, stmts, err := generateDDLs(w.debugLogsLocation, w.dbName, false)
			if err != nil {
				return errors.Wrap(err, "failed to generate DDLs from debug logs")
			}
			w.allSchema, w.createStmts = schemas, stmts

			// 3a) Auto-generate (or) load schema YAML
			if w.inputYAML == "" {
				w.workloadSchema = buildWorkloadSchema(w.allSchema, w.dbName, w.rowCount)
				data, err := yaml.Marshal(w.workloadSchema)
				if err != nil {
					return errors.Wrap(err, "failed to marshal workload schema to YAML")
				}
				yamlPath := filepath.Join(w.outputDir, fmt.Sprintf("schema_%s.yaml", w.dbName))
				if err := os.WriteFile(yamlPath, data, 0644); err != nil {
					return errors.Wrapf(err, "failed to write schema YAML to %s", yamlPath)
				}
			} else {
				raw, err := os.ReadFile(w.inputYAML)
				if err != nil {
					return errors.Wrapf(err, "failed to read schema YAML from %s", w.inputYAML)
				}
				if err := yaml.UnmarshalStrict(raw, &w.workloadSchema); err != nil {
					return errors.Wrapf(err, "failed to unmarshal schema YAML from %s", w.inputYAML)
				}
			}

			// 3b) Always generate the SQL file (even in schema-only mode).
			if err := generateWorkload(w.debugLogsLocation, w.allSchema, w.dbName, w.outputDir); err != nil {
				return errors.Wrapf(err,
					"failed to generate SQL workload to %s_read.sql and %s_write.sql",
					w.dbName, w.dbName,
				)
			}

			return nil
		},
	}
}

// Tables implements workload.Generator.Tables
func (w *workloadGeneratorStruct) Tables() []workload.Table {
	// If we are in yaml output mode, emit no tables.
	if w.schemaOnly {
		return []workload.Table{}
	}

	// tableOrder is the order in which the DDLs of different tables should be executed.
	// It currently assumes that the order in which the DDLs show up in the debug folder is the correct sequence.
	tableOrder := getTableOrder(w.workloadSchema)
	// Finding the number of batches to generate for each table.
	// We use the maximum number of rows in any table to determine the number of batches.
	// Currently, we maintain the same number of batches across all tables, vary the batchSize if the rowCount varies.
	// Since our generators seeds are made using batch numbers, this helps to ensure foreign key constraints are held.
	maxRows := 0
	for _, tblBlocks := range w.workloadSchema {
		if tblBlocks[0].Count > maxRows {
			maxRows = tblBlocks[0].Count
		}
	}
	globalNumBatches := (maxRows + baseBatchSize - 1) / baseBatchSize

	var tables []workload.Table
	for _, tableName := range tableOrder {
		blocks := w.workloadSchema[tableName]
		block := blocks[0]
		total := block.Count
		batchSizeI := total / globalNumBatches

		tables = append(tables, workload.Table{
			Name:   tableName,
			Schema: generateSchema(w.createStmts[tableName]), // This is the short DDL string , i.e. , without the preceding CREATE TABLE ... and proceeding RETURNING ...
			InitialRows: workload.BatchedTuples{
				NumBatches: globalNumBatches,
				FillBatch:  generateBatch(tableName, block, w.workloadSchema, batchSizeI, w),
			},
		})
	}
	return tables
}

// getTableOrder returns the order of tables based on their TableNumber in the schema.
// The table number was assigned based on position in the debug file.
func getTableOrder(rawSchema Schema) []string {
	tableOrder := make([]string, len(rawSchema))
	for tableName, blocks := range rawSchema {
		block := blocks[0]
		tableOrder[block.TableNumber] = tableName
	}
	return tableOrder
}

// generateSchema extracts the schema part from a CREATE TABLE statement.
// It strips the CREATE TABLE part and returns the column definitions and table constraints.
func generateSchema(createStmt string) string {
	// Find the first '('
	start := strings.Index(createStmt, "(")
	if start < 0 {
		return "" // or panic/empty, as you prefer
	}
	// Walk forward, counting nested parens until we close the top‐level one.
	depth := 0
	var end int
	for i := start; i < len(createStmt); i++ {
		switch createStmt[i] {
		case '(':
			depth++
		case ')':
			depth--
			if depth == 0 {
				end = i
				// Once we've closed the top‐level '(', we're done.
				return createStmt[start : end+1]
			}
		}
	}
	// If we get here, the DDL was malformed (unbalanced parens).
	return createStmt[start:]
}

// generateBatch returns the FillBatch func for one table.
// It generates a batch of data for the given table block.
// INPUTS-
//   - tableName: the name of the table
//   - block: the TableBlock containing metadata about the table
//   - fullSchema: the full schema of the workload (the full amp of all TableBlocks)
//   - batchSize: the size of each batch to generate, since different tables have different batchSizes
//   - d: the dbworkload instance, which contains the generators and caches
func generateBatch(
	tableName string, block TableBlock, fullSchema Schema, batchSize int, w *workloadGeneratorStruct,
) func(batchIdx int, cb coldata.Batch, _ *bufalloc.ByteAllocator) {
	// Determine the Cockroach columnar types and a stable column order.
	colOrder := block.ColumnOrder
	// Build Cockroach types in that order:
	cdTypes := make([]*types.T, len(colOrder))
	for i, colName := range colOrder {
		meta := block.Columns[colName]
		switch meta.Type {
		case "integer", "sequence":
			cdTypes[i] = types.Int
		case "float":
			cdTypes[i] = types.Float
		default:
			cdTypes[i] = types.Bytes
		}
	}

	return func(batchIdx int, cb coldata.Batch, _ *bufalloc.ByteAllocator) {
		total := block.Count
		start := batchIdx * batchSize
		end := start + batchSize
		if end > total {
			end = total
		}
		n := end - start

		// 1) Reset the vector; n rows, cdTypes[i] per column.
		cb.Reset(cdTypes, n, coldata.StandardColumnFactory)

		// 2) Instantiate one Generator per column, seeded by batchIdx.
		gens := make([]Generator, len(colOrder))
		for i, colName := range colOrder {
			meta := block.Columns[colName]
			gens[i] = buildGenerator(meta, batchIdx, baseBatchSize, fullSchema)
		}

		// 3) Fill row-by-row.
		for row := 0; row < n; row++ {
			for i, cdType := range cdTypes {
				raw := gens[i].Next()

				vec := cb.ColVec(i)
				nulls := vec.Nulls()
				if raw == "" {
					nulls.SetNull(row)
					continue
				}
				switch cdType.Family() {
				case types.IntFamily:
					// For sequences & integers.
					iv, err := strconv.ParseInt(raw, 10, 64)
					if err != nil {
						panic(fmt.Sprintf("parse int: %v", err))
					}
					vec.Int64()[row] = iv

				case types.FloatFamily:
					// For floats, decimals, numerics, doubles.
					fv, err := strconv.ParseFloat(raw, 64)
					if err != nil {
						panic(fmt.Sprintf("parse float: %v", err))
					}
					vec.Float64()[row] = fv

				default:
					// For strings, json, uuid, timestamp → bytes.
					bytesVec := vec.Bytes()
					bytesVec.Set(row, []byte(raw))
				}
			}
		}
	}
}

func (w *workloadGeneratorStruct) Ops(
	ctx context.Context, urls []string, reg *histogram.Registry,
) (workload.QueryLoad, error) {
	// Database name is set in the main workload struct.
	w.setDbName()
	// The schema information is loaded into memory from the yaml. This information is used for the runtime data generation.
	errYaml, done := w.loadYamlData()
	if done {
		return workload.QueryLoad{}, errYaml
	}
	// Transactions from the sql file are parsed.
	readPath := filepath.Join(w.outputDir, fmt.Sprintf("%s_read.sql", w.dbName))
	writePath := filepath.Join(w.outputDir, fmt.Sprintf("%s_write.sql", w.dbName))
	readTransactions, errRead := readSQL(readPath, "read")
	writeTransactions, errWrite := readSQL(writePath, "write")
	if errRead != nil {
		return workload.QueryLoad{}, errRead
	}
	if errWrite != nil {
		return workload.QueryLoad{}, errWrite
	}

	db, err := gosql.Open("postgres", strings.Join(urls, " "))
	if err != nil {
		return workload.QueryLoad{}, err
	}
	db.SetMaxOpenConns(w.connFlags.Concurrency + 1)
	db.SetMaxIdleConns(w.connFlags.Concurrency + 1)

	// The generators and column old data caches are initialised.
	if err := w.initGenerators(db); err != nil {
		return workload.QueryLoad{}, err
	}

	ql := workload.QueryLoad{}
	for i := 0; i < w.connFlags.Concurrency; i++ {
		worker := &txnWorker{
			db:                db,
			readTransactions:  readTransactions,
			writeTransactions: writeTransactions,
			rng:               rand.New(rand.NewPCG(uint64(time.Now().UnixNano()), uint64(i))),
			hists:             reg.GetHandle(),
			w:                 w, // reference to the workloadGeneratorStruct for generators
		}
		ql.WorkerFns = append(ql.WorkerFns, worker.run)
	}
	ql.Close = func(context.Context) error { return db.Close() }
	return ql, nil
}

// run executes a random transaction from the list of transactions.
func (t *txnWorker) run(ctx context.Context) error {
	txn := t.chooseTransaction()
	// Reference to the workloadGeneratorStruct for generators.
	w := t.w
	// Time for the metrics is started.
	start := timeutil.Now()

	// Each transaction gets its own debug slice.
	type debugEntry struct {
		Query string
		Args  []interface{}
	}
	err := crdb.ExecuteTx(ctx, t.db, nil, func(tx *gosql.Tx) error {
		// Inserted maintains a map of column names to the values that were inserted in this transaction.
		inserted := make(map[string][]interface{})
		queryCtx, cancel := context.WithTimeout(ctx, 5*time.Minute)
		defer cancel()
		for _, sqlQuery := range txn.Queries {
			args := make([]interface{}, len(sqlQuery.Placeholders))
			// Checking if we have a situation where all the primary keys in the query have foreign key dependency.
			allPksAreFK := checkIfAllPkAreFk(sqlQuery, w)
			// Picking a single fkIdx for ALL FK placeholders (or -1 if none).
			// This ensures that for all column in a composite fk, the same row index from the parent column cache is chosen.
			fkIndex := t.pickForeignKeyIndex(sqlQuery, w)
			// Building per-placeholder indexes.
			indexes := t.setCacheIndex(sqlQuery, w, fkIndex)
			for i, placeholder := range sqlQuery.Placeholders {
				var raw string
				// Getting the value to be inserted for the placeholder.
				raw = getColumnValue(allPksAreFK, placeholder, w, inserted, raw, indexes, i)
				// If the data was generated to be written, we need to insert it into the inserted map.
				if placeholder.Clause == insert {
					inserted[placeholder.Name] = append(inserted[placeholder.Name], raw)
				}
				// The value for the placeholder is set in the args slice based on the column type.
				err := setColumnValue(raw, placeholder, args, i)
				if err != nil {
					return err
				}
			}
			// The SQL query is ran with the args.
			if _, err := tx.ExecContext(queryCtx, sqlQuery.SQL, args...); err != nil {
				return err
			}
		}
		return nil
	})
	// The elapsed time is calculated for the transaction metrics.
	elapsed := timeutil.Since(start)
	t.hists.Get(fmt.Sprintf("typ_%v", txn.typ)).Record(elapsed)
	return err
}
