// Copyright 2025 The Cockroach Authors.
//
// Use of this software is governed by the CockroachDB Software License
// included in the /LICENSE file.

package workload_generator

import (
	"context"
	gosql "database/sql"
	"fmt"
	"math/rand/v2"
	"os"
	"path/filepath"
	"strconv"
	"strings"
	"time"

	"github.com/cockroachdb/cockroach-go/v2/crdb"
	"github.com/cockroachdb/cockroach/pkg/col/coldata"
	"github.com/cockroachdb/cockroach/pkg/sql/types"
	"github.com/cockroachdb/cockroach/pkg/util/bufalloc"
	"github.com/cockroachdb/cockroach/pkg/util/syncutil"
	"github.com/cockroachdb/cockroach/pkg/util/timeutil"
	"github.com/cockroachdb/cockroach/pkg/workload"
	"github.com/cockroachdb/cockroach/pkg/workload/histogram"
	"github.com/cockroachdb/errors"
	"github.com/lib/pq"
	"github.com/spf13/pflag"
	"gopkg.in/yaml.v2"
)

// FKRef is a struct that holds the foreign key reference information for the Placeholder struct.
type FKRef struct {
	Table  string
	Column string
}

// Placeholder is used to properly store all the metadata from the SQL query.
// Its values help dictate the runtime data generation during the execution of the SQL queries.
type Placeholder struct {
	Name         string
	ColType      string
	IsNullable   bool
	IsPrimaryKey bool
	Default      *string
	IsUnique     bool
	FKReference  *FKRef
	InlineCheck  *string
	Clause       string // e.g. "WHERE", "INSERT", "UPDATE"
	Position     int    // $1, $2, ...
	TableName    string
}

// SQLQuery has an SQL string and a slice of Placeholders.
// SQL is the raw string that is executed with $1, $2,... $x instead of real values.
// The Placeholders are used to generate the data for the $1, $2,... $x in the SQL query at runtime.
type SQLQuery struct {
	SQL          string
	Placeholders []Placeholder
}

// Transaction represents a single transaction with a type and a list of SQL queries.
type Transaction struct {
	typ     string // e.g. "read", "write"
	Queries []SQLQuery
}

// runtimeColumn holds a generator and a cache of prior values for a particular column.
type runtimeColumn struct {
	gen        Generator
	mu         syncutil.Mutex // protects gen and cache
	cache      []string       // ring buffer of recent values
	columnMeta ColumnMeta     // metadata for this column
}

// txnWorker contains all data needed for executing SQL transactions during workload runtime.
type txnWorker struct {
	db                *gosql.DB // db is the connection to the running cluster.
	readTransactions  []Transaction
	writeTransactions []Transaction
	rng               *rand.Rand
	hists             *histogram.Histograms
	generator         *workloadGenerator // reference to the workloadGenerator for generators
}

const (
	// baseBatchSize is used during data generation. Batch sizes vary based on number of rows.
	baseBatchSize = 100
	// maxCacheSize is the maximum size of the cache for runtimeColumn.
	maxCacheSize = 100_000
)

func init() {
	workload.Register(workloadGeneratorMeta)
}

var workloadGeneratorMeta = workload.Meta{
	Name:        "workload_generator",
	Description: "workloadGenerator tries to generate workloads from debug zip",
	Version:     "1.0.0",
	New: func() workload.Generator {
		g := &workloadGenerator{}
		g.flags.FlagSet = pflag.NewFlagSet("db_workload", pflag.ContinueOnError)
		g.flags.StringVar(&g.debugLogsLocation, "debug-logs", "",
			"Path to unzipped debug logs directory.")
		g.flags.IntVar(&g.rowCount, "rows", 1000,
			"Base row count for tables without foreign keys; other tables scale by foreign key depth/fanout.")
		g.flags.StringVar(&g.inputYAML, "input-yaml", "",
			"Path to an existing schema YAML file. When it is set, this file is used for data generation and auto-generation of the schema is skipped.")
		g.flags.StringVar(&g.outputDir, "output-dir", ".",
			"Directory in which to write schema YAML and SQL files (default: current directory)")
		g.flags.BoolVar(&g.schemaOnly, "schema-only", false,
			"When set, only teh schema YAML and SQL workload are generated. The tables initialisation is skipped.")
		g.flags.IntVar(&g.readPct, "read-pct", 50,
			"Percentage of SQL workload operations executed as reads (0–100; default 50).")
		g.flags.DurationVar(&g.txTimeout, "tx-timeout", 5*time.Minute,
			"Per-transaction timeout (e.g. 30s, 5m; default 5m).")
		g.flags.Meta = map[string]workload.FlagMeta{}
		g.connFlags = workload.NewConnFlags(&g.flags)
		return g
	},
}

type workloadGenerator struct {
	flags             workload.Flags
	connFlags         *workload.ConnFlags
	debugLogsLocation string // path to the unzipped debug zip file
	dbName            string // database name to use for the workload
	rowCount          int    // base number of rows per table before FK‐depth scaling

	allSchema      map[string]*TableSchema // the table-schema map generated by parsing ddl statements
	createStmts    map[string]string       // table name → DDL string
	workloadSchema Schema                  // table name → []TableBlock: the YAML marshal-able schema

	inputYAML  string                    // path to “user-provided” schema YAML file
	outputDir  string                    // directory to write generated schema & SQL
	schemaOnly bool                      // if true, only dump schema and exit
	readPct    int                       // percent reads vs writes
	txTimeout  time.Duration             // per-transaction timeout for all ExecContext calls
	columnGens map[string]*runtimeColumn // table.col → runtimeColumn
}

// Meta implements the Generator interface.
func (*workloadGenerator) Meta() workload.Meta { return workloadGeneratorMeta }

// Flags implements the Flagser interface.
func (w *workloadGenerator) Flags() workload.Flags {
	return workload.Flags{
		FlagSet: w.flags.FlagSet,
	}
}

// ConnFlags implements the ConnFlagser interface.
func (w *workloadGenerator) ConnFlags() *workload.ConnFlags {
	return w.connFlags
}

func (w *workloadGenerator) Hooks() workload.Hooks {
	return workload.Hooks{
		// Before data generation begins, we have to parse the DDLs and generate the schema required for further steps.
		PreCreate: func(db *gosql.DB) error {
			// 1) Determining the database name (use --db override if set).
			dbName := w.Meta().Name
			if w.connFlags.DBOverride != "" {
				dbName = w.connFlags.DBOverride
			}
			w.dbName = dbName

			// 2) Parsing DDLs out of the debug logs.
			schemas, stmts, err := generateDDLs(w.debugLogsLocation, w.dbName, false)
			if err != nil {
				return errors.Wrap(err, "failed to generate DDLs from debug logs")
			}
			w.allSchema, w.createStmts = schemas, stmts

			// 3a) Auto-generate (or) load schema YAML
			if w.inputYAML == "" {
				w.workloadSchema = buildWorkloadSchema(w.allSchema, w.dbName, w.rowCount)
				data, err := yaml.Marshal(w.workloadSchema)
				if err != nil {
					return errors.Wrap(err, "failed to marshal workload schema to YAML")
				}
				yamlPath := filepath.Join(w.outputDir, fmt.Sprintf("schema_%s.yaml", w.dbName))
				if err := os.WriteFile(yamlPath, data, 0644); err != nil {
					return errors.Wrapf(err, "failed to write schema YAML to %s", yamlPath)
				}
			} else {
				raw, err := os.ReadFile(w.inputYAML)
				if err != nil {
					return errors.Wrapf(err, "failed to read schema YAML from %s", w.inputYAML)
				}
				if err := yaml.UnmarshalStrict(raw, &w.workloadSchema); err != nil {
					return errors.Wrapf(err, "failed to unmarshal schema YAML from %s", w.inputYAML)
				}
			}

			// 3b) Always generate the SQL file (even in schema-only mode).
			if err := generateWorkload(w.debugLogsLocation, w.allSchema, w.dbName, w.outputDir); err != nil {
				return errors.Wrapf(err,
					"failed to generate SQL workload to %s_read.sql and %s_write.sql",
					w.dbName, w.dbName,
				)
			}

			return nil
		},
	}
}

// Tables implements workload.Generator.Tables
func (w *workloadGenerator) Tables() []workload.Table {
	// If we are in YAML output mode, emit no tables.
	if w.schemaOnly {
		return []workload.Table{}
	}

	// tableOrder is the order in which the DDLs of different tables should be executed.
	// It currently assumes that the order in which the DDLs show up in the debug folder is the correct sequence.
	tableOrder := getTableOrder(w.workloadSchema)
	// Finding the number of batches to generate for each table.
	// We use the maximum number of rows in any table to determine the number of batches.
	// Currently, we maintain the same number of batches across all tables, vary the batchSize if the rowCount varies.
	// Since our generators seeds are made using batch numbers, this helps to ensure foreign key constraints are held.
	maxRows := 0
	for _, tblBlocks := range w.workloadSchema {
		if tblBlocks[0].Count > maxRows {
			maxRows = tblBlocks[0].Count
		}
	}
	globalNumBatches := (maxRows + baseBatchSize - 1) / baseBatchSize

	var tables []workload.Table
	for _, tableName := range tableOrder {
		blocks := w.workloadSchema[tableName]
		block := blocks[0]
		total := block.Count
		batchSizeI := total / globalNumBatches

		tables = append(tables, workload.Table{
			Name:   tableName,
			Schema: generateSchema(w.createStmts[tableName]), // This is the short DDL string , i.e. , without the preceding CREATE TABLE ... and proceeding RETURNING ...
			InitialRows: workload.BatchedTuples{
				NumBatches: globalNumBatches,
				FillBatch:  generateBatch(tableName, block, w.workloadSchema, batchSizeI, w),
			},
		})
	}
	return tables
}

// getTableOrder returns the order of tables based on their TableNumber in the schema.
// The table number was assigned based on position in the debug file.
func getTableOrder(rawSchema Schema) []string {
	tableOrder := make([]string, len(rawSchema))
	for tableName, blocks := range rawSchema {
		block := blocks[0]
		tableOrder[block.TableNumber] = tableName
	}
	return tableOrder
}

// generateSchema extracts the schema part from a CREATE TABLE statement.
// It strips the CREATE TABLE part and returns the column definitions and table constraints.
func generateSchema(createStmt string) string {
	// Find the first '('
	start := strings.Index(createStmt, "(")
	if start < 0 {
		return "" // or panic/empty, as you prefer
	}
	// Walk forward, counting nested parens until we close the top‐level one.
	depth := 0
	var end int
	for i := start; i < len(createStmt); i++ {
		switch createStmt[i] {
		case '(':
			depth++
		case ')':
			depth--
			if depth == 0 {
				end = i
				// Once we've closed the top‐level '(', we're done.
				return createStmt[start : end+1]
			}
		}
	}
	// If we get here, the DDL was malformed (unbalanced parens).
	return createStmt[start:]
}

// generateBatch returns the FillBatch func for one table.
// It generates a batch of data for the given table block.
// INPUTS-
//   - tableName: the name of the table
//   - block: the TableBlock containing metadata about the table
//   - fullSchema: the full schema of the workload (the full amp of all TableBlocks)
//   - batchSize: the size of each batch to generate, since different tables have different batchSizes
//   - d: the dbworkload instance, which contains the generators and caches
func generateBatch(
	tableName string, block TableBlock, fullSchema Schema, batchSize int, w *workloadGenerator,
) func(batchIdx int, cb coldata.Batch, _ *bufalloc.ByteAllocator) {
	// Determine the Cockroach columnar types and a stable column order.
	colOrder := block.ColumnOrder
	// Build Cockroach types in that order:
	cdTypes := make([]*types.T, len(colOrder))
	for i, colName := range colOrder {
		meta := block.Columns[colName]
		switch meta.Type {
		case "integer", "sequence":
			cdTypes[i] = types.Int
		case "float":
			cdTypes[i] = types.Float
		default:
			cdTypes[i] = types.Bytes
		}
	}

	return func(batchIdx int, cb coldata.Batch, _ *bufalloc.ByteAllocator) {
		total := block.Count
		start := batchIdx * batchSize
		end := start + batchSize
		if end > total {
			end = total
		}
		n := end - start

		// 1) Reset the vector; n rows, cdTypes[i] per column.
		cb.Reset(cdTypes, n, coldata.StandardColumnFactory)

		// 2) Instantiate one Generator per column, seeded by batchIdx.
		gens := make([]Generator, len(colOrder))
		for i, colName := range colOrder {
			meta := block.Columns[colName]
			gens[i] = buildGenerator(meta, batchIdx, baseBatchSize, fullSchema)
		}

		// 3) Fill row-by-row.
		for row := 0; row < n; row++ {
			for i, cdType := range cdTypes {
				raw := gens[i].Next()

				vec := cb.ColVec(i)
				nulls := vec.Nulls()
				if raw == "" {
					nulls.SetNull(row)
					continue
				}
				switch cdType.Family() {
				case types.IntFamily:
					// For sequences & integers.
					iv, err := strconv.ParseInt(raw, 10, 64)
					if err != nil {
						panic(fmt.Sprintf("parse int: %v", err))
					}
					vec.Int64()[row] = iv

				case types.FloatFamily:
					// For floats, decimals, numerics, doubles.
					fv, err := strconv.ParseFloat(raw, 64)
					if err != nil {
						panic(fmt.Sprintf("parse float: %v", err))
					}
					vec.Float64()[row] = fv

				default:
					// For strings, json, uuid, timestamp → bytes.
					bytesVec := vec.Bytes()
					bytesVec.Set(row, []byte(raw))
				}
			}
		}
	}
}

func (w *workloadGenerator) Ops(
	ctx context.Context, urls []string, reg *histogram.Registry,
) (workload.QueryLoad, error) {
	// Database name is set in the main workload struct.
	w.setDbName()
	// The schema information is loaded into memory from the yaml. This information is used for the runtime data generation.
	errYaml, done := w.loadYamlData()
	if done {
		return workload.QueryLoad{}, errYaml
	}
	// Transactions from the SQL file are parsed.
	readPath := filepath.Join(w.outputDir, fmt.Sprintf("%s_read.sql", w.dbName))
	writePath := filepath.Join(w.outputDir, fmt.Sprintf("%s_write.sql", w.dbName))
	readTransactions, errRead := readSQL(readPath, "read")
	writeTransactions, errWrite := readSQL(writePath, "write")
	if errRead != nil {
		return workload.QueryLoad{}, errRead
	}
	if errWrite != nil {
		return workload.QueryLoad{}, errWrite
	}

	if len(urls) == 0 {
		return workload.QueryLoad{}, fmt.Errorf("no database URLs provided")
	}
	db, err := gosql.Open("postgres", urls[0])
	if err != nil {
		return workload.QueryLoad{}, err
	}
	db.SetMaxOpenConns(w.connFlags.Concurrency + 1)
	db.SetMaxIdleConns(w.connFlags.Concurrency + 1)

	// The generators and column old data caches are initialised.
	if err := w.initGenerators(db); err != nil {
		return workload.QueryLoad{}, err
	}

	ql := workload.QueryLoad{}
	for i := 0; i < w.connFlags.Concurrency; i++ {
		worker := &txnWorker{
			db:                db,
			readTransactions:  readTransactions,
			writeTransactions: writeTransactions,
			rng:               rand.New(rand.NewPCG(uint64(timeutil.Now().UnixNano()), uint64(i))),
			hists:             reg.GetHandle(),
			generator:         w, // reference to the workloadGenerator for generators
		}
		ql.WorkerFns = append(ql.WorkerFns, worker.run)
	}
	ql.Close = func(context.Context) error { return db.Close() }
	return ql, nil
}

// run executes a random transaction from the list of transactions.
func (t *txnWorker) run(ctx context.Context) error {
	txn := t.chooseTransaction()
	// Time for the metrics is started.
	start := timeutil.Now()
	// Executing teh chosen transaction.
	err := crdb.ExecuteTx(ctx, t.db, nil, t.executeTransaction(ctx, txn))
	// The elapsed time is calculated for the transaction metrics.
	elapsed := timeutil.Since(start)
	t.hists.Get(fmt.Sprintf("typ_%v", txn.typ)).Record(elapsed)
	return err
}

func (t *txnWorker) executeTransaction(
	ctx context.Context, txn Transaction,
) func(tx *gosql.Tx) error {
	// Reference to the workloadGenerator for generators.
	w := t.generator
	return func(tx *gosql.Tx) error {
		// Inserted maintains a map of column names to the values that were inserted in this transaction.
		inserted := make(map[string][]interface{})
		queryCtx, cancel := context.WithTimeout(ctx, w.txTimeout)
		defer cancel()
		for _, sqlQuery := range txn.Queries {
			args := make([]interface{}, len(sqlQuery.Placeholders))
			// Checking if we have a situation where all the primary keys in the query have foreign key dependency.
			allPksAreFK := checkIfAllPkAreFk(sqlQuery, w)
			// Picking a single fkIdx for ALL FK placeholders (or -1 if none).
			// This ensures that for all column in a composite fk, the same row index from the parent column cache is chosen.
			fkIndex := t.pickForeignKeyIndex(sqlQuery, w)
			// Building per-placeholder indexes.
			indexes := t.setCacheIndex(sqlQuery, w, fkIndex)
			for i, placeholder := range sqlQuery.Placeholders {
				// Getting the value to be inserted for the placeholder.
				raw := getColumnValue(allPksAreFK, placeholder, w, inserted, indexes, i)
				// If the data was generated to be written, we need to insert it into the inserted map.
				if placeholder.Clause == insert {
					inserted[placeholder.Name] = append(inserted[placeholder.Name], raw)
				}
				// The value for the placeholder is set in the args slice based on the column type.
				err := setColumnValue(raw, placeholder, args, i)
				if err != nil {
					return err
				}
			}
			// The SQL query is ran with the args.
			if _, err := tx.ExecContext(queryCtx, sqlQuery.SQL, args...); err != nil {
				return err
			}
		}
		return nil
	}
}

// chooseTransaction returns a random transaction of type read or write based on the readPct flag.
func (t *txnWorker) chooseTransaction() Transaction {
	var txn Transaction
	reads := t.readTransactions
	writes := t.writeTransactions

	// If neither set has any transactions, just the zero-Txn is returned.
	if len(reads) == 0 && len(writes) == 0 {
		return txn
	}
	// If there are no read txns, a write transaction is always picked.
	if len(reads) == 0 {
		return writes[t.rng.IntN(len(writes))]
	}
	// If there are no write txns, a read transaction is always picked.
	if len(writes) == 0 {
		return reads[t.rng.IntN(len(reads))]
	}

	// If both are non-empty then txn is chosen based on readPct.
	if t.rng.IntN(100) < t.generator.readPct {
		return reads[t.rng.IntN(len(reads))]
	}
	return writes[t.rng.IntN(len(writes))]
}

// setDbName registers the database name in the main workload struct.
// It uses the name provided using the --db flag, otherwise falls back to "workload_generator".
func (w *workloadGenerator) setDbName() {
	dbName := w.Meta().Name
	if w.connFlags.DBOverride != "" {
		dbName = w.connFlags.DBOverride
	}
	w.dbName = dbName
}

// getRegularColumnValue picks values from the generator or cache depending on sql clause or whether there is a fk dependency.
func (w *workloadGenerator) getRegularColumnValue(p Placeholder, idx int) string {
	tableName := getTableName(p, w)
	key := fmt.Sprintf("%s.%s", tableName, p.Name)
	rc := w.columnGens[key]
	rc.mu.Lock()
	defer rc.mu.Unlock()

	// For filling in data for where clause or columns with foreign key constraints, we use the cache.
	if p.Clause == "WHERE" || (rc.columnMeta.HasForeignKey) {
		if len(rc.cache) > 0 {
			return rc.cache[idx]
		}
	}
	// For insert or update clauses, we use the generator to get a new value.
	v := rc.gen.Next()
	if len(rc.cache) < maxCacheSize {
		rc.cache = append(rc.cache, v)
	} else {
		rc.cache[rand.IntN(len(rc.cache))] = v
	}
	return v
}

// initGenerators seeds d.columnGens with both fresh generators and a cache
// of real values pulled from the live database.
func (w *workloadGenerator) initGenerators(db *gosql.DB) error {
	// globalNumBatches is needed to seed the generators.
	// 0-globalNumBatches-1 was used during initial bulk insertions.
	// So, globalNumBatches can be the batch index for run time generators.
	maxRows := 0
	for _, tblBlocks := range w.workloadSchema {
		if tblBlocks[0].Count > maxRows {
			maxRows = tblBlocks[0].Count
		}
	}
	// TODO: make the globalBatchNumber dynamic
	globalNumBatches := (maxRows + baseBatchSize - 1) / baseBatchSize

	// 1) The generator + empty cache for every table.col is built.
	w.buildRuntimeGenerators(globalNumBatches)

	// 2) Each cache is primed by selecting up to maxInitialCacheSize existing rows.
	// We do this column-by-column to keep it simple.
	err := w.setCacheValues(db)
	if err != nil {
		return err
	}
	return nil
}

// setCacheValues fills into the runtimeColumn structs the cache data consisting of values generated during the initial bulk insert.
func (w *workloadGenerator) setCacheValues(db *gosql.DB) error {
	for tableName, blocks := range w.workloadSchema {
		block := blocks[0]
		for _, colName := range block.ColumnOrder {
			key := fmt.Sprintf("%s.%s", tableName, colName)
			rc := w.columnGens[key]

			// Building a query like: SELECT colName FROM tableName LIMIT maxInitialCacheSize
			q := fmt.Sprintf(`SELECT %s FROM %s LIMIT %d`,
				pq.QuoteIdentifier(colName), pq.QuoteIdentifier(tableName), maxCacheSize)
			rows, err := db.QueryContext(context.Background(), q)
			if err != nil {
				return fmt.Errorf("priming cache for %s: %w", key, err)
			}

			for rows.Next() {
				var raw gosql.NullString
				if err := rows.Scan(&raw); err != nil {
					errClose := rows.Close()
					if errClose != nil {
						return errClose
					}
					return fmt.Errorf("scanning cache row for %s: %w", key, err)
				}
				if raw.Valid {
					rc.cache = append(rc.cache, raw.String)
				}
				if len(rc.cache) >= maxCacheSize {
					break
				}
			}
			errClose := rows.Close()
			if errClose != nil {
				return errClose
			}
		}
	}
	return nil
}

// buildRuntimeGenerators initializes the runtime generators for each column in the workload schema.
func (w *workloadGenerator) buildRuntimeGenerators(globalNumBatches int) {
	w.columnGens = make(map[string]*runtimeColumn)
	for tableName, blocks := range w.workloadSchema {
		block := blocks[0]
		for colName, meta := range block.Columns {
			key := fmt.Sprintf("%s.%s", tableName, colName)
			gen := buildGenerator(meta, globalNumBatches, baseBatchSize, w.workloadSchema)
			w.columnGens[key] = &runtimeColumn{
				gen:        gen,
				cache:      make([]string, 0, maxCacheSize),
				columnMeta: meta,
			}
		}
	}
}

// setCacheIndex sets the indexes for each placeholder in the SQL query.
// If the placeholder is a foreign key, it uses the fkIdx; otherwise, it picks a random index from the cache.
func (t *txnWorker) setCacheIndex(sqlQuery SQLQuery, d *workloadGenerator, fkIdx int) []int {
	indexes := make([]int, len(sqlQuery.Placeholders))
	for i, p := range sqlQuery.Placeholders {
		tableName := getTableName(p, d)
		key := fmt.Sprintf("%s.%s", tableName, p.Name)
		if d.columnGens[key].columnMeta.HasForeignKey {
			indexes[i] = fkIdx
		} else {
			cacheLen := len(d.columnGens[key].cache)
			indexes[i] = t.rng.IntN(cacheLen)
		}
	}
	return indexes
}

// pickForeignKeyIndex picks a random index from the cache of a foreign key column.
// It helps ensure that all members of a composite foreign key are set to the same parent row.
func (t *txnWorker) pickForeignKeyIndex(sqlQuery SQLQuery, d *workloadGenerator) int {
	fkIdx := -1
	for _, p := range sqlQuery.Placeholders {
		tableName := getTableName(p, d)
		key := fmt.Sprintf("%s.%s", tableName, p.Name)
		if d.columnGens[key].columnMeta.HasForeignKey {
			cacheLen := len(d.columnGens[key].cache)
			if cacheLen > 0 {
				fkIdx = t.rng.IntN(cacheLen)
			}
			break
		}
	}
	return fkIdx
}

// loadYamlData first checks whether the input yaml flag is set or not.
// Accordingly, it loads the schema information into memory from the correct location.
func (w *workloadGenerator) loadYamlData() (error, bool) {
	var path string
	if w.inputYAML != "" {
		path = w.inputYAML
	} else {
		path = fmt.Sprintf("schema_%s.yaml", w.dbName)
	}
	raw, err := os.ReadFile(path)
	if err != nil {
		return errors.Wrapf(err, "could not read schema YAML from %s", path), true
	}
	if err := yaml.UnmarshalStrict(raw, &w.workloadSchema); err != nil {
		return errors.Wrapf(err, "couldn't unmarshal schema YAML"), true
	}
	return nil, false
}
