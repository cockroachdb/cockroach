// Copyright 2025 The Cockroach Authors.
//
// Use of this software is governed by the CockroachDB Software License
// included in the /LICENSE file.

package workload_generator

import (
	"context"
	gosql "database/sql"
	"fmt"
	"math/rand/v2"
	"os"
	"strconv"
	"strings"

	"github.com/cockroachdb/cockroach/pkg/col/coldata"
	"github.com/cockroachdb/cockroach/pkg/sql/types"
	"github.com/cockroachdb/cockroach/pkg/util/bufalloc"
	"github.com/cockroachdb/cockroach/pkg/workload"
	"github.com/cockroachdb/cockroach/pkg/workload/histogram"
	"github.com/cockroachdb/errors"
	"github.com/spf13/pflag"
	"gopkg.in/yaml.v2"
)

const (
	// placeholder
	fieldLength = 100
	//baseBatchSize is used during data generation. Batch sizes vary based on number of rows
	baseBatchSize = 100
)

func init() {
	workload.Register(dbworkloadMeta)
}

var dbworkloadMeta = workload.Meta{
	Name:        "workload_generator",
	Description: "workloadGeneratorStruct tries to generate workloads from debug zip",
	Version:     "1.0.0",
	New: func() workload.Generator {
		g := &workloadGeneratorStruct{}
		g.flags.FlagSet = pflag.NewFlagSet("db_workload", pflag.ContinueOnError)
		g.flags.StringVar(&g.debugZip, "debug-zip", "",
			"path to unzipped debug zip")
		g.flags.StringVar(&g.dbName, "db-name", "", "database name")
		g.flags.IntVar(&g.rowCount, "rows", 1000,
			"base number of rows per table before FK‐depth scaling")
		g.flags.StringVar(&g.outputYaml, "output-yaml", "", "path for storing the yaml for user modifications")
		g.flags.StringVar(&g.inputYaml, "input-yaml", "", "path to read the user modified yaml")
		g.flags.StringVar(&g.sqlLocation, "sql", "", "location to write and read the <schema>.sql file")
		g.flags.IntVar(&g.readPct, "read-pct", 50, " percentage of read transactions(0-100)")
		g.flags.Meta = map[string]workload.FlagMeta{}
		g.connFlags = workload.NewConnFlags(&g.flags)
		return g
	},
}

type workloadGeneratorStruct struct {
	flags     workload.Flags
	connFlags *workload.ConnFlags
	debugZip  string // path to the unzipped debug zip file
	dbName    string // database name to use for the workload
	rowCount  int    // base number of rows per table before FK‐depth scaling

	allSchema      map[string]*TableSchema //the table-schema map generated by parsing ddl statements
	createStmts    map[string]string       // table name → DDL string
	workloadSchema Schema                  // table name → []TableBlock : the yaml marshal-able schema

	inputYaml   string //path to read the user modified yaml
	outputYaml  string //path for storing teh yaml for user modifications
	sqlLocation string // location to write and read the <schema>.sql file
	readPct     int    // percentage of read transactions
}

// Meta implements the Generator interface.
func (*workloadGeneratorStruct) Meta() workload.Meta { return dbworkloadMeta }

// Flags implements the Flagser interface.
func (w *workloadGeneratorStruct) Flags() workload.Flags {
	return workload.Flags{
		FlagSet: w.flags.FlagSet,
	}
}

// ConnFlags implements the ConnFlagser interface.
func (w *workloadGeneratorStruct) ConnFlags() *workload.ConnFlags {
	return w.connFlags
}

func (w *workloadGeneratorStruct) Hooks() workload.Hooks {
	return workload.Hooks{
		//before data generation begins, we have to parse the DDLs and generate the schema required for further steps
		PreCreate: func(db *gosql.DB) error {
			//1)Setting up the database name to be used.
			//default for fall back
			dbName := w.Meta().Name
			//using the db override if provided
			if w.connFlags.DBOverride != "" {
				dbName = w.connFlags.DBOverride
			}
			w.dbName = dbName

			//2)Getting the debug zip location from the flags.
			debug := w.debugZip

			//3)Getting the schema and the DDl themselves from the debug folder.
			schemas, createStmts, errDDL := GenerateDDLs(debug, dbName, false)
			w.allSchema = schemas
			w.createStmts = createStmts
			if errDDL != nil {
				return errors.Wrap(errDDL, "failed to generate DDLs")
			}

			//4)Getting teh yaml marshal-able schema for data generation.
			w.workloadSchema = buildWorkloadSchema(w.allSchema, dbName, w.rowCount)
			yamlData, err := yaml.Marshal(w.workloadSchema)
			if err != nil {
				return errors.Wrap(err, "failed to marshal workload schema to YAML")
			}
			//if a location was given, write the yaml there, otherwise writing in current directory
			var outPath string
			if w.outputYaml != "" {
				outPath = fmt.Sprintf("%s/schema_%s.yaml", w.outputYaml, dbName)
			} else {
				outPath = fmt.Sprintf("schema_%s.yaml", dbName)
			}
			if err := os.WriteFile(outPath, yamlData, 0644); err != nil {
				return errors.Wrapf(err, "could not write schema YAML to %s", outPath)
			}

			//5)If alternate schema is provided (input-yaml), reload it.
			if w.inputYaml != "" {
				raw, err := os.ReadFile(w.inputYaml)
				if err != nil {
					panic(errors.Wrapf(err, "failed to read file %s", w.inputYaml))
				}
				if err := yaml.UnmarshalStrict(raw, &w.workloadSchema); err != nil {
					panic(errors.Wrapf(err, "failed to unmarshal yaml %s", w.inputYaml))
				}
				fmt.Println("First 5 liens of the yaml:")
				lines := strings.Split(string(raw), "\n")
				for i := 0; i < len(lines) && i < 5; i++ {
					fmt.Println(lines[i])
				}
			}
			//TODO: wire in GenerateWorkload(s.debugZip, s.allSchema, s.dbName, s.sqlLocation) for sql handling in later PR

			return nil
		},
	}
}

// Tables implements workload.Generator.Tables
func (w *workloadGeneratorStruct) Tables() []workload.Table {
	//if we are in yaml output mode, emit no tables
	if w.outputYaml != "" {
		return []workload.Table{}
	}

	//tableOrder is the order in which the DDLs of different tables should be executed.
	//It currently assumes that the order in which the DDLs show up in the debug folder is the correct sequence
	tableOrder := getTableOrder(w.workloadSchema)
	// Finding the number of batches to generate for each table.
	// We use the maximum number of rows in any table to determine the number of batches.
	// Currently, we maintain the same number of batches across all tables, vary the batchSize if the rowCount varies.
	// Since our generators seeds are made using batch numbers, this helps to ensure foreign key constraints are held.
	maxRows := 0
	for _, tblBlocks := range w.workloadSchema {
		if tblBlocks[0].Count > maxRows {
			maxRows = tblBlocks[0].Count
		}
	}
	globalNumBatches := (maxRows + baseBatchSize - 1) / baseBatchSize

	var tables []workload.Table
	for _, tableName := range tableOrder {
		blocks := w.workloadSchema[tableName]
		block := blocks[0]
		total := block.Count
		batchSizeI := total / globalNumBatches

		tables = append(tables, workload.Table{
			Name:   tableName,
			Schema: generateSchema(w.createStmts[tableName]), // This is the short DDL string , i.e. , without teh preceding CREATE TABLE ... and proceeding RETURNING ...
			InitialRows: workload.BatchedTuples{
				NumBatches: globalNumBatches,
				FillBatch:  generateBatch(tableName, block, w.workloadSchema, batchSizeI, w),
			},
		})
	}
	return tables
}

// getTableOrder returns the order of tables based on their TableNumber in the schema.
// The table number was assigned based on position in the debug file.
func getTableOrder(rawSchema Schema) []string {
	tableOrder := make([]string, len(rawSchema))
	for tableName, blocks := range rawSchema {
		block := blocks[0]
		tableOrder[block.TableNumber] = tableName
	}
	return tableOrder
}

// generateSchema extracts the schema part from a CREATE TABLE statement.
// It strips the CREATE TABLE part and returns the column definitions and table constraints
func generateSchema(createStmt string) string {
	// find the first '('
	start := strings.Index(createStmt, "(")
	if start < 0 {
		return "" // or panic/empty, as you prefer
	}
	// walk forward, counting nested parens until we close the top‐level one
	depth := 0
	var end int
	for i := start; i < len(createStmt); i++ {
		switch createStmt[i] {
		case '(':
			depth++
		case ')':
			depth--
			if depth == 0 {
				end = i
				// once we've closed the top‐level '(', we're done
				return createStmt[start : end+1]
			}
		}
	}
	// if we get here, the DDL was malformed (unbalanced parens)
	return createStmt[start:]
}

// generateBatch returns the FillBatch func for one table
// It generates a batch of data for the given table block
// INPUTS-
//   - tableName: the name of the table
//   - block: the TableBlock containing metadata about the table
//   - fullSchema: the full schema of the workload (the full amp of all TableBlocks)
//   - batchSize: the size of each batch to generate, since different tables have different batchSizes
//   - d: the dbworkload instance, which contains the generators and caches
func generateBatch(
	tableName string, block TableBlock, fullSchema Schema, batchSize int, w *workloadGeneratorStruct,
) func(batchIdx int, cb coldata.Batch, _ *bufalloc.ByteAllocator) {
	// determine the Cockroach columnar types and a stable column order
	colOrder := block.ColumnOrder
	// build Cockroach types in that order:
	cdTypes := make([]*types.T, len(colOrder))
	for i, colName := range colOrder {
		meta := block.Columns[colName]
		switch meta.Type {
		case "integer", "sequence":
			cdTypes[i] = types.Int
		case "float":
			cdTypes[i] = types.Float
		default:
			cdTypes[i] = types.Bytes
		}
	}

	return func(batchIdx int, cb coldata.Batch, _ *bufalloc.ByteAllocator) {
		total := block.Count
		start := batchIdx * batchSize
		end := start + batchSize
		if end > total {
			end = total
		}
		n := end - start

		//1) reset the vector; n rows, cdTypes[i] per column
		cb.Reset(cdTypes, n, coldata.StandardColumnFactory)

		// 2) instantiate one Generator per column, seeded by batchIdx
		gens := make([]Generator, len(colOrder))
		for i, colName := range colOrder {
			meta := block.Columns[colName]
			gens[i] = buildGenerator(meta, batchIdx, baseBatchSize, fullSchema)
		}

		// 3) fill row-by-row
		for row := 0; row < n; row++ {
			for i, cdType := range cdTypes {
				raw := gens[i].Next()

				vec := cb.ColVec(i)
				nulls := vec.Nulls()
				if raw == "" {
					nulls.SetNull(row)
					continue
				}
				switch cdType.Family() {
				case types.IntFamily:
					// For sequences & integers.
					iv, err := strconv.ParseInt(raw, 10, 64)
					if err != nil {
						panic(fmt.Sprintf("parse int: %v", err))
					}
					vec.Int64()[row] = iv

				case types.FloatFamily:
					// For floats, decimals, numerics, doubles.
					fv, err := strconv.ParseFloat(raw, 64)
					if err != nil {
						panic(fmt.Sprintf("parse float: %v", err))
					}
					vec.Float64()[row] = fv

				default:
					// For strings, json, uuid, timestamp → bytes.
					bytesVec := vec.Bytes()
					bytesVec.Set(row, []byte(raw))
				}
			}
		}
	}
}

// Ops function is a placeholder for now.
func (w *workloadGeneratorStruct) Ops(
	ctx context.Context, urls []string, reg *histogram.Registry,
) (workload.QueryLoad, error) {
	sqlDatabase, err := gosql.Open("postgres", urls[0])
	if err != nil {
		return workload.QueryLoad{}, err
	}

	db := sqlDatabase

	readStmt, err := db.Prepare("SELECT field1, field2, field3 FROM simple WHERE id = $1")
	if err != nil {
		return workload.QueryLoad{}, err
	}

	updateStmt, err := db.Prepare("UPDATE simple SET field1 = $1, field2 = $2, field3 = $3 WHERE id = $4")
	if err != nil {
		return workload.QueryLoad{}, err
	}

	ql := workload.QueryLoad{
		WorkerFns: make([]func(context.Context) error, 8),
	}
	initialRowCount := int64(10)
	for i := range ql.WorkerFns {
		workerID := i
		rng := rand.New(rand.NewPCG(uint64(workerID), 0))
		ql.WorkerFns[i] = func(ctx context.Context) error {
			// 50% reads, 50% updates
			if rng.IntN(2) == 0 {
				// Read operation
				id := rng.Int64N(initialRowCount)
				var field1, field2, field3 string
				if err := readStmt.QueryRowContext(ctx, id).Scan(&field1, &field2, &field3); err != nil {
					return err
				}
			} else {
				// Update operation
				id := rng.Int64N(initialRowCount)
				field1 := randString(rng, fieldLength)
				field2 := randString(rng, fieldLength)
				field3 := randString(rng, fieldLength)
				if _, err := updateStmt.ExecContext(ctx, field1, field2, field3, id); err != nil {
					return err
				}
			}
			return nil
		}
	}

	// Close statements and database when the workload finishes
	ql.Close = func(context.Context) error {
		err := readStmt.Close()
		if err != nil {
			return err
		}
		err = updateStmt.Close()
		if err != nil {
			return err
		}
		return sqlDatabase.Close()
	}

	return ql, nil
}

func randString(rng *rand.Rand, length int) []byte {
	const letters = "abcdefghijklmnopqrstuvwxyz"
	result := make([]byte, length)
	for i := range result {
		result[i] = letters[rng.IntN(len(letters))]
	}
	return result
}
