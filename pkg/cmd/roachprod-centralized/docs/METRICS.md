# Prometheus Metrics Reference

This document provides a comprehensive reference for all Prometheus metrics exposed by roachprod-centralized.

**üìö Related Documentation:**
- [‚Üê Back to Main README](../README.md)
- [üèóÔ∏è Architecture](ARCHITECTURE.md) - System architecture overview
- [üîå API Reference](API.md) - REST API documentation
- [üìã Tasks Service](services/TASKS.md) - Background task processing

## Table of Contents

- [Overview](#overview)
- [Accessing Metrics](#accessing-metrics)
- [Grafana Dashboard](#grafana-dashboard)
- [Tasks Service Metrics](#tasks-service-metrics)
- [Database Connection Pool Metrics](#database-connection-pool-metrics)
- [HTTP Server Metrics (Gin)](#http-server-metrics-gin)
- [Go Runtime Metrics](#go-runtime-metrics)
- [Metric Labels](#metric-labels)
- [Querying Metrics](#querying-metrics)
- [Alerting Rules](#alerting-rules)

## Overview

roachprod-centralized exposes Prometheus metrics for monitoring application health, performance, and task processing. Metrics are exposed on the `/metrics` endpoint and follow Prometheus naming conventions.

### Metric Categories

1. **Tasks Service Metrics** - Task processing and queue statistics
2. **Database Connection Pool Metrics** - SQL database connection pool statistics
3. **HTTP Server Metrics** - Gin framework request metrics
4. **Go Runtime Metrics** - Standard Go process metrics

### Metric Namespace

All tasks service metrics use the namespace `roachprod`.

## Accessing Metrics

### Endpoints

**All-in-One Mode / API Mode:**
```bash
curl http://localhost:8080/metrics
```

**Workers-Only Mode:**
```bash
curl http://localhost:9090/metrics
```

### Configuration

Metrics collection can be controlled via service options:

```go
taskService := tasks.NewService(repo, instanceID, tasks.Options{
    CollectMetrics:           true,              // Enable/disable metrics (default: true)
    StatisticsUpdateInterval: 30 * time.Second,  // Update frequency (default: 30s)
})
```

Environment variable:
```bash
export ROACHPROD_TASKS_COLLECT_METRICS=false
```

## Grafana Dashboard

A comprehensive Grafana dashboard is available for visualizing all metrics. The dashboard includes:

- **Task Queue Overview** - Task states, completion rates, worker utilization
- **Task Performance** - Processing duration, queue age, timeouts
- **API Performance** - Request rates, latency, error rates
- **System Resources** - Memory, goroutines, GC metrics
- **Database Connection Pool** - Connection usage, pool health, wait times

### Dashboard Configuration

The dashboard JSON is located at:
```
docs/grafana-dashboard.json
```

Import this file into Grafana to create the dashboard. It requires:
- A Prometheus datasource configured with your metrics endpoint
- The `service` label configured on your metrics (e.g., `roachprod-centralized-prod`, `roachprod-centralized-test`)

### Dashboard Screenshot

![Grafana Dashboard](grafana-dashboard.png)

### Dashboard Variables

The dashboard includes the following variables:
- **Datasource** - Select your Prometheus datasource
- **Service** - Filter metrics by service/deployment name (e.g., `roachprod-centralized-prod`, `roachprod-centralized-test`)
- **State** - Filter task metrics by state (default: All)

## Tasks Service Metrics

All tasks service metrics use the namespace `roachprod_tasks`.

### Task State Gauges

#### `roachprod_tasks_total`

**Type:** Gauge
**Description:** Current number of tasks in the database by state and type
**Labels:**
- `state` - Task state: `pending`, `running`, `done`, `failed`
- `task_type` - Type of task (e.g., `cluster_sync`, `dns_sync`, `health_cleanup`, `tasks_purge`)

**Example:**
```promql
# Total pending tasks across all types
sum(roachprod_tasks_total{state="pending"})

# Pending cluster_sync tasks
roachprod_tasks_total{state="pending",task_type="cluster_sync"}

# All failed tasks
sum(roachprod_tasks_total{state="failed"})

# Tasks by state (breakdown)
sum(roachprod_tasks_total) by (state)
```

**Use Cases:**
- Monitor task queue depth
- Detect task processing bottlenecks
- Track failed task accumulation
- Per-task-type workload analysis

---

### Task Completion Counter

#### `roachprod_tasks_completion_status_total`

**Type:** Counter
**Description:** Total number of completed tasks by type and outcome
**Labels:**
- `task_type` - Type of task
- `status` - Completion status: `success` or `failure`
- `instance` - Instance ID that processed the task

**Example:**
```promql
# Task completion rate (all types)
sum(rate(roachprod_tasks_completion_status_total[5m]))

# Success rate for cluster_sync tasks
rate(roachprod_tasks_completion_status_total{task_type="cluster_sync",status="success"}[5m])

# Failure rate
sum(rate(roachprod_tasks_completion_status_total{status="failure"}[5m]))

# Success ratio (percentage)
sum(rate(roachprod_tasks_completion_status_total{status="success"}[5m]))
/
sum(rate(roachprod_tasks_completion_status_total[5m]))
* 100
```

**Use Cases:**
- Calculate task success/failure rates
- Monitor task throughput per instance
- Track task reliability by type

---

### Task Processing Duration

#### `roachprod_tasks_processing_duration_seconds`

**Type:** Histogram
**Description:** Time taken to process tasks from start to completion
**Labels:**
- `task_type` - Type of task

**Buckets:** `[0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8, 25.6, 51.2, 102.4]` seconds

**Example:**
```promql
# Median processing duration (all types)
histogram_quantile(0.5, sum(rate(roachprod_tasks_processing_duration_seconds_bucket[5m])) by (le))

# 95th percentile for cluster_sync tasks
histogram_quantile(0.95,
  sum(rate(roachprod_tasks_processing_duration_seconds_bucket{task_type="cluster_sync"}[5m])) by (le)
)

# Average processing time by type
sum(rate(roachprod_tasks_processing_duration_seconds_sum[5m])) by (task_type)
/
sum(rate(roachprod_tasks_processing_duration_seconds_count[5m])) by (task_type)

# 99th percentile (slowest tasks)
histogram_quantile(0.99, sum(rate(roachprod_tasks_processing_duration_seconds_bucket[5m])) by (le))
```

**Use Cases:**
- Monitor task processing performance
- Detect slow tasks
- Capacity planning based on task duration
- SLO monitoring

---

### Task Queue Age

#### `roachprod_tasks_queue_age_seconds`

**Type:** Histogram
**Description:** Time tasks spend in pending state before processing starts
**Labels:**
- `task_type` - Type of task

**Buckets:** `[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]` seconds

**Example:**
```promql
# Median queue wait time
histogram_quantile(0.5, sum(rate(roachprod_tasks_queue_age_seconds_bucket[5m])) by (le))

# 99th percentile queue age (worst case)
histogram_quantile(0.99, sum(rate(roachprod_tasks_queue_age_seconds_bucket[5m])) by (le))

# Average queue time by task type
sum(rate(roachprod_tasks_queue_age_seconds_sum[5m])) by (task_type)
/
sum(rate(roachprod_tasks_queue_age_seconds_count[5m])) by (task_type)

# Tasks waiting more than 1 minute
sum(rate(roachprod_tasks_queue_age_seconds_bucket{le="60"}[5m]))
```

**Use Cases:**
- Monitor task queue latency
- Detect worker capacity issues
- Identify backlog buildup
- Worker scaling decisions

---

### Task Timeouts

#### `roachprod_tasks_timeouts_total`

**Type:** Counter
**Description:** Total number of tasks that timed out during processing
**Labels:**
- `task_type` - Type of task that timed out

**Example:**
```promql
# Timeout rate (all types)
sum(rate(roachprod_tasks_timeouts_total[5m]))

# Timeouts for specific task type
rate(roachprod_tasks_timeouts_total{task_type="cluster_sync"}[5m])

# Total timeouts in last hour
increase(roachprod_tasks_timeouts_total[1h])

# Timeouts by task type (breakdown)
sum(rate(roachprod_tasks_timeouts_total[5m])) by (task_type)
```

**Use Cases:**
- Monitor task timeout frequency
- Identify tasks needing longer timeouts
- Detect infrastructure issues causing timeouts

---

### Active Workers

#### `roachprod_tasks_active_workers`

**Type:** Gauge
**Description:** Number of workers currently processing tasks
**Labels:**
- `instance` - Instance ID

**Example:**
```promql
# Active workers across all instances
sum(roachprod_tasks_active_workers)

# Worker utilization percentage
roachprod_tasks_active_workers / roachprod_tasks_max_workers * 100

# Instances with idle workers
roachprod_tasks_active_workers < 1

# Per-instance worker count
roachprod_tasks_active_workers
```

**Use Cases:**
- Monitor worker utilization
- Capacity planning
- Detect worker bottlenecks

---

### Maximum Workers

#### `roachprod_tasks_max_workers`

**Type:** Gauge
**Description:** Maximum number of workers configured per instance
**Labels:**
- `instance` - Instance ID

**Example:**
```promql
# Total worker capacity across all instances
sum(roachprod_tasks_max_workers)

# Worker pool utilization percentage
sum(roachprod_tasks_active_workers) / sum(roachprod_tasks_max_workers) * 100

# Per-instance capacity
roachprod_tasks_max_workers
```

**Use Cases:**
- Track configured worker capacity
- Calculate utilization percentages
- Capacity planning and scaling decisions

---

## Database Connection Pool Metrics

When using CockroachDB or PostgreSQL backend (database type `cockroachdb`), connection pool statistics are automatically exported. These metrics use the official Prometheus Go SQL collector.

All database metrics use the namespace `go_sql` and include a `db_name` label with the database name extracted from the connection URL.

### Maximum Open Connections

#### `go_sql_max_open_connections`

**Type:** Gauge
**Description:** Maximum number of open database connections configured
**Labels:**
- `db_name` - Database name

**Example:**
```promql
# Maximum connections for roachprod database
go_sql_max_open_connections{db_name="roachprod"}
```

---

### Open Connections

#### `go_sql_open_connections`

**Type:** Gauge
**Description:** Current number of established database connections (both in use and idle)
**Labels:**
- `db_name` - Database name

**Example:**
```promql
# Total open connections
go_sql_open_connections{db_name="roachprod"}

# Connection pool utilization percentage
go_sql_open_connections / go_sql_max_open_connections * 100
```

---

### In-Use Connections

#### `go_sql_in_use_connections`

**Type:** Gauge
**Description:** Number of database connections currently executing queries
**Labels:**
- `db_name` - Database name

**Example:**
```promql
# Active database connections
go_sql_in_use_connections{db_name="roachprod"}

# Pool saturation percentage
go_sql_in_use_connections / go_sql_max_open_connections * 100
```

---

### Idle Connections

#### `go_sql_idle_connections`

**Type:** Gauge
**Description:** Number of idle database connections in the pool
**Labels:**
- `db_name` - Database name

**Example:**
```promql
# Idle connections available
go_sql_idle_connections{db_name="roachprod"}

# Connection pool health (in use vs idle)
go_sql_in_use_connections / (go_sql_in_use_connections + go_sql_idle_connections)
```

---

### Wait Count

#### `go_sql_wait_count_total`

**Type:** Counter
**Description:** Total number of times a query had to wait for an available connection
**Labels:**
- `db_name` - Database name

**Example:**
```promql
# Connection wait rate
rate(go_sql_wait_count_total{db_name="roachprod"}[5m])

# Total waits in last hour
increase(go_sql_wait_count_total{db_name="roachprod"}[1h])
```

**Use Cases:**
- Detect connection pool saturation
- Identify need to increase max connections
- Monitor connection contention

---

### Wait Duration

#### `go_sql_wait_duration_seconds_total`

**Type:** Counter
**Description:** Total time queries spent waiting for available connections
**Labels:**
- `db_name` - Database name

**Example:**
```promql
# Average wait duration per query
rate(go_sql_wait_duration_seconds_total{db_name="roachprod"}[5m])
/
rate(go_sql_wait_count_total{db_name="roachprod"}[5m])

# Total wait time in last hour
increase(go_sql_wait_duration_seconds_total{db_name="roachprod"}[1h])
```

**Use Cases:**
- Measure connection wait latency
- Identify connection pool performance issues
- Optimize max connections setting

---

### Connection Closure Metrics

The following counters track connections closed for various reasons:

#### `go_sql_max_idle_closed_total`
**Type:** Counter
**Description:** Connections closed due to `SetMaxIdleConns` limit

#### `go_sql_max_idle_time_closed_total`
**Type:** Counter
**Description:** Connections closed due to `SetConnMaxIdleTime` timeout

#### `go_sql_max_lifetime_closed_total`
**Type:** Counter
**Description:** Connections closed due to `SetConnMaxLifetime` limit

**Example:**
```promql
# Rate of connections closed by reason
sum(rate(go_sql_max_idle_closed_total{db_name="roachprod"}[5m])) by (db_name)
sum(rate(go_sql_max_idle_time_closed_total{db_name="roachprod"}[5m])) by (db_name)
sum(rate(go_sql_max_lifetime_closed_total{db_name="roachprod"}[5m])) by (db_name)
```

---

## HTTP Server Metrics (Gin)

These metrics are automatically provided by the `github.com/penglongli/gin-metrics` library.

### Request Total

#### `gin_request_total`

**Type:** Counter
**Description:** Total number of HTTP requests
**Labels:** (gin-metrics standard labels)

**Example:**
```promql
# Request rate
rate(gin_request_total[5m])

# Total requests in last hour
increase(gin_request_total[1h])
```

---

### URI Request Total

#### `gin_uri_request_total`

**Type:** Counter
**Description:** Total number of HTTP requests by URI path
**Labels:**
- `method` - HTTP method (GET, POST, etc.)
- `uri` - Request URI pattern
- `code` - HTTP status code

**Example:**
```promql
# Request rate by endpoint
sum(rate(gin_uri_request_total[5m])) by (uri)

# Error rate (4xx and 5xx responses)
sum(rate(gin_uri_request_total{code=~"[45].."}[5m]))

# Success rate for specific endpoint
rate(gin_uri_request_total{uri="/api/v1/clusters",code="200"}[5m])

# Top 5 endpoints by traffic
topk(5, sum(rate(gin_uri_request_total[5m])) by (uri))
```

---

### Request Duration

#### `gin_request_duration_seconds`

**Type:** Histogram
**Description:** HTTP request latency distribution
**Labels:**
- `method` - HTTP method
- `uri` - Request URI pattern

**Example:**
```promql
# Median request latency
histogram_quantile(0.5, sum(rate(gin_request_duration_seconds_bucket[5m])) by (le))

# 95th percentile latency by endpoint
histogram_quantile(0.95,
  sum(rate(gin_request_duration_seconds_bucket[5m])) by (uri, le)
)

# Average request duration
sum(rate(gin_request_duration_seconds_sum[5m]))
/
sum(rate(gin_request_duration_seconds_count[5m]))
```

---

### Request Body Size

#### `gin_request_body_size_bytes`

**Type:** Histogram
**Description:** HTTP request body size distribution
**Labels:**
- `method` - HTTP method
- `uri` - Request URI pattern

---

### Response Body Size

#### `gin_response_body_size_bytes`

**Type:** Histogram
**Description:** HTTP response body size distribution
**Labels:**
- `method` - HTTP method
- `uri` - Request URI pattern

---

## Go Runtime Metrics

Standard Go process metrics are automatically exposed by the Prometheus client library.

### Memory Metrics

- `go_memstats_alloc_bytes` - Bytes allocated and still in use
- `go_memstats_heap_alloc_bytes` - Heap bytes allocated
- `go_memstats_heap_inuse_bytes` - Heap bytes in use
- `go_memstats_stack_inuse_bytes` - Stack bytes in use
- `go_memstats_sys_bytes` - Total bytes obtained from system

### Goroutine Metrics

- `go_goroutines` - Number of goroutines currently running

### GC Metrics

- `go_gc_duration_seconds` - GC pause duration
- `go_memstats_gc_cpu_fraction` - Fraction of CPU time used by GC
- `go_memstats_last_gc_time_seconds` - Timestamp of last GC

### Process Metrics

- `process_cpu_seconds_total` - Total CPU time consumed
- `process_resident_memory_bytes` - Resident memory size
- `process_virtual_memory_bytes` - Virtual memory size
- `process_open_fds` - Number of open file descriptors

## Metric Labels

### Task-Specific Labels

| Label | Description | Cardinality | Example Values | Metrics |
|-------|-------------|-------------|----------------|---------|
| `state` | Task state | 4 | `pending`, `running`, `done`, `failed` | `tasks_total` |
| `task_type` | Type of task | ~10-20 | `cluster_sync`, `dns_sync`, `health_cleanup`, `tasks_purge` | All task metrics |
| `status` | Completion status | 2 | `success`, `failure` | `tasks_completion_status_total` |
| `instance` | Instance ID | N instances | `api-instance-1`, `worker-instance-2` | `tasks_completion_status_total`, `tasks_active_workers`, `tasks_max_workers` |

### Database-Specific Labels

| Label | Description | Cardinality | Example Values | Metrics |
|-------|-------------|-------------|----------------|---------|
| `db_name` | Database name | 1 | `roachprod`, `roachprod_centralized` | All `go_sql_*` metrics |

### Cardinality Considerations

**Total Task Metrics Cardinality:**
- `tasks_total`: 4 states √ó ~15 task types = **~60 time series**
- `tasks_completion_status_total`: ~15 task types √ó 2 statuses √ó N instances = **~30 √ó N time series**
- `tasks_processing_duration_seconds`: ~15 task types √ó buckets = **moderate**
- `tasks_queue_age_seconds`: ~15 task types √ó buckets = **moderate**
- `tasks_timeouts_total`: ~15 task types = **~15 time series**
- `tasks_active_workers`: N instances = **N time series**
- `tasks_max_workers`: N instances = **N time series**

**Total Database Metrics Cardinality:**
- `go_sql_*`: 9 metrics √ó 1 database = **~9 time series**

With 10 instances and 15 task types, total cardinality is **~1,000-2,000 time series** - well within acceptable limits.

## Querying Metrics

### Common Queries

#### Task Queue Health

```promql
# Tasks waiting to be processed
sum(roachprod_tasks_total{state="pending"})

# Tasks currently running
sum(roachprod_tasks_total{state="running"})

# Failed tasks requiring attention
sum(roachprod_tasks_total{state="failed"})

# Task distribution by state
sum(roachprod_tasks_total) by (state)
```

#### Task Processing Performance

```promql
# Task completion rate (tasks/second)
sum(rate(roachprod_tasks_completion_status_total[5m]))

# Task success rate (percentage)
sum(rate(roachprod_tasks_completion_status_total{status="success"}[5m]))
/
sum(rate(roachprod_tasks_completion_status_total[5m]))
* 100

# Average task duration by type
sum(rate(roachprod_tasks_processing_duration_seconds_sum[5m])) by (task_type)
/
sum(rate(roachprod_tasks_processing_duration_seconds_count[5m])) by (task_type)

# Slowest task types (p95)
topk(5,
  histogram_quantile(0.95,
    sum(rate(roachprod_tasks_processing_duration_seconds_bucket[5m])) by (task_type, le)
  )
)
```

#### Worker Utilization

```promql
# Total active workers across all instances
sum(roachprod_tasks_active_workers)

# Worker utilization percentage (assuming 5 workers per instance)
sum(roachprod_tasks_active_workers)
/
(count(roachprod_tasks_active_workers) * 5)
* 100

# Instances with maximum workers active
roachprod_tasks_active_workers >= 5
```

#### API Performance

```promql
# Request rate by endpoint
sum(rate(gin_uri_request_total[5m])) by (uri)

# Error rate (percentage)
sum(rate(gin_uri_request_total{code=~"[45].."}[5m]))
/
sum(rate(gin_uri_request_total[5m]))
* 100

# 95th percentile API latency
histogram_quantile(0.95, sum(rate(gin_request_duration_seconds_bucket[5m])) by (le))

# Slowest endpoints (p95 latency)
topk(5,
  histogram_quantile(0.95,
    sum(rate(gin_request_duration_seconds_bucket[5m])) by (uri, le)
  )
)
```

#### Database Connection Pool Health

```promql
# Connection pool utilization percentage
go_sql_in_use_connections{db_name="roachprod"}
/
go_sql_max_open_connections{db_name="roachprod"}
* 100

# Idle connections available
go_sql_idle_connections{db_name="roachprod"}

# Connection wait rate (waits per second)
rate(go_sql_wait_count_total{db_name="roachprod"}[5m])

# Average connection wait time
rate(go_sql_wait_duration_seconds_total{db_name="roachprod"}[5m])
/
rate(go_sql_wait_count_total{db_name="roachprod"}[5m])

# Connection pool saturation (high = need more connections)
go_sql_in_use_connections{db_name="roachprod"} >= go_sql_max_open_connections{db_name="roachprod"}

# Connection closure rate by reason
sum(rate(go_sql_max_idle_closed_total{db_name="roachprod"}[5m]))
sum(rate(go_sql_max_idle_time_closed_total{db_name="roachprod"}[5m]))
sum(rate(go_sql_max_lifetime_closed_total{db_name="roachprod"}[5m]))
```

## Alerting Rules

### Recommended Alerts

```yaml
groups:
  - name: roachprod_tasks
    interval: 30s
    rules:
      # High pending task count
      - alert: TaskQueueBacklog
        expr: sum(roachprod_tasks_total{state="pending"}) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Task queue has {{ $value }} pending tasks"
          description: "Task queue backlog is building up, may need more workers"

      # High failure rate
      - alert: TaskHighFailureRate
        expr: |
          sum(rate(roachprod_tasks_completion_status_total{status="failure"}[5m]))
          /
          sum(rate(roachprod_tasks_completion_status_total[5m]))
          > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Task failure rate is {{ $value | humanizePercentage }}"
          description: "More than 10% of tasks are failing"

      # Slow task processing
      - alert: TaskProcessingSlow
        expr: |
          histogram_quantile(0.95,
            sum(rate(roachprod_tasks_processing_duration_seconds_bucket[5m])) by (le)
          ) > 300
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "95th percentile task processing time is {{ $value }}s"
          description: "Tasks are taking longer than expected to process"

      # High queue wait time
      - alert: TaskQueueAgeHigh
        expr: |
          histogram_quantile(0.95,
            sum(rate(roachprod_tasks_queue_age_seconds_bucket[5m])) by (le)
          ) > 60
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Tasks waiting {{ $value }}s in queue (p95)"
          description: "Tasks are waiting too long before processing - consider adding workers"

      # Frequent timeouts
      - alert: TaskTimeoutsHigh
        expr: sum(rate(roachprod_tasks_timeouts_total[5m])) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Task timeout rate is {{ $value }}/s"
          description: "Tasks are timing out frequently"

      # Tasks stuck in pending with no active workers
      - alert: TasksStuckNoPending
        expr: |
          sum(roachprod_tasks_total{state="pending"}) > 0
          and
          sum(roachprod_tasks_active_workers) == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "{{ $value }} pending tasks but no active workers"
          description: "Tasks are queued but no workers are processing them"

      # Failed tasks accumulating
      - alert: FailedTasksAccumulating
        expr: sum(roachprod_tasks_total{state="failed"}) > 50
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "{{ $value }} failed tasks in database"
          description: "Failed tasks are accumulating and need investigation"

      # API error rate
      - alert: APIHighErrorRate
        expr: |
          sum(rate(gin_uri_request_total{code=~"5.."}[5m]))
          /
          sum(rate(gin_uri_request_total[5m]))
          > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "API error rate is {{ $value | humanizePercentage }}"
          description: "More than 5% of API requests are returning 5xx errors"

      # API latency high
      - alert: APILatencyHigh
        expr: |
          histogram_quantile(0.95,
            sum(rate(gin_request_duration_seconds_bucket[5m])) by (le)
          ) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "API 95th percentile latency is {{ $value }}s"
          description: "API response times are higher than expected"

      # Database connection pool saturation
      - alert: DatabaseConnectionPoolSaturated
        expr: |
          go_sql_in_use_connections{db_name="roachprod"}
          >=
          go_sql_max_open_connections{db_name="roachprod"}
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Database connection pool is fully saturated"
          description: "All {{ $value }} connections are in use - consider increasing max connections"

      # High connection wait rate
      - alert: DatabaseConnectionWaitHigh
        expr: rate(go_sql_wait_count_total{db_name="roachprod"}[5m]) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Database queries waiting {{ $value }}/s for connections"
          description: "High connection wait rate indicates pool saturation"

      # Long connection wait times
      - alert: DatabaseConnectionWaitSlow
        expr: |
          rate(go_sql_wait_duration_seconds_total{db_name="roachprod"}[5m])
          /
          rate(go_sql_wait_count_total{db_name="roachprod"}[5m])
          > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Average connection wait time is {{ $value }}s"
          description: "Queries are waiting too long for database connections"

      # Low idle connections
      - alert: DatabaseIdleConnectionsLow
        expr: go_sql_idle_connections{db_name="roachprod"} < 2
        for: 5m
        labels:
          severity: info
        annotations:
          summary: "Only {{ $value }} idle database connections available"
          description: "Connection pool may be under pressure - monitor for saturation"
```

---

*Last Updated: October 9, 2025*
