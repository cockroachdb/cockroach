-- Version: CockroachDB CCL v21.2.0-alpha.00000000-2657-g9bcafa6063-dirty (x86_64-linux-gnu, built 2021/07/23 21:37:11, go1.16.5)

-- reorder_joins_limit has the default value: 8
-- enable_zigzag_join has the default value: on
-- optimizer_use_histograms has the default value: on
-- optimizer_use_multicol_stats has the default value: on
-- locality_optimized_partitioned_index_scan has the default value: on
-- distsql has the default value: auto
-- vectorize has the default value: on

-- Cluster settings:
--   admission.kv.enabled = false  (when true, work performed by the KV layer is subject to admission control)
--   admission.kv_slot_adjuster.overload_threshold = 32  (when the number of runnable goroutines per CPU is greater than this threshold, the slot adjuster considers the cpu to be overloaded)
--   admission.sql_kv_response.enabled = false  (when true, work performed by the SQL layer when receiving a KV response is subject to admission control)
--   admission.sql_sql_response.enabled = false  (when true, work performed by the SQL layer when receiving a DistSQL response is subject to admission control)
--   bulkio.backup.merge_file_buffer_size = 16 MiB  (size limit used when buffering backup files before merging them)
--   bulkio.backup.merge_file_size = 16 MiB  (size under which backup files will be forwarded to another node to be merged with other smaller files (and implies files will be buffered in-memory until this size before being written to backup storage))
--   bulkio.backup.proxy_file_writes.enabled = false  (return files to the backup coordination processes to write to external storage instead of writing them directly from the storage layer)
--   bulkio.backup.read_retry_delay = 5s  (amount of time since the read-as-of time, per-prior attempt, to wait before making another attempt)
--   bulkio.backup.read_timeout = 5m0s  (amount of time after which a read attempt is considered timed out and is canceled. Hitting this timeout will cause the backup job to fail.)
--   bulkio.backup.read_with_priority_after = 1m0s  (age of read-as-of time above which a BACKUP should read with priority)
--   bulkio.column_backfill.batch_size = 200  (the number of rows updated at a time to add/remove columns)
--   bulkio.index_backfill.batch_size = 50000  (the number of rows for which we construct index entries in a single batch)
--   bulkio.stream_ingestion.cutover_signal_poll_interval = 30s  (the interval at which the stream ingestion job checks if it has been signaled to cutover)
--   bulkio.stream_ingestion.minimum_flush_interval = 5s  (the minimum timestamp between flushes; flushes may still occur if internal buffers fill up)
--   changefeed.backfill.concurrent_scan_requests = 0  (number of concurrent scan requests per node issued during a backfill)
--   changefeed.experimental_poll_interval = 1s  (polling interval for the table descriptors)
--   changefeed.frontier_checkpoint_frequency = 10m0s  (controls the frequency with which span level checkpoints will be written; if 0, disabled.)
--   changefeed.frontier_checkpoint_max_bytes = 1.0 MiB  (controls the maximum size of the checkpoint as a total size of key bytes)
--   changefeed.memory.per_changefeed_limit = 1.0 GiB  (controls amount of data that can be buffered per changefeed)
--   changefeed.min_highwater_advance = 0s  (minimum amount of time the changefeed high water mark must advance  for it to be eligible for checkpointing; Default of 0 will checkpoint every time frontier advances, as long as the rate of checkpointing keeps up with the rate of frontier changes)
--   changefeed.node_throttle_config =   (specifies node level throttling configuration for all changefeeeds)
--   changefeed.slow_span_log_threshold = 0s  (a changefeed will log spans with resolved timestamps this far behind the current wall-clock time; if 0, a default value is calculated based on other cluster settings)
--   cloudstorage.gs.chunking.enabled = true  (enable chunking of file upload to Google Cloud Storage)
--   cloudstorage.http.custom_ca =   (custom root CA (appended to system's default CAs) for verifying certificates when interacting with HTTPS storage)
--   cloudstorage.s3.session_reuse.enabled = true  (persist the last opened s3 session and re-use it when opening a new session with the same arguments)
--   cloudstorage.timeout = 10m0s  (the timeout for import/export storage operations)
--   cluster.organization =   (organization name)
--   cluster.preserve_downgrade_option =   (disable (automatic or manual) cluster version upgrade from the specified version until reset)
--   cluster.secret = ba65c0ef-be37-4798-ae73-e6a98b9087c4  (cluster specific secret)
--   debug.panic_on_failed_assertions = false  (panic when an assertion fails rather than reporting)
--   diagnostics.forced_sql_stat_reset.interval = 2h0m0s  (interval after which SQL statement statistics are refreshed even if not collected (should be more than diagnostics.sql_stat_reset.interval). It has a max value of 24H.)
--   diagnostics.reporting.enabled = true  (enable reporting diagnostic metrics to cockroach labs)
--   diagnostics.reporting.interval = 1h0m0s  (interval at which diagnostics data should be reported)
--   diagnostics.reporting.send_crash_reports = true  (send crash and panic reports)
--   diagnostics.sql_stat_reset.interval = 1h0m0s  (interval controlling how often SQL statement statistics should be reset (should be less than diagnostics.forced_sql_stat_reset.interval). It has a max value of 24H.)
--   enterprise.license =   (the encoded cluster license)
--   external.graphite.endpoint =   (if nonempty, push server metrics to the Graphite or Carbon server at the specified host:port)
--   external.graphite.interval = 10s  (the interval at which metrics are pushed to Graphite (if enabled))
--   feature.backup.enabled = true  (set to true to enable backups, false to disable; default is true)
--   feature.changefeed.enabled = true  (set to true to enable changefeeds, false to disable; default is true)
--   feature.export.enabled = true  (set to true to enable exports, false to disable; default is true)
--   feature.import.enabled = true  (set to true to enable imports, false to disable; default is true)
--   feature.restore.enabled = true  (set to true to enable restore, false to disable; default is true)
--   feature.schema_change.enabled = true  (set to true to enable schema changes, false to disable; default is true)
--   feature.stats.enabled = true  (set to true to enable CREATE STATISTICS/ANALYZE, false to disable; default is true)
--   feature.tls_auto_join.enabled = false  (set to true to enable tls auto join through join tokens, false to disable; default is false)
--   jobs.cancel_update_limit = 1000  (the number of jobs that can be updated when canceling jobs concurrently from dead sessions)
--   jobs.registry.interval.adopt = 30s  (the interval at which a node (a) claims some of the pending jobs and (b) restart its already claimed jobs that are in running or reverting states but are not running)
--   jobs.registry.interval.base = 1  (the base multiplier for other intervals such as adopt, cancel, and gc)
--   jobs.registry.interval.cancel = 10s  (the interval at which a node cancels the jobs belonging to the known dead sessions)
--   jobs.registry.interval.gc = 1h0m0s  (the interval a node deletes expired job records that have exceeded their retention duration)
--   jobs.retention_time = 336h0m0s  (the amount of time to retain records for completed jobs before)
--   jobs.scheduler.enabled = true  (enable/disable job scheduler)
--   jobs.scheduler.max_jobs_per_iteration = 10  (how many schedules to start per iteration; setting to 0 turns off this limit)
--   jobs.scheduler.pace = 1m0s  (how often to scan system.scheduled_jobs table)
--   kv.allocator.lease_rebalancing_aggressiveness = 1  (set greater than 1.0 to rebalance leases toward load more aggressively, or between 0 and 1.0 to be more conservative about rebalancing leases)
--   kv.allocator.load_based_lease_rebalancing.enabled = true  (set to enable rebalancing of range leases based on load and latency)
--   kv.allocator.load_based_rebalancing = leases and replicas  (whether to rebalance based on the distribution of QPS across stores [off = 0, leases = 1, leases and replicas = 2])
--   kv.allocator.min_lease_transfer_interval = 1s  (controls how frequently leases can be transferred for rebalancing. It does not prevent transferring leases in order to allow a replica to be removed from a range.)
--   kv.allocator.qps_rebalance_threshold = 0.25  (minimum fraction away from the mean a store's QPS (such as queries per second) can be before it is considered overfull or underfull)
--   kv.allocator.range_rebalance_threshold = 0.05  (minimum fraction away from the mean a store's range count can be before it is considered overfull or underfull)
--   kv.bulk_ingest.batch_size = 16 MiB  (the maximum size of the payload in an AddSSTable request)
--   kv.bulk_ingest.buffer_increment = 32 MiB  (the size by which the BulkAdder attempts to grow its buffer before flushing)
--   kv.bulk_ingest.index_buffer_size = 32 MiB  (the initial size of the BulkAdder buffer handling secondary index imports)
--   kv.bulk_ingest.max_index_buffer_size = 512 MiB  (the maximum size of the BulkAdder buffer handling secondary index imports)
--   kv.bulk_ingest.max_pk_buffer_size = 128 MiB  (the maximum size of the BulkAdder buffer handling primary index imports)
--   kv.bulk_ingest.pk_buffer_size = 32 MiB  (the initial size of the BulkAdder buffer handling primary index imports)
--   kv.bulk_ingest.stream_external_ssts.enabled = true  (if enabled, external SSTables are iterated directly in some cases, rather than being downloaded entirely first)
--   kv.bulk_ingest.stream_external_ssts.suffix_cache_size = 64 KiB  (size of suffix of remote SSTs to download and cache before reading from remote stream)
--   kv.bulk_io_write.concurrent_addsstable_requests = 1  (number of AddSSTable requests a store will handle concurrently before queuing)
--   kv.bulk_io_write.concurrent_export_requests = 3  (number of export requests a store will handle concurrently before queuing)
--   kv.bulk_io_write.experimental_incremental_export_enabled = true  (use experimental time-bound file filter when exporting in BACKUP)
--   kv.bulk_io_write.max_rate = 1.0 TiB  (the rate limit (bytes/sec) to use for writes to disk on behalf of bulk io ops)
--   kv.bulk_io_write.restore_node_concurrency = 1  (the number of workers processing a restore per job per node; maximum 32)
--   kv.bulk_io_write.revert_range_time_bound_iterator.enabled = true  (use the time-bound iterator optimization when processing a revert range request)
--   kv.bulk_io_write.small_write_size = 400 KiB  (size below which a 'bulk' write will be performed as a normal write instead)
--   kv.bulk_sst.max_allowed_overage = 64 MiB  (if positive, allowed size in excess of target size for SSTs from export requests)
--   kv.bulk_sst.sync_size = 512 KiB  (threshold after which non-Rocks SST writes must fsync (0 disables))
--   kv.bulk_sst.target_size = 64 MiB  (target size for SSTs emitted from export requests)
--   kv.closed_timestamp.close_fraction = 0.2  (fraction of closed timestamp target duration specifying how frequently the closed timestamp is advanced)
--   kv.closed_timestamp.follower_reads_enabled = true  (allow (all) replicas to serve consistent historical reads based on closed timestamp information)
--   kv.closed_timestamp.lead_for_global_reads_override = 0s  (if nonzero, overrides the lead time that global_read ranges use to publish closed timestamps)
--   kv.closed_timestamp.side_transport_interval = 200ms  (the interval at which the closed-timestamp side-transport attempts to advance each range's closed timestamp; set to 0 to disable the side-transport)
--   kv.closed_timestamp.target_duration = 3s  (if nonzero, attempt to provide closed timestamp notifications for timestamps trailing cluster time by approximately this duration)
--   kv.concurrency.optimistic_eval_limited_scans.enabled = true  (when true, limited scans are optimistically evaluated in the sense of not checking for conflicting latches or locks up front for the full key range of the scan, and instead subsequently checking for conflicts only over the key range that was read)
--   kv.dist_sender.concurrency_limit = 2048  (maximum number of asynchronous send requests)
--   kv.follower_read.target_multiple = 3  (if above 1, encourages the distsender to perform a read against the closest replica if a request is older than kv.closed_timestamp.target_duration * (1 + kv.closed_timestamp.close_fraction * this) less a clock uncertainty interval. This value also is used to create follower_timestamp().)
--   kv.gc.intent_age_threshold = 2h0m0s  (intents older than this threshold will be resolved when encountered by the GC queue)
--   kv.gc.intent_cleanup_batch_byte_size = 1000000  (if non zero, gc will split found intents into batches of this size when trying to resolve them)
--   kv.gc.intent_cleanup_batch_size = 5000  (if non zero, gc will split found intents into batches of this size when trying to resolve them)
--   kv.gc_ttl.strict_enforcement.enabled = true  (if true, fail to serve requests at timestamps below the TTL even if the data still exists)
--   kv.lock_table.coordinator_liveness_push_delay = 50ms  (the delay before pushing in order to detect coordinator failures of conflicting transactions)
--   kv.lock_table.deadlock_detection_push_delay = 100ms  (the delay before pushing in order to detect dependency cycles between transactions)
--   kv.lock_table.discovered_locks_threshold_for_consulting_finalized_txn_cache = 200  (the maximum number of discovered locks by a waiter, above which the finalized txn cacheis consulted and resolvable locks are not added to the lock table -- this should be a smallfraction of the maximum number of locks in the lock table)
--   kv.lock_table.maximum_lock_wait_queue_length = 0  (the maximum length of a lock wait-queue that read-write requests are willing to enter and wait in. The setting can be used to ensure some level of quality-of-service under severe per-key contention. If set to a non-zero value and an existing lock wait-queue is already equal to or exceeding this length, requests will be rejected eagerly instead of entering the queue and waiting. Set to 0 to disable.)
--   kv.prober.planner.num_steps_to_plan_at_once = 100  (the number of Steps to plan at once, where a Step is a decision on what range to probe; the order of the Steps is randomized within each planning run, so setting this to a small number will lead to close-to-lexical probing; already made plans are held in memory, so large values are advised against)
--   kv.prober.planner.scan_meta2.timeout = 2s  (timeout on scanning meta2 via db.Scan with max rows set to kv.prober.planner.num_steps_to_plan_at_once)
--   kv.prober.read.enabled = false  (whether the KV read prober is enabled)
--   kv.prober.read.interval = 1m0s  (how often each node sends a read probe to the KV layer on average (jitter is added); note that a very slow read can block kvprober from sending additional probes; kv.prober.read.timeout controls the max time kvprober can be blocked)
--   kv.prober.read.timeout = 2s  (if this much time elapses without success, a KV read probe will be treated as an error; note that a very slow read can block kvprober from sending additional probesthis setting controls the max time kvprober can be blocked)
--   kv.protectedts.max_bytes = 1048576  (if non-zero the limit of the number of bytes of spans and metadata which can be protected)
--   kv.protectedts.max_spans = 32768  (if non-zero the limit of the number of spans which can be protected)
--   kv.protectedts.poll_interval = 2m0s  (the interval at which the protectedts subsystem state is polled)
--   kv.protectedts.reconciliation.interval = 5m0s  (the frequency for reconciling jobs with protected timestamp records)
--   kv.queue.process.guaranteed_time_budget = 1m0s  (the guaranteed duration before which the processing of a queue may time out)
--   kv.raft.command.max_size = 64 MiB  (maximum size of a raft command)
--   kv.raft_log.disable_synchronization_unsafe = false  (set to true to disable synchronization on Raft log writes to persistent storage. Setting to true risks data loss or data corruption on server crashes. The setting is meant for internal testing only and SHOULD NOT be used in production.)
--   kv.range.backpressure_byte_tolerance = 32 MiB  (defines the number of bytes above the product of backpressure_range_size_multiplier and the range_max_size at which backpressure will not apply)
--   kv.range.backpressure_range_size_multiplier = 2  (multiple of range_max_bytes that a range is allowed to grow to without splitting before writes to that range are blocked, or 0 to disable)
--   kv.range_descriptor_cache.size = 1000000  (maximum number of entries in the range descriptor cache)
--   kv.range_merge.queue_enabled = true  (whether the automatic merge queue is enabled)
--   kv.range_merge.queue_interval = 5s  (how long the merge queue waits between processing replicas)
--   kv.range_split.by_load_enabled = true  (allow automatic splits of ranges based on where load is concentrated)
--   kv.range_split.by_load_merge_delay = 5m0s  (the delay that range splits created due to load will wait before considering being merged away)
--   kv.range_split.load_qps_threshold = 2500  (the QPS over which, the range becomes a candidate for load based splitting)
--   kv.rangefeed.closed_timestamp_refresh_interval = 0s  (the interval at which closed-timestamp updatesare delivered to rangefeeds; set to 0 to use kv.closed_timestamp.side_transport_interval)
--   kv.rangefeed.concurrent_catchup_iterators = 64  (number of rangefeeds catchup iterators a store will allow concurrently before queueing)
--   kv.rangefeed.enabled = false  (if set, rangefeed registration is enabled)
--   kv.replication_reports.interval = 1m0s  (the frequency for generating the replication_constraint_stats, replication_stats_report and replication_critical_localities reports (set to 0 to disable))
--   kv.snapshot_rebalance.max_rate = 8.0 MiB  (the rate limit (bytes/sec) to use for rebalance and upreplication snapshots)
--   kv.snapshot_recovery.max_rate = 8.0 MiB  (the rate limit (bytes/sec) to use for recovery snapshots)
--   kv.snapshot_sender.batch_size = 256 KiB  (size of key-value batches sent over the network during snapshots)
--   kv.snapshot_sst.sync_size = 512 KiB  (threshold after which snapshot SST writes must fsync)
--   kv.store.system_config_update.queue_add_burst = 32  (the burst rate at which the store will add all replicas to the split and merge queue due to system config gossip)
--   kv.store.system_config_update.queue_add_rate = 0.5  (the rate (per second) at which the store will add, all replicas to the split and merge queue due to system config gossip)
--   kv.tenant_rate_limiter.burst_limit_seconds = 10  (per-tenant burst limit as a multiplier of the rate)
--   kv.tenant_rate_limiter.rate_limit = 200  (per-tenant rate limit in Request Units per second)
--   kv.transaction.max_intents_bytes = 4194304  (maximum number of bytes used to track locks in transactions)
--   kv.transaction.max_refresh_spans_bytes = 256000  (maximum number of bytes used to track refresh spans in serializable transactions)
--   kv.transaction.parallel_commits_enabled = true  (if enabled, transactional commits will be parallelized with transactional writes)
--   kv.transaction.reject_over_max_intents_budget.enabled = false  (if set, transactions that exceed their lock tracking budget (kv.transaction.max_intents_bytes) are rejected instead of having their lock spans imprecisely compressed)
--   kv.transaction.write_pipelining_enabled = true  (if enabled, transactional writes are pipelined through Raft consensus)
--   kv.transaction.write_pipelining_max_batch_size = 128  (if non-zero, defines that maximum size batch that will be pipelined through Raft consensus)
--   rocksdb.ingest_backpressure.l0_file_count_threshold = 20  (number of L0 files after which to backpressure SST ingestions)
--   rocksdb.ingest_backpressure.max_delay = 5s  (maximum amount of time to backpressure a single SST ingestion)
--   rocksdb.min_wal_sync_interval = 0s  (minimum duration between syncs of the RocksDB WAL)
--   schemachanger.backfiller.buffer_increment = 32 MiB  (the size by which the BulkAdder attempts to grow its buffer before flushing)
--   schemachanger.backfiller.buffer_size = 32 MiB  (the initial size of the BulkAdder buffer handling index backfills)
--   schemachanger.backfiller.max_buffer_size = 512 MiB  (the maximum size of the BulkAdder buffer handling index backfills)
--   schemachanger.backfiller.max_commit_wait_fns = 128  (the maximum number of commit-wait functions that the columnBackfiller will accumulate before consuming them to reclaim memory)
--   schemachanger.backfiller.max_sst_size = 16 MiB  (target size for ingested files during backfills)
--   security.ocsp.mode = off  (use OCSP to check whether TLS certificates are revoked. If the OCSP server is unreachable, in strict mode all certificates will be rejected and in lax mode all certificates will be accepted. [off = 0, lax = 1, strict = 2])
--   security.ocsp.timeout = 3s  (timeout before considering the OCSP server unreachable)
--   server.auth_log.sql_connections.enabled = false  (if set, log SQL client connect and disconnect events (note: may hinder performance on loaded nodes))
--   server.auth_log.sql_sessions.enabled = false  (if set, log SQL session login/disconnection events (note: may hinder performance on loaded nodes))
--   server.authentication_cache.enabled = true  (enables a cache used during authentication to avoid lookups to system tables when retrieving per-user authentication-related information)
--   server.child_metrics.enabled = false  (enables the exporting of child metrics, additional prometheus time series with extra labels)
--   server.clock.forward_jump_check_enabled = false  (if enabled, forward clock jumps > max_offset/2 will cause a panic)
--   server.clock.persist_upper_bound_interval = 0s  (the interval between persisting the wall time upper bound of the clock. The clock does not generate a wall time greater than the persisted timestamp and will panic if it sees a wall time greater than this value. When cockroach starts, it waits for the wall time to catch-up till this persisted timestamp. This guarantees monotonic wall time across server restarts. Not setting this or setting a value of 0 disables this feature.)
--   server.consistency_check.interval = 24h0m0s  (the time between range consistency checks; set to 0 to disable consistency checking. Note that intervals that are too short can negatively impact performance.)
--   server.consistency_check.max_rate = 8.0 MiB  (the rate limit (bytes/sec) to use for consistency checks; used in conjunction with server.consistency_check.interval to control the frequency of consistency checks. Note that setting this too high can negatively impact performance.)
--   server.cpu_profile.total_dump_size_limit = 128 MiB  (maximum combined disk size of preserved CPU profiles)
--   server.declined_reservation_timeout = 1s  (the amount of time to consider the store throttled for up-replication after a reservation was declined)
--   server.eventlog.enabled = true  (if set, logged notable events are also stored in the table system.eventlog)
--   server.eventlog.ttl = 2160h0m0s  (if nonzero, entries in system.eventlog older than this duration are deleted every 10m0s. Should not be lowered below 24 hours.)
--   server.failed_reservation_timeout = 5s  (the amount of time to consider the store throttled for up-replication after a failed reservation call)
--   server.goroutine_dump.num_goroutines_threshold = 1000  (a threshold beyond which if number of goroutines increases, then goroutine dump can be triggered)
--   server.goroutine_dump.total_dump_size_limit = 500 MiB  (total size of goroutine dumps to be kept. Dumps are GC'ed in the order of creation time. The latest dump is always kept even if its size exceeds the limit.)
--   server.host_based_authentication.configuration =   (host-based authentication configuration to use during connection authentication)
--   server.mem_profile.max_profiles = 5  (maximum number of profiles to be kept per ramp-up of memory usage. A ramp-up is defined as a sequence of profiles with increasing usage.)
--   server.mem_profile.total_dump_size_limit = 128 MiB  (maximum combined disk size of preserved memory profiles)
--   server.oidc_authentication.autologin = false  (if true, logged-out visitors to the DB Console will be automatically redirected to the OIDC login endpoint (this feature is experimental))
--   server.oidc_authentication.button_text = Login with your OIDC provider  (text to show on button on DB Console login page to login with your OIDC provider (only shown if OIDC is enabled) (this feature is experimental))
--   server.oidc_authentication.claim_json_key =   (sets JSON key of principal to extract from payload after OIDC authentication completes (usually email or sid) (this feature is experimental))
--   server.oidc_authentication.client_id =   (sets OIDC client id (this feature is experimental))
--   server.oidc_authentication.client_secret =   (sets OIDC client secret (this feature is experimental))
--   server.oidc_authentication.enabled = false  (enables or disabled OIDC login for the DB Console (this feature is experimental))
--   server.oidc_authentication.principal_regex = (.+)  (regular expression to apply to extracted principal (see claim_json_key setting) to translate to SQL user (golang regex format, must include 1 grouping to extract) (this feature is experimental))
--   server.oidc_authentication.provider_url =   (sets OIDC provider URL ({provider_url}/.well-known/openid-configuration must resolve) (this feature is experimental))
--   server.oidc_authentication.redirect_url = https://localhost:8080/oidc/v1/callback  (sets OIDC redirect URL via a URL string or a JSON string containing a required `redirect_urls` key with an object that maps from region keys to URL strings (URLs should point to your load balancer and must route to the path /oidc/v1/callback) (this feature is experimental))
--   server.oidc_authentication.scopes = openid  (sets OIDC scopes to include with authentication request (space delimited list of strings, required to start with `openid`) (this feature is experimental))
--   server.rangelog.ttl = 720h0m0s  (if nonzero, range log entries older than this duration are deleted every 10m0s. Should not be lowered below 24 hours.)
--   server.shutdown.drain_wait = 0s  (the amount of time a server waits in an unready state before proceeding with the rest of the shutdown process)
--   server.shutdown.lease_transfer_wait = 5s  (the amount of time a server waits to transfer range leases before proceeding with the rest of the shutdown process)
--   server.shutdown.query_wait = 10s  (the server will wait for at least this amount of time for active queries to finish)
--   server.sqlliveness.gc_interval = 20s  (duration between attempts to delete extant sessions that have expired)
--   server.sqlliveness.gc_jitter = 0.15  (jitter fraction on the duration between attempts to delete extant sessions that have expired)
--   server.sqlliveness.heartbeat = 5s  (duration heart beats to push session expiration further out in time)
--   server.sqlliveness.storage_session_cache_size = 1024  (number of session entries to store in the LRU)
--   server.sqlliveness.ttl = 40s  (default sqlliveness session ttl)
--   server.time_after_store_suspect = 30s  (the amount of time we consider a store suspect for after it fails a node liveness heartbeat. A suspect node would not receive any new replicas or lease transfers, but will keep the replicas it has.)
--   server.time_until_store_dead = 5m0s  (the time after which if there is no new gossiped information about a store, it is considered dead)
--   server.user_login.min_password_length = 1  (the minimum length accepted for passwords set in cleartext via SQL. Note that a value lower than 1 is ignored: passwords cannot be empty in any case.)
--   server.user_login.timeout = 10s  (timeout after which client authentication times out if some system range is unavailable (0 = no timeout))
--   server.web_session_timeout = 168h0m0s  (the duration that a newly created web session will be valid)
--   sql.catalog.descriptor_lease_duration = 5m0s  (mean duration of sql descriptor leases, this actual duration is jitterred)
--   sql.catalog.descriptor_lease_jitter_fraction = 0.05  (mean duration of sql descriptor leases, this actual duration is jitterred)
--   sql.catalog.descriptor_lease_renewal_fraction = 1m0s  (controls the default time before a lease expires when acquisition to renew the lease begins)
--   sql.catalog.descs.validate_on_write.enabled = true  (set to true to validate descriptors prior to writing, false to disable; default is true)
--   sql.catalog.hydrated_tables.cache_size = 128  (number of table descriptor versions retained in the hydratedtables LRU cache)
--   sql.conn.max_read_buffer_message_size = 16 MiB  (maximum buffer size to allow for ingesting sql statements. Connections must be restarted for this to take effect.)
--   sql.crdb_internal.table_row_statistics.as_of_time = -10s  (historical query time used to build the crdb_internal.table_row_statistics table)
--   sql.cross_db_fks.enabled = false  (if true, creating foreign key references across databases is allowed)
--   sql.cross_db_sequence_owners.enabled = false  (if true, creating sequences owned by tables from other databases is allowed)
--   sql.cross_db_views.enabled = false  (if true, creating views that refer to other databases is allowed)
--   sql.defaults.default_int_size = 8  (the size, in bytes, of an INT type)
--   sql.defaults.disallow_full_table_scans.enabled = false  (setting to true rejects queries that have planned a full table scan)
--   sql.defaults.distsql = auto  (default distributed SQL execution mode [off = 0, auto = 1, on = 2])
--   sql.defaults.drop_enum_value.enabled = false  (default value for enable_drop_enum_value; allows for dropping enum values)
--   sql.defaults.experimental_alter_column_type.enabled = false  (default value for experimental_alter_column_type session setting; enables the use of ALTER COLUMN TYPE for general conversions)
--   sql.defaults.experimental_computed_column_rewrites =   (allows rewriting computed column expressions in CREATE TABLE and ALTER TABLE statements; the format is: '(before expression) -> (after expression) [, (before expression) -> (after expression) ...]')
--   sql.defaults.experimental_distsql_planning = off  (default experimental_distsql_planning mode; enables experimental opt-driven DistSQL planning [off = 0, on = 1])
--   sql.defaults.experimental_enable_unique_without_index_constraints.enabled = false  (default value for experimental_enable_unique_without_index_constraints session setting;disables unique without index constraints by default)
--   sql.defaults.experimental_hash_sharded_indexes.enabled = false  (default value for experimental_enable_hash_sharded_indexes; allows for creation of hash sharded indexes by default)
--   sql.defaults.experimental_implicit_column_partitioning.enabled = false  (default value for experimental_enable_temp_tables; allows for the use of implicit column partitioning)
--   sql.defaults.use_declarative_schema_changer = off  (default value for use_declarative_schema_changer session setting;disables new schema changer by default [off = 0, on = 1, unsafe_always = 2])
--   sql.defaults.experimental_stream_replication.enabled = false  (default value for experimental_stream_replication session setting;enables the ability to setup a replication stream)
--   sql.defaults.experimental_temporary_tables.enabled = false  (default value for experimental_enable_temp_tables; allows for use of temporary tables by default)
--   sql.defaults.foreign_key_cascades_limit = 10000  (default value for foreign_key_cascades_limit session setting; limits the number of cascading operations that run as part of a single query)
--   sql.defaults.idle_in_session_timeout = 0s  (default value for the idle_in_session_timeout; default value for the idle_in_session_timeout session setting; controls the duration a session is permitted to idle before the session is terminated; if set to 0, there is no timeout)
--   sql.defaults.idle_in_transaction_session_timeout = 0s  (default value for the idle_in_transaction_session_timeout; controls the duration a session is permitted to idle in a transaction before the session is terminated; if set to 0, there is no timeout)
--   sql.defaults.implicit_select_for_update.enabled = true  (default value for enable_implicit_select_for_update session setting; enables FOR UPDATE locking during the row-fetch phase of mutation statements)
--   sql.defaults.insert_fast_path.enabled = true  (default value for enable_insert_fast_path session setting; enables a specialized insert path)
--   sql.defaults.intervalstyle = postgres  (default value for IntervalStyle session setting [postgres = 0, iso_8601 = 1, sql_standard = 2])
--   sql.defaults.intervalstyle.enabled = false  (default value for enable_intervalstyle session setting)
--   sql.defaults.locality_optimized_partitioned_index_scan.enabled = true  (default value for locality_optimized_partitioned_index_scan session setting; enables searching for rows in the current region before searching remote regions)
--   sql.defaults.optimizer_use_histograms.enabled = true  (default value for optimizer_use_histograms session setting; enables usage of histograms in the optimizer by default)
--   sql.defaults.optimizer_use_multicol_stats.enabled = true  (default value for optimizer_use_multicol_stats session setting; enables usage of multi-column stats in the optimizer by default)
--   sql.defaults.override_multi_region_zone_config.enabled = false  (default value for override_multi_region_zone_config; allows for overriding the zone configs of a multi-region table or database)
--   sql.defaults.prefer_lookup_joins_for_fks.enabled = false  (default value for prefer_lookup_joins_for_fks session setting; causes foreign key operations to use lookup joins when possible)
--   sql.defaults.primary_region =   (if not empty, all databases created without a PRIMARY REGION will implicitly have the given PRIMARY REGION)
--   sql.defaults.reorder_joins_limit = 8  (default number of joins to reorder)
--   sql.defaults.require_explicit_primary_keys.enabled = false  (default value for requiring explicit primary keys in CREATE TABLE statements)
--   sql.defaults.results_buffer.size = 16 KiB  (default size of the buffer that accumulates results for a statement or a batch of statements before they are sent to the client. This can be overridden on an individual connection with the 'results_buffer_size' parameter. Note that auto-retries generally only happen while no results have been delivered to the client, so reducing this size can increase the number of retriable errors a client receives. On the other hand, increasing the buffer size can increase the delay until the client receives the first result row. Updating the setting only affects new connections. Setting to 0 disables any buffering.)
--   sql.defaults.serial_normalization = rowid  (default handling of SERIAL in table definitions [rowid = 0, virtual_sequence = 1, sql_sequence = 2, sql_sequence_cached = 3])
--   sql.defaults.serial_sequences_cache_size = 256  (the default cache size when the session's serial normalization mode is set to cached sequencesA cache size of 1 means no caching. Any cache size less than 1 is invalid.)
--   sql.defaults.statement_timeout = 0s  (default value for the statement_timeout; default value for the statement_timeout session setting; controls the duration a query is permitted to run before it is canceled; if set to 0, there is no timeout)
--   sql.defaults.stub_catalog_tables.enabled = true  (default value for stub_catalog_tables session setting)
--   sql.defaults.vectorize = on  (default vectorize mode [on = 0, on = 2, experimental_always = 3, off = 4])
--   sql.defaults.zigzag_join.enabled = true  (default value for enable_zigzag_join session setting; allows use of zig-zag join by default)
--   sql.distsql.flow_stream_timeout = 10s  (amount of time incoming streams wait for a flow to be set up before erroring out)
--   sql.distsql.max_running_flows = 500  (maximum number of concurrent flows that can be run on a node)
--   sql.distsql.temp_storage.hash_agg.enabled = true  (set to false to disable hash aggregator disk spilling (this will improve performance, but the query might hit the memory limit))
--   sql.distsql.temp_storage.workmem = 64 MiB  (maximum amount of memory in bytes a processor can use before falling back to temp storage)
--   sql.log.admin_audit.enabled = false  (when set, log SQL queries that are executed by a user with admin privileges)
--   sql.log.slow_query.experimental_full_table_scans.enabled = false  (when set to true, statements that perform a full table/index scan will be logged to the slow query log even if they do not meet the latency threshold. Must have the slow query log enabled for this setting to have any effect.)
--   sql.log.slow_query.internal_queries.enabled = false  (when set to true, internal queries which exceed the slow query log threshold are logged to a separate log. Must have the slow query log enabled for this setting to have any effect.)
--   sql.log.slow_query.latency_threshold = 0s  (when set to non-zero, log statements whose service latency exceeds the threshold to a secondary logger on each node)
--   sql.log.unstructured_entries.enabled = false  (when set, SQL execution and audit logs use the pre-v21.1 unstrucured format)
--   sql.metrics.index_usage_stats.enabled = true  (collect per index usage statistics)
--   sql.metrics.max_mem_reported_stmt_fingerprints = 100000  (the maximum number of reported statement fingerprints stored in memory)
--   sql.metrics.max_mem_reported_txn_fingerprints = 100000  (the maximum number of reported transaction fingerprints stored in memory)
--   sql.metrics.max_mem_stmt_fingerprints = 100000  (the maximum number of statement fingerprints stored in memory)
--   sql.metrics.max_mem_txn_fingerprints = 100000  (the maximum number of transaction fingerprints stored in memory)
--   sql.metrics.statement_details.dump_to_logs = false  (dump collected statement statistics to node logs when periodically cleared)
--   sql.metrics.statement_details.enabled = true  (collect per-statement query statistics)
--   sql.metrics.statement_details.plan_collection.enabled = true  (periodically save a logical plan for each fingerprint)
--   sql.metrics.statement_details.plan_collection.period = 5m0s  (the time until a new logical plan is collected)
--   sql.metrics.statement_details.threshold = 0s  (minimum execution time to cause statement statistics to be collected. If configured, no transaction stats are collected.)
--   sql.metrics.transaction_details.enabled = true  (collect per-application transaction statistics)
--   sql.metrics.transaction_details.max_statement_ids = 1000  (max number of statement fingerprint IDs to store for transaction statistics)
--   sql.multiregion.drop_primary_region.enabled = true  (allows dropping the PRIMARY REGION of a database if it is the last region)
--   sql.mutations.mutation_batch_byte_size = 4.0 MiB  (byte size - in key and value lengths -- for mutation batches)
--   sql.notices.enabled = true  (enable notices in the server/client protocol being sent)
--   sql.optimizer.uniqueness_checks_for_gen_random_uuid.enabled = false  (if enabled, uniqueness checks may be planned for mutations of UUID columns updated with gen_random_uuid(); otherwise, uniqueness is assumed due to near-zero collision probability)
--   sql.query_cache.enabled = true  (enable the query cache)
--   sql.show_tables.estimated_row_count.enabled = true  (whether the estimated_row_count is shown on SHOW TABLES. Turning this off will improve SHOW TABLES performance.)
--   sql.spatial.experimental_box2d_comparison_operators.enabled = false  (enables the use of certain experimental box2d comparison operators)
--   sql.stats.automatic_collection.enabled = true  (automatic statistics collection mode)
--   sql.stats.automatic_collection.fraction_stale_rows = 0.2  (target fraction of stale rows per table that will trigger a statistics refresh)
--   sql.stats.automatic_collection.max_fraction_idle = 0.9  (maximum fraction of time that automatic statistics sampler processors are idle)
--   sql.stats.automatic_collection.min_stale_rows = 500  (target minimum number of stale rows per table that will trigger a statistics refresh)
--   sql.stats.histogram_collection.enabled = true  (histogram collection mode)
--   sql.stats.max_timestamp_age = 5m0s  (maximum age of timestamp during table statistics collection)
--   sql.stats.multi_column_collection.enabled = true  (multi-column statistics collection mode)
--   sql.stats.post_events.enabled = false  (if set, an event is logged for every CREATE STATISTICS job)
--   sql.stmt_diagnostics.bundle_chunk_size = 1.0 MiB  (chunk size for statement diagnostic bundles)
--   sql.stmt_diagnostics.poll_interval = 10s  (rate at which the stmtdiagnostics.Registry polls for requests, set to zero to disable)
--   sql.tablecache.lease.refresh_limit = 500  (maximum number of descriptors to periodically refresh leases for)
--   sql.temp_object_cleaner.cleanup_interval = 30m0s  (how often to clean up orphaned temporary objects)
--   sql.trace.log_statement_execute = false  (set to true to enable logging of executed statements)
--   sql.trace.session_eventlog.enabled = false  (set to true to enable session tracing. Note that enabling this may have a non-trivial negative performance impact.)
--   sql.trace.stmt.enable_threshold = 0s  (duration beyond which all statements are traced (set to 0 to disable). This applies to individual statements within a transaction and is therefore finer-grained than sql.trace.txn.enable_threshold.)
--   sql.trace.txn.enable_threshold = 0s  (duration beyond which all transactions are traced (set to 0 to disable). This setting is coarser grained thansql.trace.stmt.enable_threshold because it applies to all statements within a transaction as well as client communication (e.g. retries).)
--   sql.truncate.preserved_split_count_multiple = 4  (set to non-zero to cause TRUNCATE to preserve range splits from the table's indexes. The multiple given will be multiplied with the number of nodes in the cluster to produce the number of preserved range splits. This can improve performance when truncating a table with significant write traffic.)
--   sql.txn_stats.sample_rate = 0.01  (the probability that a given transaction will collect execution statistics (displayed in the DB Console))
--   storage.max_sync_duration = 1m0s  (maximum duration for disk operations; any operations that take longer than this setting trigger a warning log entry or process crash)
--   storage.max_sync_duration.fatal.enabled = true  (if true, fatal the process when a disk operation exceeds storage.max_sync_duration)
--   storage.mvcc.max_intents_per_error = 5000  (maximum number of intents returned in error during export of scan requests)
--   storage.transaction.separated_intents.enabled = true  (if enabled, intents will be written to a separate lock table, instead of being interleaved with MVCC values)
--   tenant_cost_model.kv_read_cost_per_megabyte = 10  (cost of a read in Request Units per MB)
--   tenant_cost_model.kv_read_request_cost = 0.7  (base cost of a read request in Request Units)
--   tenant_cost_model.kv_write_cost_per_megabyte = 400  (cost of a write in Request Units per MB)
--   tenant_cost_model.kv_write_request_cost = 1  (base cost of a write request in Request Units)
--   tenant_cost_model.pod_cpu_second_cost = 1000  (cost of a CPU-second on the tenant POD in Request Units)
--   timeseries.storage.enabled = true  (if set, periodic timeseries data is stored within the cluster; disabling is not recommended unless you are storing the data elsewhere)
--   timeseries.storage.resolution_10s.ttl = 240h0m0s  (the maximum age of time series data stored at the 10 second resolution. Data older than this is subject to rollup and deletion.)
--   timeseries.storage.resolution_30m.ttl = 2160h0m0s  (the maximum age of time series data stored at the 30 minute resolution. Data older than this is subject to deletion.)
--   trace.datadog.agent =   (if set, traces will be sent to this DataDog agent; use <host>:<port> or "default" for localhost:8126. Only one tracer can be configured at a time.)
--   trace.datadog.project = CockroachDB  (the project under which traces will be reported to the DataDog agent if trace.datadog.agent is set. Only one tracer can be configured at a time.)
--   trace.debug.enable = false  (if set, traces for recent requests can be seen at https://<ui>/debug/requests)
--   trace.lightstep.token =   (if set, traces go to Lightstep using this token)
--   trace.zipkin.collector =   (if set, traces go to the given Zipkin instance (example: '127.0.0.1:9411'). Only one tracer can be configured at a time.)
--   version = 21.1-118  (set the active cluster version in the format '<major>.<minor>')
