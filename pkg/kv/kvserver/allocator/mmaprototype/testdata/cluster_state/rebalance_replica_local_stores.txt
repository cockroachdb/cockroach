# Tests rebalancing a replica between two local stores (stores on the same node).
#
# Scenario: Two stores (s1, s2) on node 1. Initially, s1 holds the replica and
# lease for range 1. The test rebalances the replica (and lease) from s1 to s2,
# then back from s2 to s1.
#
# Steps and expectations:
# 1. Creates pending changes for the rebalance, verifying load adjustments
#    immediately reflect the pending state.
# 2. Simulates enactment: when s2's leaseholder message arrives showing it has
#    the lease, change 1 (add to s2) is marked enacted. Change 2 (remove from
#    s1) remains pending until s1 is removed from the replica set.
# 3. Verifies enacted changes cannot be rejected (they're removed from
#    pendingChanges).
# 4. Tracks load adjustments: pending changes adjust load until store-reported
#    load reflects the change (after 10s of enactment). After that, the change
#    is removed from load tracking but may remain for GC.
# 5. Tests GC and update of gcTime.
# 6. Test update of ReplicaChange.prev based on latest leaseholder info.
# 7. Rebalances back from s2 to s1 and verifies changes are GC'd after the
#    normal GC duration.
#
# Two stores on the same node (thus necessarily same locality tiers).
set-store
  store-id=1 node-id=1 attrs=purple locality-tiers=region=anywhere
  store-id=2 node-id=1 attrs=yellow locality-tiers=region=anywhere
----
node-id=1 locality-tiers=region=anywhere,node=1
  store-id=1 attrs=purple
  store-id=2 attrs=yellow

store-load-msg
  store-id=1 node-id=1 load=[80,80,80] capacity=[100,100,100] secondary-load=0 load-time=0s
----

get-load-info
----
store-id=1 node-id=1 status=ok accepting all reported=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] adjusted=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] node-reported-cpu=80 node-adjusted-cpu=80 seq=1
store-id=2 node-id=1 status=ok accepting all reported=[cpu:0s/s, write-bandwidth:0 B/s, byte-size:0 B] adjusted=[cpu:0s/s, write-bandwidth:0 B/s, byte-size:0 B] node-reported-cpu=80 node-adjusted-cpu=80 seq=0

# Store s1 has the single replica and lease for r1.
store-leaseholder-msg 
store-id=1
  range-id=1 load=[80,80,80] raft-cpu=20
  config=(num_replicas=3 constraints={'+region=us-west-1:1'} voter_constraints={'+region=us-west-1:1'})
    store-id=1 replica-id=1 type=VOTER_FULL leaseholder=true
----

ranges
----
range-id=1 local-store=1 load=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] raft-cpu=20
  store-id=1 replica-id=1 type=VOTER_FULL leaseholder=true

# The top-k uses CPURate, with r1 as the only range.
get-load-info
----
store-id=1 node-id=1 status=ok accepting all reported=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] adjusted=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] node-reported-cpu=80 node-adjusted-cpu=80 seq=1
  top-k-ranges (local-store-id=1) dim=CPURate: r1
store-id=2 node-id=1 status=ok accepting all reported=[cpu:0s/s, write-bandwidth:0 B/s, byte-size:0 B] adjusted=[cpu:0s/s, write-bandwidth:0 B/s, byte-size:0 B] node-reported-cpu=80 node-adjusted-cpu=80 seq=0

# Transfer the replica from s1 to s2. The lease will also be transferred.
make-pending-changes range-id=1
  rebalance-replica: remove-store-id=1 add-store-id=2
----
pending(2)
change-id=1 store-id=2 node-id=1 range-id=1 load-delta=[cpu:88ns/s, write-bandwidth:88 B/s, byte-size:88 B] start=0s gc=5m0s
  prev=(replica-id=none type=VOTER_FULL)
  next=(replica-id=unknown type=VOTER_FULL leaseholder=true)
change-id=2 store-id=1 node-id=1 range-id=1 load-delta=[cpu:-80ns/s, write-bandwidth:-80 B/s, byte-size:-80 B] start=0s gc=5m0s
  prev=(replica-id=1 type=VOTER_FULL leaseholder=true)
  next=(replica-id=none type=VOTER_FULL)

# Range r1 reflects the pending change as if it has already been applied. Note
# that the local-store representing the "range owner" is still s1.
ranges
----
range-id=1 local-store=1 load=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] raft-cpu=20
  store-id=2 replica-id=unknown type=VOTER_FULL leaseholder=true

# The adjusted load is post transfer.
get-load-info
----
store-id=1 node-id=1 status=ok accepting all reported=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] adjusted=[cpu:0s/s, write-bandwidth:0 B/s, byte-size:0 B] node-reported-cpu=80 node-adjusted-cpu=88 seq=2
  top-k-ranges (local-store-id=1) dim=CPURate: r1
store-id=2 node-id=1 status=ok accepting all reported=[cpu:0s/s, write-bandwidth:0 B/s, byte-size:0 B] adjusted=[cpu:88ns/s, write-bandwidth:88 B/s, byte-size:88 B] node-reported-cpu=80 node-adjusted-cpu=88 seq=1

tick seconds=5
----
t=5s

# Store leaseholder msg from s1, showing that s2 has a replica, but not the
# lease.
store-leaseholder-msg
store-id=1
  range-id=1 load=[80,80,80] raft-cpu=20
  config=(num_replicas=3 constraints={'+region=us-west-1:1'} voter_constraints={'+region=us-west-1:1'})
    store-id=1 replica-id=1 type=VOTER_FULL leaseholder=true
    store-id=2 replica-id=2 type=VOTER_FULL
----

# Neither change is considered enacted.
get-pending-changes
----
pending(2)
change-id=1 store-id=2 node-id=1 range-id=1 load-delta=[cpu:88ns/s, write-bandwidth:88 B/s, byte-size:88 B] start=0s gc=5m0s
  prev=(replica-id=2 type=VOTER_FULL)
  next=(replica-id=unknown type=VOTER_FULL leaseholder=true)
change-id=2 store-id=1 node-id=1 range-id=1 load-delta=[cpu:-80ns/s, write-bandwidth:-80 B/s, byte-size:-80 B] start=0s gc=5m0s
  prev=(replica-id=1 type=VOTER_FULL leaseholder=true)
  next=(replica-id=none type=VOTER_FULL)

# Store leaseholder msg from s2, showing that s2 is now the leaseholder.
store-leaseholder-msg
store-id=2
  range-id=1 load=[80,80,80] raft-cpu=20
  config=(num_replicas=3 constraints={'+region=us-west-1:1'} voter_constraints={'+region=us-west-1:1'})
    store-id=1 replica-id=1 type=VOTER_FULL
    store-id=2 replica-id=2 type=VOTER_FULL leaseholder=true
----

# The local-store representing the "range owner" is now s2.
ranges
----
range-id=1 local-store=2 load=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] raft-cpu=20
  store-id=2 replica-id=2 type=VOTER_FULL leaseholder=true

# The addition of replica and lease on s2 is considered enacted. The removal
# of replica and lease from s1 is not yet enacted. The gc time is changed to
# be 30s after the enactment. Note that the prev state of change 2 no longer
# shows leaseholder=true since the lease has already been transferred.
get-pending-changes
----
pending(2)
change-id=1 store-id=2 node-id=1 range-id=1 load-delta=[cpu:88ns/s, write-bandwidth:88 B/s, byte-size:88 B] start=0s gc=5m0s enacted=5s
  prev=(replica-id=2 type=VOTER_FULL)
  next=(replica-id=unknown type=VOTER_FULL leaseholder=true)
change-id=2 store-id=1 node-id=1 range-id=1 load-delta=[cpu:-80ns/s, write-bandwidth:-80 B/s, byte-size:-80 B] start=0s gc=35s
  prev=(replica-id=1 type=VOTER_FULL)
  next=(replica-id=none type=VOTER_FULL)

get-load-info
----
store-id=1 node-id=1 status=ok accepting all reported=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] adjusted=[cpu:0s/s, write-bandwidth:0 B/s, byte-size:0 B] node-reported-cpu=80 node-adjusted-cpu=88 seq=2
store-id=2 node-id=1 status=ok accepting all reported=[cpu:0s/s, write-bandwidth:0 B/s, byte-size:0 B] adjusted=[cpu:88ns/s, write-bandwidth:88 B/s, byte-size:88 B] node-reported-cpu=80 node-adjusted-cpu=88 seq=1
  top-k-ranges (local-store-id=2) dim=ByteSize: r1

tick seconds=5
----
t=10s

# Change 1 will not be found, so can't be rejected.
reject-pending-changes change-ids=(1) expect-panic
----
pending(2)
change-id=1 store-id=2 node-id=1 range-id=1 load-delta=[cpu:88ns/s, write-bandwidth:88 B/s, byte-size:88 B] start=0s gc=5m0s enacted=5s
  prev=(replica-id=2 type=VOTER_FULL)
  next=(replica-id=unknown type=VOTER_FULL leaseholder=true)
change-id=2 store-id=1 node-id=1 range-id=1 load-delta=[cpu:-80ns/s, write-bandwidth:-80 B/s, byte-size:-80 B] start=0s gc=35s
  prev=(replica-id=1 type=VOTER_FULL)
  next=(replica-id=none type=VOTER_FULL)

# Store load msg from s2, showing its new load.
store-load-msg
  store-id=2 node-id=1 load=[80,80,80] capacity=[100,100,100] secondary-load=1 load-time=15s
----

# The adjusted load on s2 reflects both the reported load and the addition of
# the replica and lease, since the load-time is not more than 10s after the
# enactment time of change 1 (see lagForChangeReflectedInLoad).
get-load-info
----
store-id=1 node-id=1 status=ok accepting all reported=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] adjusted=[cpu:0s/s, write-bandwidth:0 B/s, byte-size:0 B] node-reported-cpu=160 node-adjusted-cpu=168 seq=2
store-id=2 node-id=1 status=ok accepting all reported=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] adjusted=[cpu:168ns/s, write-bandwidth:168 B/s, byte-size:168 B] node-reported-cpu=160 node-adjusted-cpu=168 seq=3
  top-k-ranges (local-store-id=2) dim=ByteSize: r1

# Store load msg from s2 at time 16s, which is 11s after the enactment of
# change 1. So the change is no longer needed for load adjustment.
store-load-msg
  store-id=2 node-id=1 load=[80,80,80] capacity=[100,100,100] secondary-load=1 load-time=16s
----

# We are no longer tracking change 1 for the sake of load.
get-pending-changes
----
pending(1)
change-id=2 store-id=1 node-id=1 range-id=1 load-delta=[cpu:-80ns/s, write-bandwidth:-80 B/s, byte-size:-80 B] start=0s gc=35s
  prev=(replica-id=1 type=VOTER_FULL)
  next=(replica-id=none type=VOTER_FULL)

# The adjusted load on s2 no longer reflects the addition of the replica and
# lease, since the load-time was more than 10s after the enactment time of
# change 1.
get-load-info
----
store-id=1 node-id=1 status=ok accepting all reported=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] adjusted=[cpu:0s/s, write-bandwidth:0 B/s, byte-size:0 B] node-reported-cpu=160 node-adjusted-cpu=80 seq=2
store-id=2 node-id=1 status=ok accepting all reported=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] adjusted=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] node-reported-cpu=160 node-adjusted-cpu=80 seq=4
  top-k-ranges (local-store-id=2) dim=ByteSize: r1

# Advance time, but not enough to GC change 2.
tick seconds=10
----
t=20s

# Change 2 is not undone.
gc-pending-changes
----
pending(1)
change-id=2 store-id=1 node-id=1 range-id=1 load-delta=[cpu:-80ns/s, write-bandwidth:-80 B/s, byte-size:-80 B] start=0s gc=35s
  prev=(replica-id=1 type=VOTER_FULL)
  next=(replica-id=none type=VOTER_FULL)

ranges
----
range-id=1 local-store=2 load=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] raft-cpu=20
  store-id=2 replica-id=2 type=VOTER_FULL leaseholder=true

# Store leaseholder msg from s2, showing that s1 is no longer a replica, so
# change 2 is considered enacted.
store-leaseholder-msg
store-id=2
  range-id=1 load=[80,80,80] raft-cpu=20
  config=(num_replicas=3 constraints={'+region=us-west-1:1'} voter_constraints={'+region=us-west-1:1'})
    store-id=2 replica-id=2 type=VOTER_FULL leaseholder=true
----

get-pending-changes
----
pending(1)
change-id=2 store-id=1 node-id=1 range-id=1 load-delta=[cpu:-80ns/s, write-bandwidth:-80 B/s, byte-size:-80 B] start=0s gc=35s enacted=20s
  prev=(replica-id=1 type=VOTER_FULL)
  next=(replica-id=none type=VOTER_FULL)

# Change 2 is still serving the purpose of adjusting the load on s1.
get-load-info
----
store-id=1 node-id=1 status=ok accepting all reported=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] adjusted=[cpu:0s/s, write-bandwidth:0 B/s, byte-size:0 B] node-reported-cpu=160 node-adjusted-cpu=80 seq=2
store-id=2 node-id=1 status=ok accepting all reported=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] adjusted=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] node-reported-cpu=160 node-adjusted-cpu=80 seq=4
  top-k-ranges (local-store-id=2) dim=CPURate: r1

# Store load msg from s2, showing its new load.
store-load-msg
  store-id=2 node-id=1 load=[85,85,85] capacity=[100,100,100] secondary-load=1 load-time=20s
----

get-load-info
----
store-id=1 node-id=1 status=ok accepting all reported=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] adjusted=[cpu:0s/s, write-bandwidth:0 B/s, byte-size:0 B] node-reported-cpu=165 node-adjusted-cpu=85 seq=2
store-id=2 node-id=1 status=ok accepting all reported=[cpu:85ns/s, write-bandwidth:85 B/s, byte-size:85 B] adjusted=[cpu:85ns/s, write-bandwidth:85 B/s, byte-size:85 B] node-reported-cpu=165 node-adjusted-cpu=85 seq=5
  top-k-ranges (local-store-id=2) dim=CPURate: r1

tick seconds=20
----
t=40s

# Store load msg from s1, showing its new load. The enacted change is no
# longer needed for load adjustment since load-time is 20s after the enactment
# time (and lagForChangeReflectedInLoad is 10s).
store-load-msg
  store-id=1 node-id=1 load=[5,5,5] capacity=[100,100,100] secondary-load=1 load-time=40s
----

# No longer tracking change 2 for the sake of load.
get-pending-changes
----
pending(0)

get-load-info
----
store-id=1 node-id=1 status=ok accepting all reported=[cpu:5ns/s, write-bandwidth:5 B/s, byte-size:5 B] adjusted=[cpu:5ns/s, write-bandwidth:5 B/s, byte-size:5 B] node-reported-cpu=90 node-adjusted-cpu=90 seq=3
store-id=2 node-id=1 status=ok accepting all reported=[cpu:85ns/s, write-bandwidth:85 B/s, byte-size:85 B] adjusted=[cpu:85ns/s, write-bandwidth:85 B/s, byte-size:85 B] node-reported-cpu=90 node-adjusted-cpu=90 seq=5
  top-k-ranges (local-store-id=2) dim=CPURate: r1

# Transfer the replica from s2 back to s1. The lease will also be transferred.
make-pending-changes range-id=1
  rebalance-replica: remove-store-id=2 add-store-id=1
----
pending(2)
change-id=3 store-id=1 node-id=1 range-id=1 load-delta=[cpu:88ns/s, write-bandwidth:88 B/s, byte-size:88 B] start=40s gc=5m40s
  prev=(replica-id=none type=VOTER_FULL)
  next=(replica-id=unknown type=VOTER_FULL leaseholder=true)
change-id=4 store-id=2 node-id=1 range-id=1 load-delta=[cpu:-80ns/s, write-bandwidth:-80 B/s, byte-size:-80 B] start=40s gc=5m40s
  prev=(replica-id=2 type=VOTER_FULL leaseholder=true)
  next=(replica-id=none type=VOTER_FULL)

# Adjusted load shows the change.
get-load-info
----
store-id=1 node-id=1 status=ok accepting all reported=[cpu:5ns/s, write-bandwidth:5 B/s, byte-size:5 B] adjusted=[cpu:93ns/s, write-bandwidth:93 B/s, byte-size:93 B] node-reported-cpu=90 node-adjusted-cpu=98 seq=4
store-id=2 node-id=1 status=ok accepting all reported=[cpu:85ns/s, write-bandwidth:85 B/s, byte-size:85 B] adjusted=[cpu:5ns/s, write-bandwidth:5 B/s, byte-size:5 B] node-reported-cpu=90 node-adjusted-cpu=98 seq=6
  top-k-ranges (local-store-id=2) dim=CPURate: r1

# Enough time elapses to GC the changes.
tick seconds=310
----
t=5m50s

gc-pending-changes
----
pending(0)

# No more pending changes.
get-pending-changes
----
pending(0)

# Adjusted load no longer reflects the changes.
get-load-info
----
store-id=1 node-id=1 status=ok accepting all reported=[cpu:5ns/s, write-bandwidth:5 B/s, byte-size:5 B] adjusted=[cpu:5ns/s, write-bandwidth:5 B/s, byte-size:5 B] node-reported-cpu=90 node-adjusted-cpu=90 seq=5
store-id=2 node-id=1 status=ok accepting all reported=[cpu:85ns/s, write-bandwidth:85 B/s, byte-size:85 B] adjusted=[cpu:85ns/s, write-bandwidth:85 B/s, byte-size:85 B] node-reported-cpu=90 node-adjusted-cpu=90 seq=7
  top-k-ranges (local-store-id=2) dim=CPURate: r1

# Transfer the replica from s2 back to s1. The lease will also be transferred.
make-pending-changes range-id=1
  rebalance-replica: remove-store-id=2 add-store-id=1
----
pending(2)
change-id=5 store-id=1 node-id=1 range-id=1 load-delta=[cpu:88ns/s, write-bandwidth:88 B/s, byte-size:88 B] start=5m50s gc=10m50s
  prev=(replica-id=none type=VOTER_FULL)
  next=(replica-id=unknown type=VOTER_FULL leaseholder=true)
change-id=6 store-id=2 node-id=1 range-id=1 load-delta=[cpu:-80ns/s, write-bandwidth:-80 B/s, byte-size:-80 B] start=5m50s gc=10m50s
  prev=(replica-id=2 type=VOTER_FULL leaseholder=true)
  next=(replica-id=none type=VOTER_FULL)

ranges
----
range-id=1 local-store=2 load=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] raft-cpu=20
  store-id=1 replica-id=unknown type=VOTER_FULL leaseholder=true

get-load-info
----
store-id=1 node-id=1 status=ok accepting all reported=[cpu:5ns/s, write-bandwidth:5 B/s, byte-size:5 B] adjusted=[cpu:93ns/s, write-bandwidth:93 B/s, byte-size:93 B] node-reported-cpu=90 node-adjusted-cpu=98 seq=6
store-id=2 node-id=1 status=ok accepting all reported=[cpu:85ns/s, write-bandwidth:85 B/s, byte-size:85 B] adjusted=[cpu:5ns/s, write-bandwidth:5 B/s, byte-size:5 B] node-reported-cpu=90 node-adjusted-cpu=98 seq=8
  top-k-ranges (local-store-id=2) dim=CPURate: r1

# Store leaseholder msg from s1, showing that s1 is a replica and has the lease, so
# change 5 is considered enacted.
store-leaseholder-msg
store-id=1
  range-id=1 load=[80,80,80] raft-cpu=20
  config=(num_replicas=3 constraints={'+region=us-west-1:1'} voter_constraints={'+region=us-west-1:1'})
    store-id=1 replica-id=3 type=VOTER_FULL leaseholder=true
    store-id=2 replica-id=2 type=VOTER_FULL
----

# Change 6 is not yet enacted, but the prev state no longer shows
# leaseholder=true.
get-pending-changes
----
pending(2)
change-id=5 store-id=1 node-id=1 range-id=1 load-delta=[cpu:88ns/s, write-bandwidth:88 B/s, byte-size:88 B] start=5m50s gc=10m50s enacted=5m50s
  prev=(replica-id=none type=VOTER_FULL)
  next=(replica-id=unknown type=VOTER_FULL leaseholder=true)
change-id=6 store-id=2 node-id=1 range-id=1 load-delta=[cpu:-80ns/s, write-bandwidth:-80 B/s, byte-size:-80 B] start=5m50s gc=6m20s
  prev=(replica-id=2 type=VOTER_FULL)
  next=(replica-id=none type=VOTER_FULL)

ranges
----
range-id=1 local-store=1 load=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] raft-cpu=20
  store-id=1 replica-id=3 type=VOTER_FULL leaseholder=true

# store-id=2 still has top-k-ranges for local store s2 populated, since we
# haven't yet received a store leaseholder msg from s2 indicating it no longer
# is the leaseholder for r1.
get-load-info
----
store-id=1 node-id=1 status=ok accepting all reported=[cpu:5ns/s, write-bandwidth:5 B/s, byte-size:5 B] adjusted=[cpu:93ns/s, write-bandwidth:93 B/s, byte-size:93 B] node-reported-cpu=90 node-adjusted-cpu=98 seq=6
  top-k-ranges (local-store-id=1) dim=ByteSize: r1
store-id=2 node-id=1 status=ok accepting all reported=[cpu:85ns/s, write-bandwidth:85 B/s, byte-size:85 B] adjusted=[cpu:5ns/s, write-bandwidth:5 B/s, byte-size:5 B] node-reported-cpu=90 node-adjusted-cpu=98 seq=8
  top-k-ranges (local-store-id=2) dim=CPURate: r1

# Change 6 is garbage collected since enough time has elapsed.
gc-pending-changes
----
pending(2)
change-id=5 store-id=1 node-id=1 range-id=1 load-delta=[cpu:88ns/s, write-bandwidth:88 B/s, byte-size:88 B] start=5m50s gc=10m50s enacted=5m50s
  prev=(replica-id=none type=VOTER_FULL)
  next=(replica-id=unknown type=VOTER_FULL leaseholder=true)
change-id=6 store-id=2 node-id=1 range-id=1 load-delta=[cpu:-80ns/s, write-bandwidth:-80 B/s, byte-size:-80 B] start=5m50s gc=6m20s
  prev=(replica-id=2 type=VOTER_FULL)
  next=(replica-id=none type=VOTER_FULL)

# So one change enacted and one was rolled back. This one leaseholder
# invariant is still satisfied.
ranges
----
range-id=1 local-store=1 load=[cpu:80ns/s, write-bandwidth:80 B/s, byte-size:80 B] raft-cpu=20
  store-id=1 replica-id=3 type=VOTER_FULL leaseholder=true
