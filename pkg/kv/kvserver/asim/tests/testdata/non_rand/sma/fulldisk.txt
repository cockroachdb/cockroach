# This test verifies that the allocator correctly handles disk fullness by
#  shedding replicas from stores that exceed the storage capacity threshold. It
# sets up a cluster where one store (s5) has significantly less capacity (100 GiB)
# than others (512 GiB), causing it to hit the disk fullness threshold and
# continuously shed replicas to maintain disk usage less than 95%.
skip_under_ci
----

# Set every store's capacity to 512 GiB, we will later adjust just one store to
# have less free capacity.
gen_cluster nodes=5 store_byte_capacity_gib=512
----

gen_ranges ranges=500 bytes_mib=286
----

gen_load rate=500 max_block=60000 min_block=60000
----
29 MiB/s goodput

# Set the disk storage capacity of s5 to 100 GiB. This will necessitate
# shedding replicas from s5 continously as the workload fills up ranges.
set_capacity store=5 capacity=107374182400
----

# We will repeatedly hit the disk fullness threshold which causes shedding
# replicas on store 5. We should see s5 hovering right around 92.5-95%
# (the storage capacity threshold value).
eval duration=20m seed=42 metrics=(replicas,disk_fraction_used) cfgs=(sma-count,mma-only,mma-count)
----
disk_fraction_used#1: first: [s1=0.20, s2=0.20, s3=0.20, s4=0.20, s5=1.05] (stddev=0.34, mean=0.37, sum=2)
disk_fraction_used#1: last:  [s1=0.27, s2=0.27, s3=0.27, s4=0.27, s5=0.92] (stddev=0.26, mean=0.40, sum=2)
disk_fraction_used#1: thrash_pct: [s1=17%, s2=13%, s3=13%, s4=16%, s5=59%]  (sum=118%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300] (stddev=0.00, mean=300.00, sum=1500)
replicas#1: last:  [s1=320, s2=323, s3=321, s4=323, s5=213] (stddev=43.52, mean=300.00, sum=1500)
replicas#1: thrash_pct: [s1=200%, s2=151%, s3=156%, s4=174%, s5=42%]  (sum=723%)
artifacts[sma-count]: 44017709e15df9bd
==========================
disk_fraction_used#1: first: [s1=0.20, s2=0.20, s3=0.20, s4=0.20, s5=1.05] (stddev=0.34, mean=0.37, sum=2)
disk_fraction_used#1: last:  [s1=0.28, s2=0.28, s3=0.28, s4=0.26, s5=0.86] (stddev=0.24, mean=0.39, sum=2)
disk_fraction_used#1: thrash_pct: [s1=1%, s2=1%, s3=1%, s4=0%, s5=33%]  (sum=35%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300] (stddev=0.00, mean=300.00, sum=1500)
replicas#1: last:  [s1=329, s2=329, s3=329, s4=313, s5=200] (stddev=50.38, mean=300.00, sum=1500)
replicas#1: thrash_pct: [s1=8%, s2=6%, s3=11%, s4=0%, s5=0%]  (sum=25%)
artifacts[mma-only]: b8b99125e978f8e3
==========================
disk_fraction_used#1: first: [s1=0.20, s2=0.20, s3=0.20, s4=0.20, s5=1.05] (stddev=0.34, mean=0.37, sum=2)
disk_fraction_used#1: last:  [s1=0.28, s2=0.28, s3=0.27, s4=0.27, s5=0.85] (stddev=0.23, mean=0.39, sum=2)
disk_fraction_used#1: thrash_pct: [s1=16%, s2=16%, s3=16%, s4=21%, s5=28%]  (sum=97%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300] (stddev=0.00, mean=300.00, sum=1500)
replicas#1: last:  [s1=328, s2=329, s3=323, s4=324, s5=196] (stddev=52.05, mean=300.00, sum=1500)
replicas#1: thrash_pct: [s1=155%, s2=158%, s3=159%, s4=193%, s5=0%]  (sum=664%)
artifacts[mma-count]: 4cbecdef59614451
==========================
