skip_under_ci
----

# Set every store's capacity to 512 GiB, we will later adjust just one store to
# have less free capacity.
gen_cluster nodes=5 store_byte_capacity=549755813888
----

gen_ranges ranges=500 bytes=300000000
----

gen_load rate=500 max_block=60000 min_block=60000
----

# Set the disk storage capacity of s5 to 100 GiB. This will necessitate
# shedding replicas from s5 continously as the workload fills up ranges.
set_capacity store=5 capacity=107374182400
----

# We will repeatedly hit the disk fullness threshold which causes shedding
# replicas on store 5. We should see s5 hovering right around 92.5-95%
# (the storage capacity threshold value).
eval duration=30m seed=42 metrics=(replicas,disk_fraction_used)
----
disk_fraction_used#1: first: [s1=0.20, s2=0.20, s3=0.20, s4=0.20, s5=1.05] (stddev=0.34, mean=0.37, sum=2)
disk_fraction_used#1: last:  [s1=0.30, s2=0.30, s3=0.30, s4=0.30, s5=0.95] (stddev=0.26, mean=0.43, sum=2)
disk_fraction_used#1: thrash_pct: [s1=25%, s2=20%, s3=24%, s4=24%, s5=98%]  (sum=190%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300] (stddev=0.00, mean=300.00, sum=1500)
replicas#1: last:  [s1=322, s2=328, s3=327, s4=323, s5=200] (stddev=50.05, mean=300.00, sum=1500)
replicas#1: thrash_pct: [s1=243%, s2=199%, s3=231%, s4=237%, s5=41%]  (sum=951%)
artifacts[default]: 686ff7227dd03ade
