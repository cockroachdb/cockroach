# This test verifies that the allocator can satisfy zone constraints when stores
# have limited disk capacity and replicas need to be rebalanced due to disk
# fullness. The test sets up a 6-node cluster across 3 regions (a, b, c) with
# 2, 3, and 1 nodes respectively. The span config requires 2 replicas in region
# a, 2 in region b, and 1 in region c, with lease preferences for region a. The
# test creates 12 ranges with 5 replicas each with a replica each on s1, s2
# (region a), and s6 (region c) and equally balanced between the 3 nodes s3, s4,
# and s4 (region b). Each store has 10 GiB capacity and each range is 500MiB,
# making stores s1, s2, and s6 around 73% full while s3, s4, and s5 are only 50%
# full.
#
# Expected outcome: The allocator should not perform any rebalancing. Stores s1
# and s2 in region a and s3 in region c are fuller than stores in region b,
# however moving any replicas from them would violate zone constraints. Leases
# should be distributed within region a (s1, s2) due to lease preferences when
# count-based rebalancing is enabled.
gen_cluster nodes=6 region=(a,b,c) nodes_per_region=(2,3,1) store_byte_capacity_gib=10
----

gen_ranges ranges=12 repl_factor=5 placement_type=replica_placement bytes_mib=500
{s1,s2,s3,s4,s6}:1 {s1,s2,s4,s5,s6}:1 {s1,s2,s3,s5,s6}:1
----
{s1:*,s2,s3,s4,s6}:1
{s1:*,s2,s4,s5,s6}:1
{s1:*,s2,s3,s5,s6}:1

set_span_config
[0,9999999999): num_replicas=5 num_voters=5 constraints={'+region=a':2,'+region=b':2,'+region=c':1} lease_preferences=[['+region=a']]
----

setting split_queue_enabled=false
----

assertion type=steady stat=replicas ticks=180 exact_bound=0
----
asserting: |replicas(t)/mean_{T}(replicas) - 1| = 0.00 ∀ t∈T and each store (T=last 180 ticks)

assertion type=conformance under=0 over=0 unavailable=0 violating=0
----

eval duration=3m samples=1 seed=42 cfgs=(sma-count,mma-count) metrics=(cpu,cpu_util,leases,replicas,disk_fraction_used)
----
sma-count:
disk_fraction_used#1: first: [s1=0.73, s2=0.73, s3=0.49, s4=0.49, s5=0.49, s6=0.73] (stddev=0.12, mean=0.61, sum=4)
disk_fraction_used#1: last:  [s1=0.73, s2=0.73, s3=0.49, s4=0.49, s5=0.49, s6=0.73] (stddev=0.12, mean=0.61, sum=4)
disk_fraction_used#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%, s6=0%]  (sum=0%)
leases#1: first: [s1=12, s2=0, s3=0, s4=0, s5=0, s6=0] (stddev=4.47, mean=2.00, sum=12)
leases#1: last:  [s1=10, s2=2, s3=0, s4=0, s5=0, s6=0] (stddev=3.65, mean=2.00, sum=12)
leases#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%, s6=0%]  (sum=0%)
replicas#1: first: [s1=12, s2=12, s3=8, s4=8, s5=8, s6=12] (stddev=2.00, mean=10.00, sum=60)
replicas#1: last:  [s1=12, s2=12, s3=8, s4=8, s5=8, s6=12] (stddev=2.00, mean=10.00, sum=60)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%, s6=0%]  (sum=0%)
hash: 8171cff9be3d23
==========================
mma-count:
disk_fraction_used#1: first: [s1=0.73, s2=0.73, s3=0.49, s4=0.49, s5=0.49, s6=0.73] (stddev=0.12, mean=0.61, sum=4)
disk_fraction_used#1: last:  [s1=0.73, s2=0.73, s3=0.49, s4=0.49, s5=0.49, s6=0.73] (stddev=0.12, mean=0.61, sum=4)
disk_fraction_used#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%, s6=0%]  (sum=0%)
leases#1: first: [s1=12, s2=0, s3=0, s4=0, s5=0, s6=0] (stddev=4.47, mean=2.00, sum=12)
leases#1: last:  [s1=10, s2=2, s3=0, s4=0, s5=0, s6=0] (stddev=3.65, mean=2.00, sum=12)
leases#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%, s6=0%]  (sum=0%)
replicas#1: first: [s1=12, s2=12, s3=8, s4=8, s5=8, s6=12] (stddev=2.00, mean=10.00, sum=60)
replicas#1: last:  [s1=12, s2=12, s3=8, s4=8, s5=8, s6=12] (stddev=2.00, mean=10.00, sum=60)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%, s6=0%]  (sum=0%)
hash: 8171cff9be3d23
==========================
