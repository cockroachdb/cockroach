# This test verifies that MMA can balance disks when the write patterns are
# skewed towards a set of nodes such that without MMA it would lead to
# imbalanced disk usage in the long run. It creates six nodes with 20 GiB
# store capacity. It creates 200 ranges of 1 MiB with three replicas each, such
# that each node has a 100 replicas (1% full). The first three nodes are
# targeted with a read-only workload. The last three nodes are targeted with a
# write-only workload. Initially, the CPU utilization is identical on all nodes.
#
# Expectation: MMA should perform rebalancing based on disk usage and write
# bandwidth to prevent one set of nodes from becoming full way in advance of the
# others
gen_cluster nodes=6 node_cpu_cores=2 store_byte_capacity_gib=20
----

gen_ranges ranges=100 repl_factor=3 min_key=0 max_key=10000 placement_type=replica_placement bytes_mib=1
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_ranges ranges=100 repl_factor=3 min_key=10000 max_key=20000 placement_type=replica_placement bytes_mib=1
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

# Read-only workload with 1 CPU s/s targeting the first node.
gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=1000000 min_key=0 max_key=10000
----
1.00 access-vcpus

# Write-only workload with 1 CPU s/s and 5 MiB/s write bandwidth targeting the
# second node.
gen_load rate=5000 rw_ratio=0 min_block=1024 max_block=1024 request_cpu_per_access=100000 raft_cpu_per_write=100000 min_key=10000 max_key=20000
----
0.50 access-vcpus, 0.50 raft-vcpus, 4.9 MiB/s goodput

eval duration=100m samples=1 seed=42 cfgs=(mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases,disk_fraction_used)
----
mma-count:
cpu#1: last:  [s1=520610798, s2=474823521, s3=495225579, s4=500445393, s5=500004395, s6=510019352] (stddev=14034650.80, mean=500188173.00, sum=3001129038)
cpu#1: thrash_pct: [s1=224%, s2=206%, s3=207%, s4=296%, s5=297%, s6=323%]  (sum=1553%)
cpu_util#1: last:  [s1=0.26, s2=0.24, s3=0.25, s4=0.25, s5=0.25, s6=0.26] (stddev=0.01, mean=0.25, sum=2)
cpu_util#1: thrash_pct: [s1=224%, s2=206%, s3=207%, s4=296%, s5=297%, s6=323%]  (sum=1553%)
disk_fraction_used#1: first: [s1=0.01, s2=0.01, s3=0.01, s4=0.01, s5=0.01, s6=0.01] (stddev=0.00, mean=0.01, sum=0)
disk_fraction_used#1: last:  [s1=0.85, s2=0.86, s3=0.92, s4=0.90, s5=0.94, s6=0.94] (stddev=0.03, mean=0.90, sum=5)
disk_fraction_used#1: thrash_pct: [s1=23%, s2=29%, s3=27%, s4=49%, s5=37%, s6=37%]  (sum=203%)
leases#1: first: [s1=100, s2=0, s3=0, s4=100, s5=0, s6=0] (stddev=47.14, mean=33.33, sum=200)
leases#1: last:  [s1=37, s2=33, s3=34, s4=33, s5=32, s6=31] (stddev=1.89, mean=33.33, sum=200)
leases#1: thrash_pct: [s1=126%, s2=131%, s3=133%, s4=119%, s5=106%, s6=112%]  (sum=727%)
replicas#1: first: [s1=100, s2=100, s3=100, s4=100, s5=100, s6=100] (stddev=0.00, mean=100.00, sum=600)
replicas#1: last:  [s1=104, s2=104, s3=106, s4=91, s5=99, s6=96] (stddev=5.26, mean=100.00, sum=600)
replicas#1: thrash_pct: [s1=419%, s2=478%, s3=488%, s4=344%, s5=362%, s6=334%]  (sum=2424%)
write_bytes_per_second#1: last:  [s1=2407223, s2=2458357, s3=2611476, s4=2560014, s5=2662641, s6=2664705] (stddev=98163.94, mean=2560736.00, sum=15364416)
write_bytes_per_second#1: thrash_pct: [s1=202%, s2=203%, s3=214%, s4=417%, s5=391%, s6=427%]  (sum=1853%)
hash: 1d243026d5373c8f
==========================
