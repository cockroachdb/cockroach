# This test verifies that MMA can balance disks when the write patterns are
# skewed towards a set of nodes such that without MMA it would lead to
# imbalanced disk usage in the long run. It creates six nodes with 20 GiB
# store capacity. It creates 200 ranges of 1 MiB with three replicas each, such
# that each node has a 100 replicas (1% full). The first three nodes are
# targeted with a read-only workload. The last three nodes are targeted with a
# write-only workload. Initially, the CPU utilization is identical on all nodes.
#
# Expectation: MMA should perform rebalancing based on disk usage and write
# bandwidth to prevent one set of nodes from becoming full way in advance of the
# others
gen_cluster nodes=6 node_cpu_cores=2 store_byte_capacity_gib=20
----

gen_ranges ranges=100 repl_factor=3 min_key=0 max_key=10000 placement_type=replica_placement bytes_mib=1
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_ranges ranges=100 repl_factor=3 min_key=10000 max_key=20000 placement_type=replica_placement bytes_mib=1
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

# Read-only workload with 1 CPU s/s targeting the first node.
gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=1000000 min_key=0 max_key=10000
----
1.00 access-vcpus

# Write-only workload with 1 CPU s/s and 5 MiB/s write bandwidth targeting the
# second node.
gen_load rate=5000 rw_ratio=0 min_block=1024 max_block=1024 request_cpu_per_access=100000 raft_cpu_per_write=100000 min_key=10000 max_key=20000
----
0.50 access-vcpus, 0.50 raft-vcpus, 4.9 MiB/s goodput

eval duration=100m samples=1 seed=42 cfgs=(mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases,disk_fraction_used)
----
mma-count:
cpu#1: last:  [s1=505923913, s2=514840618, s3=489492474, s4=501292062, s5=445445970, s6=544674650] (stddev=29829839.97, mean=500278281.17, sum=3001669687)
cpu#1: thrash_pct: [s1=201%, s2=167%, s3=179%, s4=256%, s5=280%, s6=276%]  (sum=1360%)
cpu_util#1: last:  [s1=0.25, s2=0.26, s3=0.24, s4=0.25, s5=0.22, s6=0.27] (stddev=0.01, mean=0.25, sum=2)
cpu_util#1: thrash_pct: [s1=201%, s2=167%, s3=179%, s4=256%, s5=280%, s6=276%]  (sum=1360%)
disk_fraction_used#1: first: [s1=0.01, s2=0.01, s3=0.01, s4=0.01, s5=0.01, s6=0.01] (stddev=0.00, mean=0.01, sum=0)
disk_fraction_used#1: last:  [s1=0.90, s2=0.90, s3=0.88, s4=0.90, s5=0.90, s6=0.92] (stddev=0.01, mean=0.90, sum=5)
disk_fraction_used#1: thrash_pct: [s1=19%, s2=18%, s3=23%, s4=35%, s5=47%, s6=31%]  (sum=174%)
leases#1: first: [s1=100, s2=0, s3=0, s4=100, s5=0, s6=0] (stddev=47.14, mean=33.33, sum=200)
leases#1: last:  [s1=35, s2=34, s3=33, s4=31, s5=30, s6=37] (stddev=2.36, mean=33.33, sum=200)
leases#1: thrash_pct: [s1=120%, s2=104%, s3=118%, s4=98%, s5=98%, s6=88%]  (sum=628%)
replicas#1: first: [s1=100, s2=100, s3=100, s4=100, s5=100, s6=100] (stddev=0.00, mean=100.00, sum=600)
replicas#1: last:  [s1=102, s2=101, s3=102, s4=101, s5=92, s6=102] (stddev=3.61, mean=100.00, sum=600)
replicas#1: thrash_pct: [s1=401%, s2=383%, s3=422%, s4=354%, s5=344%, s6=316%]  (sum=2219%)
write_bytes_per_second#1: last:  [s1=2559741, s2=2559784, s3=2508010, s4=2562066, s5=2561532, s6=2612294] (stddev=30117.53, mean=2560571.17, sum=15363427)
write_bytes_per_second#1: thrash_pct: [s1=153%, s2=162%, s3=169%, s4=357%, s5=372%, s6=361%]  (sum=1573%)
hash: 32eb18d2fd76980b
==========================
