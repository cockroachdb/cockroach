# This test verifies the allocator's behavior with opposing CPU and write load
# distributions, complicated by the presence of many "cold" (idle) ranges that
# prevent count-based rebalancing from "accidentally" fixing the load imbalance.
#
#   Keyspace          Ranges   Placement   Load
#   ─────────────────────────────────────────────────────────────
#   [1, 10000)         36      s1-s3       read-heavy
#   [10001, 20000)     36      s4-s6       write-heavy
#   [20001, 30000)    100      s1-s3       (cold)
#   [30001, 40000)    100      s4-s6       (cold)
#   ─────────────────────────────────────────────────────────────
#
# The cold ranges are important to prevent count-based rebalancing from
# "accidentally" fixing the write imbalance (after CPU-based rebalancing
# has created a count-imbalance).
#
# Expected outcome:
#
# MMA balances both CPU and writes intentionally, reaching a stable equilibrium
# where both metrics are evenly distributed. It moves the hot ranges (both read
# and write) to achieve this, ignoring the cold ranges.
#
# SMA (sma-count) only optimizes for CPU and has no visibility into write load.
# It balances CPU but leaves writes heavily skewed on s4-s6. The high thrash
# (~1500% CPU) comes from the store rebalancer (optimizing CPU) and the
# replicate/lease queues (optimizing counts) working against each other.
gen_cluster nodes=6 node_cpu_cores=5
----

# Hot read ranges on s1-s3: high CPU, no writes
gen_ranges ranges=36 min_key=1 max_key=10000 placement_type=replica_placement
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=5000000 min_key=1 max_key=10000
----
5.00 access-vcpus

# Cold ranges co-located with the hot read ranges on s1-s3.
#
# These raise the absolute threshold for count-based rebalancing, preventing
# small movements from triggering fixes that might accidentally redistribute
# the hot ranges and fix IO imbalance.
gen_ranges ranges=100 min_key=20001 max_key=30000 placement_type=replica_placement
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

# Hot write ranges on s4-s6: high write throughput, minimal CPU
gen_ranges ranges=36 min_key=10001 max_key=20000 placement_type=replica_placement
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

# raft_cpu_per_write=1 makes writes contribute negligible CPU. This isolates the
# two dimensions: s1-s3 are CPU-hot (reads), and s4-s6 are write-hot but
# CPU-cold.
gen_load rate=20000 rw_ratio=0 min_block=1000 max_block=1000 raft_cpu_per_write=1 min_key=10001 max_key=20000
----
0.00 raft-vcpus, 19 MiB/s goodput

# Cold ranges co-located with the hot write ranges on s4-s6 (same principle).
gen_ranges ranges=100 min_key=30001 max_key=40000 placement_type=replica_placement
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

setting split_queue_enabled=false
----

eval duration=25m samples=1 seed=42 cfgs=(sma-count,mma-only,mma-count) metrics=(cpu_util,write_bytes_per_second,replicas,leases)
----
sma-count:
cpu_util#1: last:  [s1=0.17, s2=0.17, s3=0.19, s4=0.17, s5=0.17, s6=0.14] (stddev=0.02, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=317%, s2=139%, s3=420%, s4=230%, s5=181%, s6=204%]  (sum=1490%)
leases#1: first: [s1=136, s2=0, s3=0, s4=136, s5=0, s6=0] (stddev=64.11, mean=45.33, sum=272)
leases#1: last:  [s1=42, s2=44, s3=41, s4=51, s5=48, s6=46] (stddev=3.45, mean=45.33, sum=272)
leases#1: thrash_pct: [s1=86%, s2=49%, s3=103%, s4=68%, s5=46%, s6=49%]  (sum=401%)
replicas#1: first: [s1=136, s2=136, s3=136, s4=136, s5=136, s6=136] (stddev=0.00, mean=136.00, sum=816)
replicas#1: last:  [s1=135, s2=129, s3=135, s4=138, s5=139, s6=140] (stddev=3.65, mean=136.00, sum=816)
replicas#1: thrash_pct: [s1=0%, s2=245%, s3=13%, s4=115%, s5=103%, s6=66%]  (sum=543%)
write_bytes_per_second#1: last:  [s1=0, s2=2772297, s3=0, s4=18336933, s5=20000880, s6=18892529] (stddev=9136363.37, mean=10000439.83, sum=60002639)
write_bytes_per_second#1: thrash_pct: [s1=0%, s2=2%, s3=0%, s4=89%, s5=100%, s6=101%]  (sum=292%)
hash: bbe8bd0a4c78e7a1
==========================
mma-only:
cpu_util#1: last:  [s1=0.17, s2=0.17, s3=0.17, s4=0.17, s5=0.17, s6=0.17] (stddev=0.00, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=7%, s2=36%, s3=58%, s4=11%, s5=14%, s6=11%]  (sum=137%)
leases#1: first: [s1=136, s2=0, s3=0, s4=136, s5=0, s6=0] (stddev=64.11, mean=45.33, sum=272)
leases#1: last:  [s1=106, s2=15, s3=13, s4=125, s5=7, s6=6] (stddev=50.02, mean=45.33, sum=272)
leases#1: thrash_pct: [s1=0%, s2=5%, s3=10%, s4=10%, s5=0%, s6=0%]  (sum=25%)
replicas#1: first: [s1=136, s2=136, s3=136, s4=136, s5=136, s6=136] (stddev=0.00, mean=136.00, sum=816)
replicas#1: last:  [s1=143, s2=152, s3=147, s4=125, s5=125, s6=124] (stddev=11.63, mean=136.00, sum=816)
replicas#1: thrash_pct: [s1=52%, s2=20%, s3=36%, s4=36%, s5=42%, s6=36%]  (sum=222%)
write_bytes_per_second#1: last:  [s1=8919836, s2=10580413, s3=9472741, s4=10523766, s5=10522676, s6=9966818] (stddev=623194.21, mean=9997708.33, sum=59986250)
write_bytes_per_second#1: thrash_pct: [s1=1%, s2=13%, s3=5%, s4=2%, s5=109%, s6=109%]  (sum=240%)
hash: aeb61e39aec0ac0f
==========================
mma-count:
cpu_util#1: last:  [s1=0.17, s2=0.17, s3=0.17, s4=0.17, s5=0.17, s6=0.17] (stddev=0.00, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=11%, s2=55%, s3=44%, s4=19%, s5=13%, s6=11%]  (sum=154%)
leases#1: first: [s1=136, s2=0, s3=0, s4=136, s5=0, s6=0] (stddev=64.11, mean=45.33, sum=272)
leases#1: last:  [s1=48, s2=51, s3=44, s4=45, s5=41, s6=43] (stddev=3.30, mean=45.33, sum=272)
leases#1: thrash_pct: [s1=17%, s2=15%, s3=13%, s4=14%, s5=19%, s6=18%]  (sum=96%)
replicas#1: first: [s1=136, s2=136, s3=136, s4=136, s5=136, s6=136] (stddev=0.00, mean=136.00, sum=816)
replicas#1: last:  [s1=140, s2=140, s3=141, s4=134, s5=131, s6=130] (stddev=4.51, mean=136.00, sum=816)
replicas#1: thrash_pct: [s1=130%, s2=162%, s3=152%, s4=160%, s5=141%, s6=132%]  (sum=877%)
write_bytes_per_second#1: last:  [s1=8919708, s2=10580352, s3=10582195, s4=9419023, s5=10519058, s6=9970520] (stddev=639088.01, mean=9998476.00, sum=59990856)
write_bytes_per_second#1: thrash_pct: [s1=11%, s2=5%, s3=25%, s4=209%, s5=203%, s6=221%]  (sum=673%)
hash: da053356de7643f
==========================
