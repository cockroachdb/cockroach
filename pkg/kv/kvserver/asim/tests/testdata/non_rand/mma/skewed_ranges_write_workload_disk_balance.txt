# This test verifies that MMA can balance disks when the imbalance is already
# high under a write-heavy workload. In this setting, doing disk based
# rebalancing goes against the goals of CPU and write bandwidth balancing. The
# test creates six nodes with 20 GiB store capacity. The first three nodes have
# 100 ranges of 90 MiB each (55% full). The last three nodes have 100 ranges of
# 1 MiB each (1% full). A write-only workload is started that targets both sets
# of nodes evenly.
#
# Expectation: MMA should perform rebalancing based on disk usage while trying
# to keep CPU utilization and write bandwidth also balanced. The hope is that
# MMA extends the time it takes for the first node to hit high disk-usage
# threshold compared to SMA.
gen_cluster nodes=6 node_cpu_cores=2 store_byte_capacity_gib=20
----

gen_ranges ranges=100 repl_factor=3 min_key=0 max_key=10000 placement_type=replica_placement bytes_mib=90
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_ranges ranges=100 repl_factor=3 min_key=10000 max_key=20000 placement_type=replica_placement bytes_mib=1
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

gen_load rate=5000 rw_ratio=0 min_block=1024 max_block=1024 request_cpu_per_access=100000 raft_cpu_per_write=100000 min_key=0 max_key=20000
----
0.50 access-vcpus, 0.50 raft-vcpus, 4.9 MiB/s goodput

eval duration=75m samples=1 seed=42 cfgs=(mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases,disk_fraction_used)
----
mma-count:
cpu#1: last:  [s1=334729142, s2=302843142, s3=305291543, s4=337441649, s5=357714800, s6=342103174] (stddev=19749852.86, mean=330020575.00, sum=1980123450)
cpu#1: thrash_pct: [s1=386%, s2=405%, s3=348%, s4=372%, s5=398%, s6=354%]  (sum=2264%)
cpu_util#1: last:  [s1=0.17, s2=0.15, s3=0.15, s4=0.17, s5=0.18, s6=0.17] (stddev=0.01, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=386%, s2=405%, s3=348%, s4=372%, s5=398%, s6=354%]  (sum=2264%)
disk_fraction_used#1: first: [s1=0.55, s2=0.55, s3=0.55, s4=0.01, s5=0.01, s6=0.01] (stddev=0.27, mean=0.28, sum=2)
disk_fraction_used#1: last:  [s1=0.99, s2=0.91, s3=0.94, s4=0.93, s5=0.95, s6=0.96] (stddev=0.02, mean=0.95, sum=6)
disk_fraction_used#1: thrash_pct: [s1=135%, s2=130%, s3=74%, s4=57%, s5=72%, s6=97%]  (sum=565%)
leases#1: first: [s1=100, s2=0, s3=0, s4=100, s5=0, s6=0] (stddev=47.14, mean=33.33, sum=200)
leases#1: last:  [s1=36, s2=27, s3=31, s4=36, s5=37, s6=33] (stddev=3.50, mean=33.33, sum=200)
leases#1: thrash_pct: [s1=66%, s2=73%, s3=59%, s4=66%, s5=68%, s6=55%]  (sum=387%)
replicas#1: first: [s1=100, s2=100, s3=100, s4=100, s5=100, s6=100] (stddev=0.00, mean=100.00, sum=600)
replicas#1: last:  [s1=101, s2=94, s3=93, s4=101, s5=107, s6=104] (stddev=5.03, mean=100.00, sum=600)
replicas#1: thrash_pct: [s1=638%, s2=612%, s3=331%, s4=497%, s5=635%, s6=647%]  (sum=3359%)
write_bytes_per_second#1: last:  [s1=2532535, s2=2407806, s3=2331280, s4=2560085, s5=2716437, s6=2659201] (stddev=133328.91, mean=2534557.33, sum=15207344)
write_bytes_per_second#1: thrash_pct: [s1=1132%, s2=1144%, s3=968%, s4=1058%, s5=1156%, s6=1044%]  (sum=6502%)
hash: ce455037b65afaed
==========================
