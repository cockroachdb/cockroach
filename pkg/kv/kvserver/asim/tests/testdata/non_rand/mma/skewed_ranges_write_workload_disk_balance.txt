# This test verifies that MMA can balance disks when the imbalance is already
# high under a write-heavy workload. In this setting, doing disk based
# rebalancing goes against the goals of CPU and write bandwidth balancing. The
# test creates six nodes with 20 GiB store capacity. The first three nodes have
# 100 ranges of 90 MiB each (55% full). The last three nodes have 100 ranges of
# 1 MiB each (1% full). A write-only workload is started that targets both sets
# of nodes evenly.
#
# Expectation: MMA should perform rebalancing based on disk usage while trying
# to keep CPU utilization and write bandwidth also balanced. The hope is that
# MMA extends the time it takes for the first node to hit high disk-usage
# threshold compared to SMA.
gen_cluster nodes=6 node_cpu_cores=2 store_byte_capacity_gib=20
----

gen_ranges ranges=100 repl_factor=3 min_key=0 max_key=10000 placement_type=replica_placement bytes_mib=90
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_ranges ranges=100 repl_factor=3 min_key=10000 max_key=20000 placement_type=replica_placement bytes_mib=1
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

gen_load rate=5000 rw_ratio=0 min_block=1024 max_block=1024 request_cpu_per_access=100000 raft_cpu_per_write=100000 min_key=0 max_key=20000
----
0.50 access-vcpus, 0.50 raft-vcpus, 4.9 MiB/s goodput

eval duration=75m samples=1 seed=42 cfgs=(mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases,disk_fraction_used)
----
mma-count:
cpu#1: last:  [s1=330429138, s2=307506061, s3=316863483, s4=344402356, s5=347292593, s6=332700350] (stddev=14097131.94, mean=329865663.50, sum=1979193981)
cpu#1: thrash_pct: [s1=491%, s2=413%, s3=461%, s4=425%, s5=431%, s6=386%]  (sum=2605%)
cpu_util#1: last:  [s1=0.17, s2=0.15, s3=0.16, s4=0.17, s5=0.17, s6=0.17] (stddev=0.01, mean=0.16, sum=1)
cpu_util#1: thrash_pct: [s1=491%, s2=413%, s3=461%, s4=425%, s5=431%, s6=386%]  (sum=2605%)
disk_fraction_used#1: first: [s1=0.55, s2=0.55, s3=0.55, s4=0.01, s5=0.01, s6=0.01] (stddev=0.27, mean=0.28, sum=2)
disk_fraction_used#1: last:  [s1=0.96, s2=0.95, s3=0.93, s4=0.97, s5=0.94, s6=0.94] (stddev=0.01, mean=0.95, sum=6)
disk_fraction_used#1: thrash_pct: [s1=175%, s2=100%, s3=139%, s4=93%, s5=93%, s6=82%]  (sum=683%)
leases#1: first: [s1=100, s2=0, s3=0, s4=100, s5=0, s6=0] (stddev=47.14, mean=33.33, sum=200)
leases#1: last:  [s1=34, s2=30, s3=30, s4=39, s5=34, s6=33] (stddev=3.04, mean=33.33, sum=200)
leases#1: thrash_pct: [s1=83%, s2=63%, s3=81%, s4=83%, s5=74%, s6=59%]  (sum=443%)
replicas#1: first: [s1=100, s2=100, s3=100, s4=100, s5=100, s6=100] (stddev=0.00, mean=100.00, sum=600)
replicas#1: last:  [s1=98, s2=94, s3=98, s4=103, s5=106, s6=101] (stddev=3.87, mean=100.00, sum=600)
replicas#1: thrash_pct: [s1=935%, s2=538%, s3=693%, s4=786%, s5=813%, s6=651%]  (sum=4416%)
write_bytes_per_second#1: last:  [s1=2511195, s2=2380741, s3=2479587, s4=2582128, s5=2685024, s6=2561533] (stddev=93852.18, mean=2533368.00, sum=15200208)
write_bytes_per_second#1: thrash_pct: [s1=1694%, s2=1429%, s3=1563%, s4=1400%, s5=1464%, s6=1353%]  (sum=8901%)
hash: fcee53f9469a8659
==========================
