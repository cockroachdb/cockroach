# This test verifies the allocator's behavior with replication factor 1 and
# skewed workloads across two stores. The test sets up a 2-node cluster where
# store s1 handles all read traffic (high CPU load from request processing)
# while store s2 handles all write traffic (higher write bandwidth).
#
# Expected outcome: two stores should roughly equalize their cpu load and write
# load via range rebalancing.
gen_cluster nodes=2 node_cpu_cores=1
----

gen_ranges ranges=100 repl_factor=1 min_key=1 max_key=10000 placement_type=replica_placement bytes_mib=26
{s1}:1
----
{s1:*}:1

gen_ranges ranges=100 repl_factor=1 min_key=10001 max_key=20000 placement_type=replica_placement bytes_mib=26
{s2}:1
----
{s2:*}:1

# read cpu load of 1000x1m=1g, all hitting s1, which is then at 100% cpu.
gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=1000000 min_key=1 max_key=10000
----
1.00 access-vcpus

# Write only workload, which generates 20% cpu and 5mb of writes per second.
# over the second half of the keyspace.
gen_load rate=5000 rw_ratio=0 min_block=1024 max_block=1024 request_cpu_per_access=20000 raft_cpu_per_write=20000 min_key=10001 max_key=20000
----
0.10 access-vcpus, 0.10 raft-vcpus, 4.9 MiB/s goodput

setting split_queue_enabled=false
----

eval duration=75m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=654035360, s2=545548889] (stddev=54243235.50, mean=599792124.50, sum=1199584249)
cpu#1: thrash_pct: [s1=37%, s2=87%]  (sum=124%)
cpu_util#1: last:  [s1=0.65, s2=0.55] (stddev=0.05, mean=0.60, sum=1)
cpu_util#1: thrash_pct: [s1=37%, s2=87%]  (sum=124%)
leases#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
leases#1: last:  [s1=103, s2=97] (stddev=3.00, mean=100.00, sum=200)
leases#1: thrash_pct: [s1=341%, s2=341%]  (sum=681%)
replicas#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
replicas#1: last:  [s1=103, s2=97] (stddev=3.00, mean=100.00, sum=200)
replicas#1: thrash_pct: [s1=341%, s2=341%]  (sum=681%)
write_bytes_per_second#1: last:  [s1=2407032, s2=2712709] (stddev=152838.50, mean=2559870.50, sum=5119741)
write_bytes_per_second#1: thrash_pct: [s1=26%, s2=3%]  (sum=29%)
artifacts[mma-only]: 11c4fca9097a1184
==========================
cpu#1: last:  [s1=643963508, s2=556180363] (stddev=43891572.50, mean=600071935.50, sum=1200143871)
cpu#1: thrash_pct: [s1=45%, s2=96%]  (sum=140%)
cpu_util#1: last:  [s1=0.64, s2=0.56] (stddev=0.04, mean=0.60, sum=1)
cpu_util#1: thrash_pct: [s1=45%, s2=96%]  (sum=140%)
leases#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
leases#1: last:  [s1=102, s2=98] (stddev=2.00, mean=100.00, sum=200)
leases#1: thrash_pct: [s1=482%, s2=482%]  (sum=964%)
replicas#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
replicas#1: last:  [s1=102, s2=98] (stddev=2.00, mean=100.00, sum=200)
replicas#1: thrash_pct: [s1=482%, s2=482%]  (sum=964%)
write_bytes_per_second#1: last:  [s1=2406098, s2=2714619] (stddev=154260.50, mean=2560358.50, sum=5120717)
write_bytes_per_second#1: thrash_pct: [s1=26%, s2=6%]  (sum=32%)
artifacts[mma-count]: d268eb8a6d6bf0b8
==========================
