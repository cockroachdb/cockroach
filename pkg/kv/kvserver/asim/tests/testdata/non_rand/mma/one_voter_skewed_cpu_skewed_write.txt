# This test verifies the allocator's behavior with replication factor 1 and
# skewed workloads across two stores. The test sets up a 2-node cluster where
# store s1 handles all read traffic (high CPU load from request processing)
# while store s2 handles all write traffic (higher write bandwidth).
#
# Expected outcome: two stores should roughly equalize their cpu load and write
# load via range rebalancing.
gen_cluster nodes=2 node_cpu_cores=1
----

gen_ranges ranges=100 repl_factor=1 min_key=1 max_key=10000 placement_type=replica_placement bytes_mib=26
{s1}:1
----
{s1:*}:1

gen_ranges ranges=100 repl_factor=1 min_key=10001 max_key=20000 placement_type=replica_placement bytes_mib=26
{s2}:1
----
{s2:*}:1

# read cpu load of 1000x1m=1g, all hitting s1, which is then at 100% cpu.
gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=1000000 min_key=1 max_key=10000
----
1.00 access-vcpus

# Write only workload, which generates 20% cpu and 5mb of writes per second.
# over the second half of the keyspace.
gen_load rate=5000 rw_ratio=0 min_block=1024 max_block=1024 request_cpu_per_access=20000 raft_cpu_per_write=20000 min_key=10001 max_key=20000
----
0.10 access-vcpus, 0.10 raft-vcpus, 4.9 MiB/s goodput

setting split_queue_enabled=false
----

eval duration=100m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=653996124, s2=546088219] (stddev=53953952.50, mean=600042171.50, sum=1200084343)
cpu#1: thrash_pct: [s1=39%, s2=90%]  (sum=129%)
cpu_util#1: last:  [s1=0.65, s2=0.55] (stddev=0.05, mean=0.60, sum=1)
cpu_util#1: thrash_pct: [s1=39%, s2=90%]  (sum=129%)
leases#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
leases#1: last:  [s1=103, s2=97] (stddev=3.00, mean=100.00, sum=200)
leases#1: thrash_pct: [s1=341%, s2=341%]  (sum=681%)
replicas#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
replicas#1: last:  [s1=103, s2=97] (stddev=3.00, mean=100.00, sum=200)
replicas#1: thrash_pct: [s1=341%, s2=341%]  (sum=681%)
write_bytes_per_second#1: last:  [s1=2407102, s2=2714950] (stddev=153924.00, mean=2561026.00, sum=5122052)
write_bytes_per_second#1: thrash_pct: [s1=27%, s2=4%]  (sum=31%)
artifacts[mma-only]: 5d128a0cfbc69195
==========================
cpu#1: last:  [s1=644095382, s2=556195483] (stddev=43949949.50, mean=600145432.50, sum=1200290865)
cpu#1: thrash_pct: [s1=47%, s2=98%]  (sum=146%)
cpu_util#1: last:  [s1=0.64, s2=0.56] (stddev=0.04, mean=0.60, sum=1)
cpu_util#1: thrash_pct: [s1=47%, s2=98%]  (sum=146%)
leases#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
leases#1: last:  [s1=102, s2=98] (stddev=2.00, mean=100.00, sum=200)
leases#1: thrash_pct: [s1=482%, s2=482%]  (sum=964%)
replicas#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
replicas#1: last:  [s1=102, s2=98] (stddev=2.00, mean=100.00, sum=200)
replicas#1: thrash_pct: [s1=482%, s2=482%]  (sum=964%)
write_bytes_per_second#1: last:  [s1=2407198, s2=2714428] (stddev=153615.00, mean=2560813.00, sum=5121626)
write_bytes_per_second#1: thrash_pct: [s1=27%, s2=7%]  (sum=34%)
artifacts[mma-count]: 6dd2870d4095afcb
==========================
