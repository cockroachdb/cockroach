# This test verifies the allocator's behavior with replication factor 1 and
# skewed workloads across two stores. The test sets up a 2-node cluster where
# store s1 handles all read traffic (high CPU load from request processing)
# while store s2 handles all write traffic (higher write bandwidth).
#
# Expected outcome: two stores should roughly equalize their cpu load and write
# load via range rebalancing.
gen_cluster nodes=2 node_cpu_rate_capacity=1000000000
----

gen_ranges ranges=100 repl_factor=1 min_key=1 max_key=10000 placement_type=replica_placement bytes_mib=26
{s1}:1
----
{s1:*}:1

gen_ranges ranges=100 repl_factor=1 min_key=10001 max_key=20000 placement_type=replica_placement bytes_mib=26
{s2}:1
----
{s2:*}:1

# read cpu load of 1000x100=10k, all hitting s1, which is then at 100% cpu.
# TODO(tbg): the CPU count is accidentally too low.
gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=500000 min_key=1 max_key=10000
----
0.50 access-vcpus

# Write only workload, which generates 20% cpu and 5mb of writes per second.
# over the second half of the keyspace.
# TODO(tbg): the CPU load is too low.
gen_load rate=5000 rw_ratio=0 min_block=1000 max_block=1000 raft_cpu_per_write=1 min_key=10001 max_key=20000
----
0.00 raft-vcpus, 4.8 MiB/s goodput

setting split_queue_enabled=false
----

eval duration=65m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=269749279, s2=229873426] (stddev=19937926.50, mean=249811352.50, sum=499622705)
cpu#1: thrash_pct: [s1=2%, s2=10%]  (sum=13%)
cpu_util#1: last:  [s1=0.27, s2=0.23] (stddev=0.02, mean=0.25, sum=0)
cpu_util#1: thrash_pct: [s1=2%, s2=10%]  (sum=13%)
leases#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
leases#1: last:  [s1=99, s2=101] (stddev=1.00, mean=100.00, sum=200)
leases#1: thrash_pct: [s1=1237%, s2=1237%]  (sum=2473%)
replicas#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
replicas#1: last:  [s1=99, s2=101] (stddev=1.00, mean=100.00, sum=200)
replicas#1: thrash_pct: [s1=1237%, s2=1237%]  (sum=2473%)
write_bytes_per_second#1: last:  [s1=2249878, s2=2748782] (stddev=249452.00, mean=2499330.00, sum=4998660)
write_bytes_per_second#1: thrash_pct: [s1=6%, s2=1%]  (sum=7%)
artifacts[mma-only]: 56a02c8deb41c744
==========================
cpu#1: last:  [s1=269749279, s2=229873426] (stddev=19937926.50, mean=249811352.50, sum=499622705)
cpu#1: thrash_pct: [s1=2%, s2=10%]  (sum=13%)
cpu_util#1: last:  [s1=0.27, s2=0.23] (stddev=0.02, mean=0.25, sum=0)
cpu_util#1: thrash_pct: [s1=2%, s2=10%]  (sum=13%)
leases#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
leases#1: last:  [s1=99, s2=101] (stddev=1.00, mean=100.00, sum=200)
leases#1: thrash_pct: [s1=1237%, s2=1237%]  (sum=2473%)
replicas#1: first: [s1=100, s2=100] (stddev=0.00, mean=100.00, sum=200)
replicas#1: last:  [s1=99, s2=101] (stddev=1.00, mean=100.00, sum=200)
replicas#1: thrash_pct: [s1=1237%, s2=1237%]  (sum=2473%)
write_bytes_per_second#1: last:  [s1=2249619, s2=2749047] (stddev=249714.00, mean=2499333.00, sum=4998666)
write_bytes_per_second#1: thrash_pct: [s1=6%, s2=1%]  (sum=7%)
artifacts[mma-count]: 234702ad556b9e82
==========================
