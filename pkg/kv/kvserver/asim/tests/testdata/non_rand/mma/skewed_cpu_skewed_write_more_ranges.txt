# This test is similar to skewed_cpu_skewed_write.txt, but with more ranges 300. 
# To have the same amount of total load, bytes per range is reduced to 26 MiB.
# 
# Expected outcome: The allocator should rebalance both cpu and write load across
# all stores, with mma achieving better results than sma. 
gen_cluster nodes=6 node_cpu_rate_capacity=5000000000
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 26 MiB.
gen_ranges ranges=300 min_key=1 max_key=10000 placement_type=replica_placement bytes_mib=26
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=5000000 min_key=1 max_key=10000
----
5.00 access-vcpus

# Write only workload, which generates little CPU and 100_000 (x replication
# factor) write bytes per second over the second half of the keyspace.
gen_ranges ranges=300 min_key=10001 max_key=20000 placement_type=replica_placement bytes_mib=26
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

gen_load rate=20000 rw_ratio=0 min_block=1000 max_block=1000 raft_cpu_per_write=1 min_key=10001 max_key=20000
----
0.00 raft-vcpus, 19 MiB/s goodput

setting split_queue_enabled=false
----

eval duration=75m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=902873183, s2=905589538, s3=906752439, s4=759908753, s5=759586191, s6=758744466] (stddev=72839187.68, mean=832242428.33, sum=4993454570)
cpu#1: thrash_pct: [s1=4%, s2=27%, s3=27%, s4=5%, s5=6%, s6=5%]  (sum=74%)
cpu_util#1: last:  [s1=0.18, s2=0.18, s3=0.18, s4=0.15, s5=0.15, s6=0.15] (stddev=0.01, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=4%, s2=27%, s3=27%, s4=5%, s5=6%, s6=5%]  (sum=74%)
leases#1: first: [s1=300, s2=0, s3=0, s4=300, s5=0, s6=0] (stddev=141.42, mean=100.00, sum=600)
leases#1: last:  [s1=56, s2=128, s3=115, s4=209, s5=46, s6=46] (stddev=58.68, mean=100.00, sum=600)
leases#1: thrash_pct: [s1=1%, s2=22%, s3=20%, s4=32%, s5=0%, s6=0%]  (sum=76%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300, s6=300] (stddev=0.00, mean=300.00, sum=1800)
replicas#1: last:  [s1=310, s2=429, s3=431, s4=209, s5=210, s6=211] (stddev=98.49, mean=300.00, sum=1800)
replicas#1: thrash_pct: [s1=54%, s2=23%, s3=21%, s4=34%, s5=34%, s6=31%]  (sum=197%)
write_bytes_per_second#1: last:  [s1=6347413, s2=10496790, s3=10498414, s4=10961624, s5=10828666, s6=10885611] (stddev=1644803.25, mean=10003086.33, sum=60018518)
write_bytes_per_second#1: thrash_pct: [s1=0%, s2=1%, s3=1%, s4=1%, s5=17%, s6=24%]  (sum=46%)
artifacts[mma-only]: 287723e9d1375d40
==========================
cpu#1: last:  [s1=807916467, s2=793350680, s3=725882458, s4=890643481, s5=891481052, s6=892857069] (stddev=63243245.47, mean=833688534.50, sum=5002131207)
cpu#1: thrash_pct: [s1=8%, s2=41%, s3=44%, s4=12%, s5=5%, s6=5%]  (sum=114%)
cpu_util#1: last:  [s1=0.16, s2=0.16, s3=0.15, s4=0.18, s5=0.18, s6=0.18] (stddev=0.01, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=8%, s2=41%, s3=44%, s4=12%, s5=5%, s6=5%]  (sum=114%)
leases#1: first: [s1=300, s2=0, s3=0, s4=300, s5=0, s6=0] (stddev=141.42, mean=100.00, sum=600)
leases#1: last:  [s1=100, s2=105, s3=113, s4=99, s5=91, s6=92] (stddev=7.53, mean=100.00, sum=600)
leases#1: thrash_pct: [s1=61%, s2=46%, s3=48%, s4=58%, s5=47%, s6=52%]  (sum=312%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300, s6=300] (stddev=0.00, mean=300.00, sum=1800)
replicas#1: last:  [s1=306, s2=313, s3=312, s4=290, s5=290, s6=289] (stddev=10.57, mean=300.00, sum=1800)
replicas#1: thrash_pct: [s1=326%, s2=293%, s3=270%, s4=300%, s5=250%, s6=273%]  (sum=1712%)
write_bytes_per_second#1: last:  [s1=9969193, s2=8119291, s3=9307798, s4=10830875, s5=10894531, s6=10896662] (stddev=1025641.64, mean=10003058.33, sum=60018350)
write_bytes_per_second#1: thrash_pct: [s1=31%, s2=18%, s3=17%, s4=70%, s5=50%, s6=46%]  (sum=234%)
artifacts[mma-count]: 7c7b6e27ef706ce8
==========================
