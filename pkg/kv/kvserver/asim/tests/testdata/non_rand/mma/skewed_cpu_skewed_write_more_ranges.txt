# This test is similar to skewed_cpu_skewed_write.txt, but with more ranges 300. 
# To have the same amount of total load, bytes per range is reduced to 26 MiB.
# 
# Expected outcome: The allocator should rebalance both cpu and write load across
# all stores, with mma achieving better results than sma. 
gen_cluster nodes=6 node_cpu_cores=5
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 26 MiB.
gen_ranges ranges=300 min_key=1 max_key=10000 placement_type=replica_placement bytes_mib=26
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=5000000 min_key=1 max_key=10000
----
5.00 access-vcpus

# Write only workload, which generates little CPU and 100_000 (x replication
# factor) write bytes per second over the second half of the keyspace.
gen_ranges ranges=300 min_key=10001 max_key=20000 placement_type=replica_placement bytes_mib=26
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

gen_load rate=20000 rw_ratio=0 min_block=1000 max_block=1000 raft_cpu_per_write=1 min_key=10001 max_key=20000
----
0.00 raft-vcpus, 19 MiB/s goodput

setting split_queue_enabled=false
----

eval duration=75m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=2729214632, s2=10497, s3=10498, s4=759755528, s5=758360833, s6=741887873] (stddev=913108002.08, mean=831539976.83, sum=4989239861)
cpu#1: thrash_pct: [s1=6%, s2=0%, s3=0%, s4=30%, s5=32%, s6=29%]  (sum=98%)
cpu_util#1: last:  [s1=0.55, s2=0.00, s3=0.00, s4=0.15, s5=0.15, s6=0.15] (stddev=0.18, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=6%, s2=0%, s3=0%, s4=30%, s5=32%, s6=29%]  (sum=98%)
leases#1: first: [s1=300, s2=0, s3=0, s4=300, s5=0, s6=0] (stddev=141.42, mean=100.00, sum=600)
leases#1: last:  [s1=167, s2=73, s3=60, s4=209, s5=46, s6=45] (stddev=64.08, mean=100.00, sum=600)
leases#1: thrash_pct: [s1=1%, s2=0%, s3=0%, s4=32%, s5=0%, s6=0%]  (sum=33%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300, s6=300] (stddev=0.00, mean=300.00, sum=1800)
replicas#1: last:  [s1=233, s2=456, s3=456, s4=209, s5=210, s6=236] (stddev=110.78, mean=300.00, sum=1800)
replicas#1: thrash_pct: [s1=50%, s2=0%, s3=0%, s4=33%, s5=35%, s6=35%]  (sum=153%)
write_bytes_per_second#1: last:  [s1=4632477, s2=10497642, s3=10498633, s4=10961880, s5=10829126, s6=12600626] (stddev=2506073.79, mean=10003397.33, sum=60020384)
write_bytes_per_second#1: thrash_pct: [s1=1%, s2=9%, s3=8%, s4=3%, s5=71%, s6=94%]  (sum=187%)
artifacts[mma-only]: 3e8fb8334143c24f
==========================
cpu#1: last:  [s1=758699996, s2=776288048, s3=743879934, s4=908590895, s5=907930194, s6=906077198] (stddev=74549663.54, mean=833577710.83, sum=5001466265)
cpu#1: thrash_pct: [s1=20%, s2=55%, s3=62%, s4=32%, s5=30%, s6=31%]  (sum=231%)
cpu_util#1: last:  [s1=0.15, s2=0.16, s3=0.15, s4=0.18, s5=0.18, s6=0.18] (stddev=0.01, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=20%, s2=55%, s3=62%, s4=32%, s5=30%, s6=31%]  (sum=231%)
leases#1: first: [s1=300, s2=0, s3=0, s4=300, s5=0, s6=0] (stddev=141.42, mean=100.00, sum=600)
leases#1: last:  [s1=101, s2=100, s3=111, s4=98, s5=98, s6=92] (stddev=5.69, mean=100.00, sum=600)
leases#1: thrash_pct: [s1=80%, s2=68%, s3=72%, s4=60%, s5=69%, s6=84%]  (sum=434%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300, s6=300] (stddev=0.00, mean=300.00, sum=1800)
replicas#1: last:  [s1=311, s2=309, s3=313, s4=288, s5=289, s6=290] (stddev=11.08, mean=300.00, sum=1800)
replicas#1: thrash_pct: [s1=423%, s2=355%, s3=384%, s4=288%, s5=286%, s6=331%]  (sum=2066%)
write_bytes_per_second#1: last:  [s1=9968412, s2=8187484, s3=9113651, s4=10830046, s5=10960447, s6=10958377] (stddev=1048943.23, mean=10003069.50, sum=60018417)
write_bytes_per_second#1: thrash_pct: [s1=93%, s2=98%, s3=99%, s4=171%, s5=194%, s6=201%]  (sum=855%)
artifacts[mma-count]: 739eb92acf446d16
==========================
