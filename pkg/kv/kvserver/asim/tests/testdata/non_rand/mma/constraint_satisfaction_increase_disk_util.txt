# This test verifies that the allocator can satisfy zone constraints when they
# are being violated even when fixing constraints would make disk utilization
# even more imbalanced. The test sets up a 7-node cluster across 3 regions
# (a, b, c) with 2, 2, and 3 nodes respectively. The span config requires 2
# replicas in region a, with lease preferences for region a. The test creates 12
# ranges with 3 replicas each with a replica on s1, s2 (region a), and equally
# balanced between s3 & s4 (region b). 3 additional ranges are created with 3
# replicas on s5, s6, and s7 (region c). Each store has 10 GiB capacity and each
# range is 500 MiB, making stores s1 and s2 around 73% full, while s3 and s4 are
# around 36% full, and s5, s6, and s7 are only 18% full.
#
# Expected outcome: The allocator should move replicas for the 3 ranges on
# region c to s1 and s2 to satisfy the zone constraints, making the stores
# about 91% full. The other replicas might be balanced between the stores in
# region b and c.
gen_cluster nodes=7 region=(a,b,c) nodes_per_region=(2,2,3) store_byte_capacity_gib=10
----

gen_ranges ranges=12 repl_factor=3 placement_type=replica_placement bytes_mib=500 min_key=0 max_key=20000
{s1,s2,s3}:1 {s1,s2,s4}:1
----
{s1:*,s2,s3}:1
{s1:*,s2,s4}:1

# Note that the number of ranges and their sizes is specifically chosen such
# that the final disk usage doesn't exceed 95% as that would trigger shedding of
# replicas even if that violates constraints (#161958)
gen_ranges ranges=3 repl_factor=3 placement_type=replica_placement bytes_mib=500 min_key=20000 max_key=30000
{s5,s6,s7}:1
----
{s5:*,s6,s7}:1

set_span_config
[0,9999999999): num_replicas=3 num_voters=3 constraints={'+region=a':2} lease_preferences=[['+region=a']]
----

setting split_queue_enabled=false
----

eval duration=10m samples=1 seed=42 cfgs=(sma-count,mma-count) metrics=(cpu,cpu_util,leases,replicas,disk_fraction_used)
----
disk_fraction_used#1: first: [s1=0.73, s2=0.73, s3=0.37, s4=0.37, s5=0.18, s6=0.18, s7=0.18] (stddev=0.23, mean=0.39, sum=3)
disk_fraction_used#1: last:  [s1=0.92, s2=0.92, s3=0.37, s4=0.37, s5=0.06, s6=0.06, s7=0.06] (stddev=0.35, mean=0.39, sum=3)
disk_fraction_used#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%, s6=0%, s7=0%]  (sum=0%)
leases#1: first: [s1=12, s2=0, s3=0, s4=0, s5=3, s6=0, s7=0] (stddev=4.16, mean=2.14, sum=15)
leases#1: last:  [s1=11, s2=4, s3=0, s4=0, s5=0, s6=0, s7=0] (stddev=3.87, mean=2.14, sum=15)
leases#1: thrash_pct: [s1=18%, s2=0%, s3=0%, s4=0%, s5=0%, s6=0%, s7=0%]  (sum=18%)
replicas#1: first: [s1=12, s2=12, s3=6, s4=6, s5=3, s6=3, s7=3] (stddev=3.74, mean=6.43, sum=45)
replicas#1: last:  [s1=15, s2=15, s3=6, s4=6, s5=1, s6=1, s7=1] (stddev=5.80, mean=6.43, sum=45)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%, s6=0%, s7=0%]  (sum=0%)
artifacts[sma-count]: b398fc0813853498
==========================
disk_fraction_used#1: first: [s1=0.73, s2=0.73, s3=0.37, s4=0.37, s5=0.18, s6=0.18, s7=0.18] (stddev=0.23, mean=0.39, sum=3)
disk_fraction_used#1: last:  [s1=0.92, s2=0.92, s3=0.37, s4=0.37, s5=0.06, s6=0.06, s7=0.06] (stddev=0.35, mean=0.39, sum=3)
disk_fraction_used#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%, s6=0%, s7=0%]  (sum=0%)
leases#1: first: [s1=12, s2=0, s3=0, s4=0, s5=3, s6=0, s7=0] (stddev=4.16, mean=2.14, sum=15)
leases#1: last:  [s1=11, s2=4, s3=0, s4=0, s5=0, s6=0, s7=0] (stddev=3.87, mean=2.14, sum=15)
leases#1: thrash_pct: [s1=18%, s2=0%, s3=0%, s4=0%, s5=0%, s6=0%, s7=0%]  (sum=18%)
replicas#1: first: [s1=12, s2=12, s3=6, s4=6, s5=3, s6=3, s7=3] (stddev=3.74, mean=6.43, sum=45)
replicas#1: last:  [s1=15, s2=15, s3=6, s4=6, s5=1, s6=1, s7=1] (stddev=5.80, mean=6.43, sum=45)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%, s6=0%, s7=0%]  (sum=0%)
artifacts[mma-count]: b398fc0813853498
==========================
