# This test verifies that for remotely cpu-overloaded stores, mma wait for lease
# shedding grace period (remoteStoreLeaseSheddingGraceDuration) before rebalancing
# replicas away from the store. The test sets up a 5-node cluster where store s1
# has a high replica count (25 out of 75 total replicas) but holds no leases. All
# leases are distributed among stores s2-s5. A write-only workload with high raft
# CPU cost creates CPU pressure primarily on s1 due to its replica count, but s1
# cannot shed leases since it holds none.
#
# Expected outcome:
# Want to test two cases:
# (1) high_cpu_unable_to_shed_leases.txt: Where its impossible to shed leases
# from the cpu overloaded s1, so we should initially observe a period of no
# rebalancing activity away from the store before
# any replica based rebalancing.
# (2) high_cpu_able_to_shed_leases.txt: Where its possible to shed leases from
# the CPU overloaded s1, so we should observe a period of lease transfers before
# any replica based rebalancing away from the store occurs.

gen_cluster nodes=5 node_cpu_rate_capacity=9000000000
----

setting split_queue_enabled=false
----

gen_ranges ranges=25 min_key=0 max_key=10000 placement_type=replica_placement
{s1,s2:*,s3}:7
{s1,s4,s5:*}:6
{s1,s2,s4:*}:6
{s1,s3:*,s5}:6
----
{s1,s2:*,s3}:7
{s1,s4,s5:*}:6
{s1,s2,s4:*}:6
{s1,s3:*,s5}:6

gen_load rate=5000 rw_ratio=0 min_key=0 max_key=10000 raft_cpu_per_write=1000000
----
5.00 raft-vcpus, 4.9 KiB/s goodput

eval duration=5m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=2594640000, s2=2997280000, s3=3203736666, s4=3000476666, s5=3203866666] (stddev=222438576.60, mean=2999999999.60, sum=14999999998)
cpu#1: thrash_pct: [s1=2%, s2=8%, s3=8%, s4=9%, s5=8%]  (sum=35%)
cpu_util#1: last:  [s1=0.29, s2=0.33, s3=0.36, s4=0.33, s5=0.36] (stddev=0.02, mean=0.33, sum=2)
cpu_util#1: thrash_pct: [s1=2%, s2=8%, s3=8%, s4=9%, s5=8%]  (sum=35%)
leases#1: first: [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: last:  [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
replicas#1: first: [s1=25, s2=13, s3=13, s4=12, s5=12] (stddev=5.02, mean=15.00, sum=75)
replicas#1: last:  [s1=13, s2=15, s3=16, s4=15, s5=16] (stddev=1.10, mean=15.00, sum=75)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
write_bytes_per_second#1: last:  [s1=2594, s2=2997, s3=3203, s4=3000, s5=3203] (stddev=222.38, mean=2999.40, sum=14997)
write_bytes_per_second#1: thrash_pct: [s1=10%, s2=8%, s3=8%, s4=8%, s5=8%]  (sum=42%)
artifacts[mma-only]: 40790b28b0714c13
==========================
cpu#1: last:  [s1=2602423333, s2=3197976666, s3=3199683333, s4=2997496666, s5=3002420000] (stddev=217783159.58, mean=2999999999.60, sum=14999999998)
cpu#1: thrash_pct: [s1=17%, s2=17%, s3=16%, s4=13%, s5=13%]  (sum=77%)
cpu_util#1: last:  [s1=0.29, s2=0.36, s3=0.36, s4=0.33, s5=0.33] (stddev=0.02, mean=0.33, sum=2)
cpu_util#1: thrash_pct: [s1=17%, s2=17%, s3=16%, s4=13%, s5=13%]  (sum=77%)
leases#1: first: [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: last:  [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
replicas#1: first: [s1=25, s2=13, s3=13, s4=12, s5=12] (stddev=5.02, mean=15.00, sum=75)
replicas#1: last:  [s1=13, s2=16, s3=16, s4=15, s5=15] (stddev=1.10, mean=15.00, sum=75)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
write_bytes_per_second#1: last:  [s1=2602, s2=3197, s3=3199, s4=2997, s5=3002] (stddev=217.64, mean=2999.40, sum=14997)
write_bytes_per_second#1: thrash_pct: [s1=17%, s2=16%, s3=15%, s4=12%, s5=14%]  (sum=74%)
artifacts[mma-count]: 7b308e57d4c5870e
==========================
