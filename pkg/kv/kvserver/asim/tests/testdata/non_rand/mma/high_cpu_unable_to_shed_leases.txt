# This test verifies that for remotely cpu-overloaded stores, mma wait for lease
# shedding grace period (remoteStoreLeaseSheddingGraceDuration) before rebalancing
# replicas away from the store. The test sets up a 5-node cluster where store s1
# has a high replica count (25 out of 75 total replicas) but holds no leases. All
# leases are distributed among stores s2-s5. A write-only workload with high raft
# CPU cost creates CPU pressure primarily on s1 due to its replica count, but s1
# cannot shed leases since it holds none.
#
# Expected outcome:
# Want to test two cases:
# (1) high_cpu_unable_to_shed_leases.txt: Where its impossible to shed leases
# from the cpu overloaded s1, so we should initially observe a period of no
# rebalancing activity away from the store before
# any replica based rebalancing.
# (2) high_cpu_able_to_shed_leases.txt: Where its possible to shed leases from
# the CPU overloaded s1, so we should observe a period of lease transfers before
# any replica based rebalancing away from the store occurs.

gen_cluster nodes=5 node_cpu_rate_capacity=9000000000
----

setting split_queue_enabled=false
----

gen_ranges ranges=25 min_key=0 max_key=10000 placement_type=replica_placement
{s1,s2:*,s3}:7
{s1,s4,s5:*}:6
{s1,s2,s4:*}:6
{s1,s3:*,s5}:6
----
{s1,s2:*,s3}:7
{s1,s4,s5:*}:6
{s1,s2,s4:*}:6
{s1,s3:*,s5}:6

gen_load rate=5000 rw_ratio=0 min_key=0 max_key=10000 raft_cpu_per_write=1000000
----
5.00 raft-vcpus, 4.9 KiB/s goodput

eval duration=5m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=2594072289, s2=2997342512, s3=3203414802, s4=3001025817, s5=3204144578] (stddev=222637827.65, mean=2999999999.60, sum=14999999998)
cpu#1: thrash_pct: [s1=1%, s2=3%, s3=3%, s4=1%, s5=1%]  (sum=9%)
cpu_util#1: last:  [s1=0.29, s2=0.33, s3=0.36, s4=0.33, s5=0.36] (stddev=0.02, mean=0.33, sum=2)
cpu_util#1: thrash_pct: [s1=1%, s2=3%, s3=3%, s4=1%, s5=1%]  (sum=9%)
leases#1: first: [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: last:  [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
replicas#1: first: [s1=25, s2=13, s3=13, s4=12, s5=12] (stddev=5.02, mean=15.00, sum=75)
replicas#1: last:  [s1=13, s2=15, s3=16, s4=15, s5=16] (stddev=1.10, mean=15.00, sum=75)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
write_bytes_per_second#1: last:  [s1=2594, s2=2997, s3=3203, s4=3001, s5=3204] (stddev=222.56, mean=2999.80, sum=14999)
write_bytes_per_second#1: thrash_pct: [s1=2%, s2=3%, s3=3%, s4=1%, s5=2%]  (sum=10%)
artifacts[mma-only]: 2dd858f5d2a8c982
==========================
cpu#1: last:  [s1=2600595524, s2=3199421686, s3=3000702237, s4=2999566265, s5=3199714285] (stddev=218714161.04, mean=2999999999.40, sum=14999999997)
cpu#1: thrash_pct: [s1=4%, s2=4%, s3=3%, s4=3%, s5=4%]  (sum=18%)
cpu_util#1: last:  [s1=0.29, s2=0.36, s3=0.33, s4=0.33, s5=0.36] (stddev=0.02, mean=0.33, sum=2)
cpu_util#1: thrash_pct: [s1=4%, s2=4%, s3=3%, s4=3%, s5=4%]  (sum=18%)
leases#1: first: [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: last:  [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
replicas#1: first: [s1=25, s2=13, s3=13, s4=12, s5=12] (stddev=5.02, mean=15.00, sum=75)
replicas#1: last:  [s1=13, s2=16, s3=15, s4=15, s5=16] (stddev=1.10, mean=15.00, sum=75)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
write_bytes_per_second#1: last:  [s1=2600, s2=3199, s3=3000, s4=2999, s5=3199] (stddev=218.72, mean=2999.40, sum=14997)
write_bytes_per_second#1: thrash_pct: [s1=4%, s2=4%, s3=2%, s4=4%, s5=4%]  (sum=17%)
artifacts[mma-count]: b3d0c2bd781cf37b
==========================
