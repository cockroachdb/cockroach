# This test verifies that the allocator does not perform count based rebalancing
# when that would go against the higher priority goals of zone constraints and
# disk fullness. The test sets up a 2-node cluster across 2 regions (a, b) with
# 1 node each. The test creates 50 ranges of size 100 MiB with a single replica
# placed on s1. The test creates 2 more ranges of size 2500 MiB each with a
# replica on s2. The span config for the second set of ranges requires them to
# be in region b, pinning them on s2. The vast difference between the size of
# the ranges and the counts, makes it so that the s1 has a much higher replica
# count (50) than s2 (2), despite having the same amount of disk utilization
# (61%).

# Expected outcome: The allocator should perform not perform (much) rebalancing.
# Despite the immense difference between the replica counts, performing
# rebalancing would violate either zone constraints, or make disk utilization
# unbalanced beyond the current threshold (10%).
gen_cluster nodes=2 region=(a,b) nodes_per_region=(1,1) store_byte_capacity_gib=10
----

gen_ranges ranges=50 repl_factor=1 placement_type=replica_placement bytes_mib=100 min_key=0 max_key=300000
{s1}:1
----
{s1:*}:1

gen_ranges ranges=2 repl_factor=1 placement_type=replica_placement bytes_mib=2500 min_key=300000 max_key=400000
{s2}:1
----
{s2:*}:1

set_span_config
[300000,400000): num_replicas=1 num_voters=1 constraints={'+region=b':1} lease_preferences=[['+region=b']]
----

setting split_queue_enabled=false
----

eval duration=5m samples=1 seed=42 cfgs=(sma-count) metrics=(leases,replicas,disk_fraction_used)
----
disk_fraction_used#1: first: [s1=0.61, s2=0.61] (stddev=0.00, mean=0.61, sum=1)
disk_fraction_used#1: last:  [s1=0.33, s2=0.89] (stddev=0.28, mean=0.61, sum=1)
disk_fraction_used#1: thrash_pct: [s1=0%, s2=0%]  (sum=0%)
leases#1: first: [s1=50, s2=3] (stddev=23.50, mean=26.50, sum=53)
leases#1: last:  [s1=27, s2=26] (stddev=0.50, mean=26.50, sum=53)
leases#1: thrash_pct: [s1=0%, s2=0%]  (sum=0%)
replicas#1: first: [s1=50, s2=3] (stddev=23.50, mean=26.50, sum=53)
replicas#1: last:  [s1=27, s2=26] (stddev=0.50, mean=26.50, sum=53)
replicas#1: thrash_pct: [s1=0%, s2=0%]  (sum=0%)
artifacts[sma-count]: 48dbff430f9a21be
==========================

assertion type=balance ticks=5 stat=disk_fraction_used upper_bound=1.1
----
asserting: max_{stores}(disk_fraction_used)/mean_{stores}(disk_fraction_used) ≤ 1.10 at each of last 5 ticks

assertion type=steady ticks=100 stat=replicas exact_bound=0
----
asserting: |replicas(t)/mean_{T}(replicas) - 1| = 0.00 ∀ t∈T and each store (T=last 100 ticks)

assertion type=conformance under=0 over=0 unavailable=0 violating=0
----

eval duration=5m samples=1 seed=42 cfgs=(mma-count) metrics=(leases,replicas,disk_fraction_used)
----
disk_fraction_used#1: first: [s1=0.61, s2=0.61] (stddev=0.00, mean=0.61, sum=1)
disk_fraction_used#1: last:  [s1=0.57, s2=0.65] (stddev=0.04, mean=0.61, sum=1)
disk_fraction_used#1: thrash_pct: [s1=0%, s2=0%]  (sum=0%)
leases#1: first: [s1=50, s2=3] (stddev=23.50, mean=26.50, sum=53)
leases#1: last:  [s1=47, s2=6] (stddev=20.50, mean=26.50, sum=53)
leases#1: thrash_pct: [s1=0%, s2=0%]  (sum=0%)
replicas#1: first: [s1=50, s2=3] (stddev=23.50, mean=26.50, sum=53)
replicas#1: last:  [s1=47, s2=6] (stddev=20.50, mean=26.50, sum=53)
replicas#1: thrash_pct: [s1=0%, s2=0%]  (sum=0%)
artifacts[mma-count]: d2a2823701a6f92a
==========================
