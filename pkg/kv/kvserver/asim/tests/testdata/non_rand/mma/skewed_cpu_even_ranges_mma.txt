# The test sets up a 9-node cluster where every store initiallyhas the same
# number of replicas.
# 
# s1 handles significant cpu load, s2,s3 handle small follower cpu load, while
# s4-s9 handle minimal cpu load and high write bandwidth.
#
# Expected outcome: The allocator should rebalance both leases and replicas to
# achieve more even cpu and write distribution across all nodes.
gen_cluster nodes=9 node_cpu_rate_capacity=5000000000
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 256 MiB.
gen_ranges ranges=36 min_key=1 max_key=10000 placement_type=replica_placement bytes_mib=256
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

# 5ms of request CPU per access and 500Âµs of raft CPU per write @ 1000/s.
gen_load rate=1000 rw_ratio=0.95 min_block=100 max_block=100 request_cpu_per_access=5000000 raft_cpu_per_write=500000 min_key=1 max_key=10000
----

# Almost empty workload, which generates no CPU and small amount of writes
# over the second half of the keyspace, scattered over s4-s9.
gen_ranges ranges=72 min_key=10001 max_key=20000 placement_type=replica_placement bytes_mib=256
{s4,s5,s6}:1
{s7,s8,s9}:1
----
{s4:*,s5,s6}:1
{s7:*,s8,s9}:1

gen_load rate=100 rw_ratio=0 min_block=128 max_block=128 min_key=10001 max_key=20000
----

setting split_queue_enabled=false
----

# TODO(tbg): it's interesting that sma-only does better on write throughput than
# mma-only. Looking at the graphs, the mma-only flavor is much slower in moving
# load around. Possibly a bug?
eval duration=22m samples=1 seed=42 cfgs=(sma-count,mma-only,mma-count) metrics=(cpu,cpu_util,leases,replicas,write_bytes_per_second)
----
cpu#1: last:  [s1=983823267, s2=1123530532, s3=703189464, s4=419443481, s5=432770664, s6=559726455, s7=141944069, s8=282935590, s9=425381126] (stddev=302430545.53, mean=563638294.22, sum=5072744648)
cpu#1: thrash_pct: [s1=13%, s2=49%, s3=46%, s4=8%, s5=9%, s6=10%, s7=4%, s8=13%, s9=24%]  (sum=176%)
cpu_util#1: last:  [s1=0.20, s2=0.22, s3=0.14, s4=0.08, s5=0.09, s6=0.11, s7=0.03, s8=0.06, s9=0.09] (stddev=0.06, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=13%, s2=49%, s3=46%, s4=8%, s5=9%, s6=10%, s7=4%, s8=13%, s9=24%]  (sum=176%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=13, s2=12, s3=12, s4=17, s5=11, s6=9, s7=15, s8=10, s9=9] (stddev=2.54, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=53%, s2=38%, s3=44%, s4=21%, s5=20%, s6=26%, s7=15%, s8=26%, s9=37%]  (sum=281%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=36, s2=37, s3=33, s4=37, s5=36, s6=36, s7=36, s8=35, s9=38] (stddev=1.33, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=433%, s2=402%, s3=672%, s4=135%, s5=117%, s6=217%, s7=117%, s8=218%, s9=403%]  (sum=2713%)
write_bytes_per_second#1: last:  [s1=5620, s2=5631, s3=5274, s4=6278, s5=6149, s6=6141, s7=6163, s8=5851, s9=6287] (stddev=336.21, mean=5932.67, sum=53394)
write_bytes_per_second#1: thrash_pct: [s1=559%, s2=643%, s3=817%, s4=499%, s5=526%, s6=579%, s7=550%, s8=547%, s9=686%]  (sum=5406%)
artifacts[sma-count]: f4911f910a2a6031
==========================
cpu#1: last:  [s1=569562730, s2=572002191, s3=578085131, s4=557945087, s5=554889308, s6=555817692, s7=569231042, s8=555128356, s9=558243133] (stddev=8275220.45, mean=563433852.22, sum=5070904670)
cpu#1: thrash_pct: [s1=9%, s2=76%, s3=61%, s4=9%, s5=22%, s6=9%, s7=10%, s8=11%, s9=17%]  (sum=225%)
cpu_util#1: last:  [s1=0.11, s2=0.11, s3=0.12, s4=0.11, s5=0.11, s6=0.11, s7=0.11, s8=0.11, s9=0.11] (stddev=0.00, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=9%, s2=76%, s3=61%, s4=9%, s5=22%, s6=9%, s7=10%, s8=11%, s9=17%]  (sum=225%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=4, s2=4, s3=4, s4=40, s5=4, s6=4, s7=40, s8=4, s9=4] (stddev=14.97, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=0%, s2=48%, s3=37%, s4=0%, s5=11%, s6=0%, s7=0%, s8=0%, s9=6%]  (sum=102%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=27, s2=28, s3=29, s4=40, s5=40, s6=40, s7=40, s8=40, s9=40] (stddev=5.68, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=30%, s6=0%, s7=0%, s8=0%, s9=16%]  (sum=46%)
write_bytes_per_second#1: last:  [s1=3751, s2=3916, s3=4019, s4=6903, s5=6911, s6=6921, s7=7004, s8=6996, s9=6985] (stddev=1443.38, mean=5934.00, sum=53406)
write_bytes_per_second#1: thrash_pct: [s1=565%, s2=570%, s3=599%, s4=134%, s5=162%, s6=134%, s7=162%, s8=159%, s9=173%]  (sum=2659%)
artifacts[mma-only]: 8b414911c677f8f9
==========================
cpu#1: last:  [s1=564321656, s2=570703549, s3=567256633, s4=561501935, s5=560527044, s6=555833144, s7=574286961, s8=558080746, s9=557632305] (stddev=5946449.12, mean=563349330.33, sum=5070143973)
cpu#1: thrash_pct: [s1=67%, s2=123%, s3=155%, s4=40%, s5=102%, s6=49%, s7=72%, s8=26%, s9=56%]  (sum=689%)
cpu_util#1: last:  [s1=0.11, s2=0.11, s3=0.11, s4=0.11, s5=0.11, s6=0.11, s7=0.11, s8=0.11, s9=0.11] (stddev=0.00, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=67%, s2=123%, s3=155%, s4=40%, s5=102%, s6=49%, s7=72%, s8=26%, s9=56%]  (sum=689%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=8, s2=7, s3=7, s4=16, s5=14, s6=11, s7=15, s8=13, s9=17] (stddev=3.68, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=72%, s2=94%, s3=123%, s4=94%, s5=184%, s6=108%, s7=106%, s8=74%, s9=127%]  (sum=981%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=35, s2=36, s3=34, s4=38, s5=33, s6=36, s7=38, s8=38, s9=36] (stddev=1.70, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=348%, s2=213%, s3=216%, s4=589%, s5=1204%, s6=840%, s7=643%, s8=643%, s9=987%]  (sum=5682%)
write_bytes_per_second#1: last:  [s1=5514, s2=5506, s3=5089, s4=6396, s5=5508, s6=6117, s7=6365, s8=6580, s9=6210] (stddev=492.12, mean=5920.56, sum=53285)
write_bytes_per_second#1: thrash_pct: [s1=1214%, s2=1091%, s3=1145%, s4=1119%, s5=1628%, s6=1261%, s7=1349%, s8=1334%, s9=1459%]  (sum=11600%)
artifacts[mma-count]: 6790eb7f413fc1f4
==========================
