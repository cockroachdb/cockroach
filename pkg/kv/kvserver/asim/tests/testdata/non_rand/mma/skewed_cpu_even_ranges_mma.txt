# The test sets up a 9-node cluster where every store initiallyhas the same
# number of replicas.
# 
# s1 handles significant cpu load, s2,s3 handle small follower cpu load, while
# s4-s9 handle minimal cpu load and high write bandwidth.
#
# Expected outcome: The allocator should rebalance both leases and replicas to
# achieve more even cpu and write distribution across all nodes.
gen_cluster nodes=9 node_cpu_rate_capacity=5000000000
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 256 MiB.
gen_ranges ranges=36 min_key=1 max_key=10000 placement_type=replica_placement bytes_mib=256
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

# 5ms of request CPU per access and 500Âµs of raft CPU per write @ 1000/s.
gen_load rate=1000 rw_ratio=0.95 min_block=100 max_block=100 request_cpu_per_access=5000000 raft_cpu_per_write=500000 min_key=1 max_key=10000
----
5.00 access-vcpus, 0.03 raft-vcpus, 4.9 KiB/s goodput

# Almost empty workload, which generates no CPU and small amount of writes
# over the second half of the keyspace, scattered over s4-s9.
gen_ranges ranges=72 min_key=10001 max_key=20000 placement_type=replica_placement bytes_mib=256
{s4,s5,s6}:1
{s7,s8,s9}:1
----
{s4:*,s5,s6}:1
{s7:*,s8,s9}:1

# TODO(tbg): this is barely anything, is this intentional?
gen_load rate=100 rw_ratio=0 min_block=128 max_block=128 min_key=10001 max_key=20000
----
12 KiB/s goodput

setting split_queue_enabled=false
----

# TODO(tbg): it's interesting that sma-only does better on write throughput than
# mma-only. Looking at the graphs, the mma-only flavor is much slower in moving
# load around. Possibly a bug?
eval duration=22m samples=1 seed=42 cfgs=(sma-count,mma-only,mma-count) metrics=(cpu,cpu_util,leases,replicas,write_bytes_per_second)
----
cpu#1: last:  [s1=1122741141, s2=986769657, s3=1006211826, s4=281944758, s5=278676257, s6=419573860, s7=282370241, s8=279268541, s9=416736921] (stddev=341625894.80, mean=563810355.78, sum=5074293202)
cpu#1: thrash_pct: [s1=4%, s2=36%, s3=27%, s4=2%, s5=2%, s6=10%, s7=2%, s8=1%, s9=10%]  (sum=94%)
cpu_util#1: last:  [s1=0.22, s2=0.20, s3=0.20, s4=0.06, s5=0.06, s6=0.08, s7=0.06, s8=0.06, s9=0.08] (stddev=0.07, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=4%, s2=36%, s3=27%, s4=2%, s5=2%, s6=10%, s7=2%, s8=1%, s9=10%]  (sum=94%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=12, s2=11, s3=7, s4=17, s5=12, s6=10, s7=17, s8=11, s9=11] (stddev=3.02, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=31%, s2=28%, s3=20%, s4=9%, s5=8%, s6=8%, s7=16%, s8=8%, s9=15%]  (sum=142%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=34, s2=35, s3=37, s4=37, s5=34, s6=36, s7=37, s8=36, s9=38] (stddev=1.33, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=203%, s2=202%, s3=18%, s4=68%, s5=53%, s6=83%, s7=51%, s8=50%, s9=86%]  (sum=815%)
write_bytes_per_second#1: last:  [s1=5202, s2=5243, s3=5203, s4=6295, s5=5900, s6=6208, s7=6458, s8=6293, s9=6608] (stddev=538.72, mean=5934.44, sum=53410)
write_bytes_per_second#1: thrash_pct: [s1=137%, s2=147%, s3=60%, s4=97%, s5=107%, s6=111%, s7=90%, s8=72%, s9=106%]  (sum=927%)
artifacts[sma-count]: 621561332fad3632
==========================
cpu#1: last:  [s1=569513826, s2=572298795, s3=577759789, s4=558016031, s5=554689473, s6=555920651, s7=569728945, s8=554949123, s9=557987028] (stddev=8328915.05, mean=563429295.67, sum=5070863661)
cpu#1: thrash_pct: [s1=3%, s2=46%, s3=39%, s4=1%, s5=14%, s6=2%, s7=3%, s8=3%, s9=9%]  (sum=119%)
cpu_util#1: last:  [s1=0.11, s2=0.11, s3=0.12, s4=0.11, s5=0.11, s6=0.11, s7=0.11, s8=0.11, s9=0.11] (stddev=0.00, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=3%, s2=46%, s3=39%, s4=1%, s5=14%, s6=2%, s7=3%, s8=3%, s9=9%]  (sum=119%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=4, s2=4, s3=4, s4=40, s5=4, s6=4, s7=40, s8=4, s9=4] (stddev=14.97, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=0%, s2=48%, s3=37%, s4=0%, s5=11%, s6=0%, s7=0%, s8=0%, s9=6%]  (sum=102%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=27, s2=28, s3=29, s4=40, s5=40, s6=40, s7=40, s8=40, s9=40] (stddev=5.68, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=30%, s6=0%, s7=0%, s8=0%, s9=16%]  (sum=46%)
write_bytes_per_second#1: last:  [s1=3751, s2=3920, s3=4020, s4=6906, s5=6914, s6=6922, s7=7008, s8=7000, s9=6985] (stddev=1443.79, mean=5936.22, sum=53426)
write_bytes_per_second#1: thrash_pct: [s1=210%, s2=230%, s3=248%, s4=21%, s5=41%, s6=23%, s7=51%, s8=50%, s9=60%]  (sum=934%)
artifacts[mma-only]: d0a52183d0b4ff28
==========================
cpu#1: last:  [s1=712014782, s2=709206180, s3=559998176, s4=419377055, s5=557415866, s6=562702169, s7=420556007, s8=566804070, s9=560022722] (stddev=96932380.45, mean=563121891.89, sum=5068097027)
cpu#1: thrash_pct: [s1=103%, s2=138%, s3=82%, s4=35%, s5=31%, s6=42%, s7=59%, s8=73%, s9=49%]  (sum=611%)
cpu_util#1: last:  [s1=0.14, s2=0.14, s3=0.11, s4=0.08, s5=0.11, s6=0.11, s7=0.08, s8=0.11, s9=0.11] (stddev=0.02, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=103%, s2=138%, s3=82%, s4=35%, s5=31%, s6=42%, s7=59%, s8=73%, s9=49%]  (sum=611%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=9, s2=6, s3=10, s4=16, s5=12, s6=11, s7=15, s8=17, s9=12] (stddev=3.33, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=119%, s2=128%, s3=72%, s4=76%, s5=60%, s6=101%, s7=106%, s8=120%, s9=90%]  (sum=871%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=36, s2=35, s3=36, s4=34, s5=35, s6=35, s7=38, s8=38, s9=37] (stddev=1.33, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=400%, s2=87%, s3=357%, s4=417%, s5=359%, s6=644%, s7=246%, s8=774%, s9=559%]  (sum=3842%)
write_bytes_per_second#1: last:  [s1=5684, s2=4971, s3=5872, s4=5876, s5=5899, s6=5722, s7=6502, s8=6420, s9=6250] (stddev=434.23, mean=5910.67, sum=53196)
write_bytes_per_second#1: thrash_pct: [s1=529%, s2=457%, s3=441%, s4=389%, s5=382%, s6=594%, s7=308%, s8=716%, s9=541%]  (sum=4356%)
artifacts[mma-count]: ef9784853f64f7a3
==========================
