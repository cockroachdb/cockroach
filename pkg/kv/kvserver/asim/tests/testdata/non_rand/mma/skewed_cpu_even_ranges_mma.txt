# The test sets up a 9-node cluster where every store initiallyhas the same
# number of replicas.
# 
# s1 handles significant cpu load, s2,s3 handle small follower cpu load, while
# s4-s9 handle minimal cpu load and high write bandwidth.
#
# Expected outcome: The allocator should rebalance both leases and replicas to
# achieve more even cpu and write distribution across all nodes.
gen_cluster nodes=9 node_cpu_cores=5
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 256 MiB.
gen_ranges ranges=36 min_key=1 max_key=10000 placement_type=replica_placement bytes_mib=256
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

# 5ms of request CPU per access and 500Âµs of raft CPU per write @ 1000/s.
gen_load rate=1000 rw_ratio=0.95 min_block=100 max_block=100 request_cpu_per_access=5000000 raft_cpu_per_write=500000 min_key=1 max_key=10000
----
5.00 access-vcpus, 0.03 raft-vcpus, 4.9 KiB/s goodput

# Almost empty workload, which generates no CPU and small amount of writes
# over the second half of the keyspace, scattered over s4-s9.
gen_ranges ranges=72 min_key=10001 max_key=20000 placement_type=replica_placement bytes_mib=256
{s4,s5,s6}:1
{s7,s8,s9}:1
----
{s4:*,s5,s6}:1
{s7:*,s8,s9}:1

# TODO(tbg): this is barely anything, is this intentional?
gen_load rate=100 rw_ratio=0 min_block=128 max_block=128 min_key=10001 max_key=20000
----
12 KiB/s goodput

setting split_queue_enabled=false
----

# TODO(tbg): it's interesting that sma-only does better on write throughput than
# mma-only. Looking at the graphs, the mma-only flavor is much slower in moving
# load around. Possibly a bug?
eval duration=22m samples=1 seed=42 cfgs=(sma-count,mma-only,mma-count) metrics=(cpu,cpu_util,leases,replicas,write_bytes_per_second)
----
cpu#1: last:  [s1=563639640, s2=579095747, s3=561019502, s4=561591114, s5=560289458, s6=563986913, s7=561058911, s8=559174716, s9=561248205] (stddev=5708749.39, mean=563456022.89, sum=5071104206)
cpu#1: thrash_pct: [s1=61%, s2=93%, s3=85%, s4=33%, s5=25%, s6=31%, s7=26%, s8=31%, s9=38%]  (sum=423%)
cpu_util#1: last:  [s1=0.11, s2=0.12, s3=0.11, s4=0.11, s5=0.11, s6=0.11, s7=0.11, s8=0.11, s9=0.11] (stddev=0.00, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=61%, s2=93%, s3=85%, s4=33%, s5=25%, s6=31%, s7=26%, s8=31%, s9=38%]  (sum=423%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=8, s2=12, s3=10, s4=17, s5=11, s6=15, s7=15, s8=12, s9=8] (stddev=2.98, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=90%, s2=91%, s3=78%, s4=64%, s5=44%, s6=33%, s7=40%, s8=38%, s9=66%]  (sum=544%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=36, s2=38, s3=34, s4=36, s5=34, s6=36, s7=36, s8=38, s9=36] (stddev=1.33, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=814%, s2=660%, s3=474%, s4=343%, s5=288%, s6=271%, s7=229%, s8=146%, s9=486%]  (sum=3711%)
write_bytes_per_second#1: last:  [s1=5732, s2=6066, s3=5520, s4=6053, s5=5656, s6=5984, s7=6012, s8=6348, s9=5994] (stddev=236.75, mean=5929.44, sum=53365)
write_bytes_per_second#1: thrash_pct: [s1=1071%, s2=1051%, s3=901%, s4=774%, s5=695%, s6=717%, s7=684%, s8=692%, s9=916%]  (sum=7501%)
artifacts[sma-count]: 9f6f3c953198a939
==========================
cpu#1: last:  [s1=830405303, s2=24973243, s3=24973243, s4=697007199, s5=695419793, s6=698908890, s7=711425342, s8=695654299, s9=695205357] (stddev=290882839.25, mean=563774741.00, sum=5073972669)
cpu#1: thrash_pct: [s1=8%, s2=2%, s3=2%, s4=18%, s5=12%, s6=13%, s7=12%, s8=19%, s9=12%]  (sum=98%)
cpu_util#1: last:  [s1=0.17, s2=0.00, s3=0.00, s4=0.14, s5=0.14, s6=0.14, s7=0.14, s8=0.14, s9=0.14] (stddev=0.06, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=8%, s2=2%, s3=2%, s4=18%, s5=12%, s6=13%, s7=12%, s8=19%, s9=12%]  (sum=98%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=6, s2=0, s3=0, s4=41, s5=5, s6=5, s7=41, s8=5, s9=5] (stddev=15.64, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=6%, s5=0%, s6=0%, s7=0%, s8=6%, s9=0%]  (sum=12%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=6, s2=36, s3=36, s4=41, s5=41, s6=41, s7=41, s8=41, s9=41] (stddev=10.80, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=7%, s5=0%, s6=0%, s7=0%, s8=7%, s9=0%]  (sum=14%)
write_bytes_per_second#1: last:  [s1=834, s2=4994, s3=4994, s4=7043, s5=7065, s6=7061, s7=7137, s8=7127, s9=7125] (stddev=1995.70, mean=5931.11, sum=53380)
write_bytes_per_second#1: thrash_pct: [s1=22%, s2=247%, s3=247%, s4=93%, s5=86%, s6=82%, s7=91%, s8=95%, s9=92%]  (sum=1056%)
artifacts[mma-only]: 4256f7169654d976
==========================
cpu#1: last:  [s1=283411502, s2=571643002, s3=561988695, s4=1103313406, s5=572156420, s6=427374035, s7=560621820, s8=413882624, s9=562060463] (stddev=213669641.04, mean=561827996.33, sum=5056451967)
cpu#1: thrash_pct: [s1=82%, s2=146%, s3=122%, s4=67%, s5=84%, s6=76%, s7=71%, s8=71%, s9=115%]  (sum=834%)
cpu_util#1: last:  [s1=0.06, s2=0.11, s3=0.11, s4=0.22, s5=0.11, s6=0.09, s7=0.11, s8=0.08, s9=0.11] (stddev=0.04, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=82%, s2=146%, s3=122%, s4=67%, s5=84%, s6=76%, s7=71%, s8=71%, s9=115%]  (sum=834%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=10, s2=7, s3=12, s4=17, s5=11, s6=10, s7=12, s8=13, s9=16] (stddev=2.91, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=125%, s2=117%, s3=137%, s4=174%, s5=228%, s6=147%, s7=165%, s8=143%, s9=219%]  (sum=1455%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=35, s2=35, s3=36, s4=40, s5=38, s6=36, s7=35, s8=31, s9=38] (stddev=2.40, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=506%, s2=233%, s3=958%, s4=1194%, s5=1549%, s6=726%, s7=980%, s8=826%, s9=1539%]  (sum=8511%)
write_bytes_per_second#1: last:  [s1=5897, s2=5554, s3=6150, s4=6460, s5=6226, s6=5985, s7=5812, s8=5116, s9=6365] (stddev=397.76, mean=5951.67, sum=53565)
write_bytes_per_second#1: thrash_pct: [s1=1282%, s2=1140%, s3=1556%, s4=1726%, s5=1997%, s6=1433%, s7=1601%, s8=1548%, s9=1993%]  (sum=14276%)
artifacts[mma-count]: 8bee1cf51712942a
==========================
