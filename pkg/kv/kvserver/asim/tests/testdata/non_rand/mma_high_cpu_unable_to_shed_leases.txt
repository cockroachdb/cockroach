skip_under_ci
----

# Want to test two cases:
# (1) Where its impossible to shed leases from the CPU overloaded store, so we
#     should initially observe a period of no rebalancing activity away from
#     the store.
# (2) Where its possible to shed leases from the CPU overloaded store, so we
#     should observe a period of lease transfers before any replica based
#     rebalancing away from the store occurs.
gen_cluster nodes=5 node_cpu_rate_capacity=9000000000
----

setting split_queue_enabled=false
----

# Case (1) where s1 has no leases and is CPU overloaded due to raft CPU. It
# won't be able to shed its own replicas because it is not the leaseholder for
# any of the ranges.

# Originally, this test uses replica_weights=(0.3,0.175,0.175,0.175,0.175)
# lease_weights=(0,0.25,0.25,0.25,0.25). Replication factor is 3 by default. 75
# replicas in total. replicas distribution is approximately s1: 23, s2: 13, s3:
# 13, s4: 13, s5: 13 leaseholder weights: s2: 7 leaseholder, s3: 6 leaseholder,
# s4: 6 leaseholder, s5: 6 leaseholder. To approximate this, we use replica
# placement: As an approximation, (s1,s2*,s3):7, (s1,s4,s5*):6, (s1,s2,s4*):6,
# (s1,s3*,s5):6 s1 does not have the lease. Other stores have the same
gen_ranges ranges=25 min_key=0 max_key=10000 placement_type=replica_placement
{s1,s2:*,s3}:7
{s1,s4,s5:*}:6
{s1,s2,s4:*}:6
{s1,s3:*,s5}:6
----
{s1,s2:*,s3}:7
{s1,s4,s5:*}:6
{s1,s2,s4:*}:6
{s1,s3:*,s5}:6

gen_load rate=5000 rw_ratio=0 min_key=0 max_key=10000 raft_cpu_per_write=1000000
----

eval duration=30m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=2600894000, s2=3001220000, s3=3199993333, s4=3002060000, s5=3200832666] (stddev=218914332.20, mean=3000999999.80, sum=15004999999)
cpu#1: thrash_pct: [s1=6%, s2=11%, s3=11%, s4=12%, s5=11%]  (sum=51%)
cpu_util#1: last:  [s1=0.29, s2=0.33, s3=0.36, s4=0.33, s5=0.36] (stddev=0.02, mean=0.33, sum=2)
cpu_util#1: thrash_pct: [s1=6%, s2=11%, s3=11%, s4=12%, s5=11%]  (sum=51%)
leases#1: first: [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: last:  [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
replicas#1: first: [s1=25, s2=13, s3=13, s4=12, s5=12] (stddev=5.02, mean=15.00, sum=75)
replicas#1: last:  [s1=13, s2=15, s3=16, s4=15, s5=16] (stddev=1.10, mean=15.00, sum=75)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
write_bytes_per_second#1: last:  [s1=2600, s2=3001, s3=3199, s4=3002, s5=3200] (stddev=218.91, mean=3000.40, sum=15002)
write_bytes_per_second#1: thrash_pct: [s1=14%, s2=10%, s3=11%, s4=13%, s5=12%]  (sum=60%)
artifacts[mma-only]: 3334492f2bf1a359
==========================
cpu#1: last:  [s1=2600261333, s2=3200356666, s3=3201182666, s4=3001334000, s5=3001865333] (stddev=219275406.57, mean=3000999999.60, sum=15004999998)
cpu#1: thrash_pct: [s1=25%, s2=21%, s3=24%, s4=21%, s5=17%]  (sum=107%)
cpu_util#1: last:  [s1=0.29, s2=0.36, s3=0.36, s4=0.33, s5=0.33] (stddev=0.02, mean=0.33, sum=2)
cpu_util#1: thrash_pct: [s1=25%, s2=21%, s3=24%, s4=21%, s5=17%]  (sum=107%)
leases#1: first: [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: last:  [s1=0, s2=7, s3=6, s4=6, s5=6] (stddev=2.53, mean=5.00, sum=25)
leases#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
replicas#1: first: [s1=25, s2=13, s3=13, s4=12, s5=12] (stddev=5.02, mean=15.00, sum=75)
replicas#1: last:  [s1=13, s2=16, s3=16, s4=15, s5=15] (stddev=1.10, mean=15.00, sum=75)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=0%]  (sum=0%)
write_bytes_per_second#1: last:  [s1=2600, s2=3200, s3=3201, s4=3001, s5=3001] (stddev=219.27, mean=3000.60, sum=15003)
write_bytes_per_second#1: thrash_pct: [s1=26%, s2=21%, s3=28%, s4=22%, s5=15%]  (sum=113%)
artifacts[mma-count]: ee0f38e1cccdb06b
==========================

# TODO(kvoli): Case (2)
