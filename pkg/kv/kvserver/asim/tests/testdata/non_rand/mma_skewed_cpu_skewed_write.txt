skip_under_ci
----

# This test can now roughly equalize both cpu and write bandwidth. It didn't
# use to be able to do this, because the highest cpu node had the lowest write
# bandwidth and vice versa, so neither was able to shed to the other. The
# ignoreLevel logic in rebalanceStores with the grace duration to start
# shedding more aggressively and other related changes have made this much
# better.

gen_cluster nodes=6 node_cpu_rate_capacity=5000000000
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 256 MiB.
gen_ranges ranges=36 min_key=1 max_key=10000 placement_type=replica_placement bytes=268435456
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=5000000 min_key=1 max_key=10000
----

# Write only workload, which generates little CPU and 100_000 (x replication
# factor) write bytes per second over the second half of the keyspace.
gen_ranges ranges=36 min_key=10001 max_key=20000 placement_type=replica_placement bytes=268435456
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

gen_load rate=20000 rw_ratio=0 min_block=1000 max_block=1000 raft_cpu_per_write=1 min_key=10001 max_key=20000
----

setting split_queue_enabled=false
----

eval duration=60m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases) full=true
----
cpu#1: last:  [s1=832615034, s2=830099242, s3=833159174, s4=830633175, s5=843839094, s6=831645833] (stddev=4670012.58, mean=833665258.67, sum=5001991552)
cpu#1: thrash_pct: [s1=9%, s2=38%, s3=55%, s4=24%, s5=17%, s6=16%]  (sum=160%)
cpu_util#1: last:  [s1=0.17, s2=0.17, s3=0.17, s4=0.17, s5=0.17, s6=0.17] (stddev=0.00, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=9%, s2=38%, s3=55%, s4=24%, s5=17%, s6=16%]  (sum=160%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0] (stddev=16.97, mean=12.00, sum=72)
leases#1: last:  [s1=6, s2=13, s3=16, s4=25, s5=6, s6=6] (stddev=7.00, mean=12.00, sum=72)
leases#1: thrash_pct: [s1=0%, s2=20%, s3=33%, s4=44%, s5=0%, s6=0%]  (sum=98%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36] (stddev=0.00, mean=36.00, sum=216)
replicas#1: last:  [s1=41, s2=52, s3=50, s4=25, s5=25, s6=23] (stddev=12.17, mean=36.00, sum=216)
replicas#1: thrash_pct: [s1=56%, s2=20%, s3=31%, s4=42%, s5=47%, s6=37%]  (sum=233%)
write_bytes_per_second#1: last:  [s1=8367588, s2=10584074, s3=10581703, s4=10533730, s5=10533670, s6=9420617] (stddev=841582.79, mean=10003563.67, sum=60021382)
write_bytes_per_second#1: thrash_pct: [s1=2%, s2=6%, s3=7%, s4=3%, s5=115%, s6=111%]  (sum=244%)
artifacts[mma-only]: f7d623afc5d4b914
==========================
cpu#1: last:  [s1=830645684, s2=831667817, s3=831618892, s4=844303252, s5=831510232, s6=831594288] (stddev=4818790.74, mean=833556694.17, sum=5001340165)
cpu#1: thrash_pct: [s1=26%, s2=55%, s3=60%, s4=16%, s5=21%, s6=30%]  (sum=207%)
cpu_util#1: last:  [s1=0.17, s2=0.17, s3=0.17, s4=0.17, s5=0.17, s6=0.17] (stddev=0.00, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=26%, s2=55%, s3=60%, s4=16%, s5=21%, s6=30%]  (sum=207%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0] (stddev=16.97, mean=12.00, sum=72)
leases#1: last:  [s1=13, s2=12, s3=12, s4=16, s5=7, s6=12] (stddev=2.65, mean=12.00, sum=72)
leases#1: thrash_pct: [s1=182%, s2=171%, s3=383%, s4=273%, s5=249%, s6=286%]  (sum=1543%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36] (stddev=0.00, mean=36.00, sum=216)
replicas#1: last:  [s1=42, s2=44, s3=46, s4=24, s5=29, s6=31] (stddev=8.35, mean=36.00, sum=216)
replicas#1: thrash_pct: [s1=554%, s2=584%, s3=878%, s4=794%, s5=698%, s6=689%]  (sum=4197%)
write_bytes_per_second#1: last:  [s1=8868115, s2=9477005, s3=8309304, s4=8366830, s5=11627990, s6=13357155] (stddev=1870882.99, mean=10001066.50, sum=60006399)
write_bytes_per_second#1: thrash_pct: [s1=694%, s2=798%, s3=817%, s4=1285%, s5=1290%, s6=1222%]  (sum=6106%)
artifacts[mma-count]: 6a42f6a0fade4baa
==========================
Cluster Set Up
	n1(AU_EAST,AU_EAST_1,5vcpu): {s1:(256GiB)}
	n2(AU_EAST,AU_EAST_1,5vcpu): {s2:(256GiB)}
	n3(AU_EAST,AU_EAST_1,5vcpu): {s3:(256GiB)}
	n4(AU_EAST,AU_EAST_1,5vcpu): {s4:(256GiB)}
	n5(AU_EAST,AU_EAST_1,5vcpu): {s5:(256GiB)}
	n6(AU_EAST,AU_EAST_1,5vcpu): {s6:(256GiB)}
Key Space
	[1,10000): 36(rf=3), 256MiB, [{s1*,s2,s3}:36]
	[10001,20000): 36(rf=3), 256MiB, [{s4*,s5,s6}:36]
Event
	set LBRebalancingMode to 3
Workload Set Up
	[1,10000): read-only high-cpu [5000.00cpu-us/op, 1B/op, 1000ops/s]
	[10001,20000): write-only large-block [0.00cpu-us/write(raft), 1000B/op, 20000ops/s]
Changed Settings
	SplitQueueEnabled: false (default: true)
==========================
