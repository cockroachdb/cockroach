skip_under_ci
----

# This test can now roughly equalize both cpu and write bandwidth. It didn't
# use to be able to do this, because the highest cpu node had the lowest write
# bandwidth and vice versa, so neither was able to shed to the other. The
# ignoreLevel logic in rebalanceStores with the grace duration to start
# shedding more aggressively and other related changes have made this much
# better.

gen_cluster nodes=6 node_cpu_rate_capacity=5000000000
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 26 MiB.
gen_ranges ranges=300 min_key=1 max_key=10000 placement_type=replica_placement bytes=26843545
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

gen_load rate=1000 rw_ratio=1.0 request_cpu_per_access=5000000 min_key=1 max_key=10000
----

# Write only workload, which generates little CPU and 100_000 (x replication
# factor) write bytes per second over the second half of the keyspace.
gen_ranges ranges=300 min_key=10001 max_key=20000 placement_type=replica_placement bytes=26843545
{s4,s5,s6}:1
----
{s4:*,s5,s6}:1

gen_load rate=20000 rw_ratio=0 min_block=1000 max_block=1000 raft_cpu_per_write=1 min_key=10001 max_key=20000
----

setting split_queue_enabled=false
----

eval duration=90m samples=1 seed=42 cfgs=(mma-only,mma-count) metrics=(cpu,cpu_util,write_bytes_per_second,replicas,leases)
----
cpu#1: last:  [s1=906849605, s2=909718506, s3=906276510, s4=759654953, s5=759918246, s6=759053366] (stddev=74044440.95, mean=833578531.00, sum=5001471186)
cpu#1: thrash_pct: [s1=11%, s2=56%, s3=54%, s4=27%, s5=28%, s6=27%]  (sum=203%)
cpu_util#1: last:  [s1=0.18, s2=0.18, s3=0.18, s4=0.15, s5=0.15, s6=0.15] (stddev=0.01, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=11%, s2=56%, s3=54%, s4=27%, s5=28%, s6=27%]  (sum=203%)
leases#1: first: [s1=300, s2=0, s3=0, s4=300, s5=0, s6=0] (stddev=141.42, mean=100.00, sum=600)
leases#1: last:  [s1=55, s2=129, s3=115, s4=212, s5=43, s6=46] (stddev=60.28, mean=100.00, sum=600)
leases#1: thrash_pct: [s1=0%, s2=23%, s3=19%, s4=34%, s5=0%, s6=0%]  (sum=77%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300, s6=300] (stddev=0.00, mean=300.00, sum=1800)
replicas#1: last:  [s1=310, s2=427, s3=432, s4=212, s5=208, s6=211] (stddev=98.13, mean=300.00, sum=1800)
replicas#1: thrash_pct: [s1=58%, s2=24%, s3=21%, s4=36%, s5=34%, s6=36%]  (sum=209%)
write_bytes_per_second#1: last:  [s1=6272062, s2=10497859, s3=10497096, s4=10961474, s5=10895186, s6=10895652] (stddev=1679305.92, mean=10003221.50, sum=60019329)
write_bytes_per_second#1: thrash_pct: [s1=2%, s2=9%, s3=10%, s4=3%, s5=71%, s6=95%]  (sum=189%)
artifacts[mma-only]: 63d26a1cb7a5914
==========================
cpu#1: last:  [s1=822531844, s2=645020962, s3=827310776, s4=906368721, s5=907524778, s6=892325384] (stddev=91225816.55, mean=833513744.17, sum=5001082465)
cpu#1: thrash_pct: [s1=23%, s2=63%, s3=75%, s4=38%, s5=32%, s6=34%]  (sum=265%)
cpu_util#1: last:  [s1=0.16, s2=0.13, s3=0.17, s4=0.18, s5=0.18, s6=0.18] (stddev=0.02, mean=0.17, sum=1)
cpu_util#1: thrash_pct: [s1=23%, s2=63%, s3=75%, s4=38%, s5=32%, s6=34%]  (sum=265%)
leases#1: first: [s1=300, s2=0, s3=0, s4=300, s5=0, s6=0] (stddev=141.42, mean=100.00, sum=600)
leases#1: last:  [s1=104, s2=100, s3=97, s4=99, s5=102, s6=98] (stddev=2.38, mean=100.00, sum=600)
leases#1: thrash_pct: [s1=88%, s2=88%, s3=104%, s4=87%, s5=88%, s6=95%]  (sum=550%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300, s6=300] (stddev=0.00, mean=300.00, sum=1800)
replicas#1: last:  [s1=311, s2=303, s3=302, s4=301, s5=298, s6=285] (stddev=7.79, mean=300.00, sum=1800)
replicas#1: thrash_pct: [s1=662%, s2=699%, s3=831%, s4=686%, s5=558%, s6=500%]  (sum=3936%)
write_bytes_per_second#1: last:  [s1=8452722, s2=9507616, s3=9771657, s4=10367369, s5=10958667, s6=10958886] (stddev=881482.54, mean=10002819.50, sum=60016917)
write_bytes_per_second#1: thrash_pct: [s1=114%, s2=115%, s3=147%, s4=237%, s5=244%, s6=252%]  (sum=1108%)
artifacts[mma-count]: a54438ae6d1f91e8
==========================
