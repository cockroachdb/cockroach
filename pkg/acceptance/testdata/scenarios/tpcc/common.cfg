# The cluster role controls the entire cluster.
role cluster
  # at start and at end, we kill all pending commands and reset the log file(s).
  cleanup export HOME=$ROACHPROD_HOME/..; \\
          $ROACHPROD stop $CLUSTER --tag run --wait || true; \\
		  get-logs.sh logs || true
  # Lifecycle for every test: resetlogs (implies quit), start.
  # The start action actually cycles the cluster once, to
  # avoid stalls/delays due to stale liveness records in gossip.
  # We split these operations as separate actions so that their
  # execution time appears clearly in the final plots.
  :resetlogs export HOME=$ROACHPROD_HOME/.. TMPDIR=/tmp; reset-logs.sh
  :start   export HOME=$ROACHPROD_HOME/.. TMPDIR=/tmp; \\
		   start-crdb.sh; \\
		   quit-crdb.sh; \\
		   start-crdb.sh
  # ready checks that a client can perform a simple query.
  :ready   export HOME=$ROACHPROD_HOME/.. TMPDIR=/tmp; \\
           $ROACHPROD sql $CLUSTER:$CLUSTER_MAPPING -- -e "show databases"
end
cast
  crdb plays cluster
end
script
  scene B entails for crdb: resetlogs
  scene s entails for crdb: start
  scene y entails for crdb: ready
  scene l mood starts green
  scene r mood ends clear
  storyline Bls ...yr
end
attention the green mood is the period where node are started

# The node role exists to monitor the individual nodes' log files.
# (not to start/stop the cluster. This is done by the "cluster" role.)
role node
  cleanup   test -e start_spotlight || mkfifo start_spotlight
  :start    echo start>start_spotlight
  spotlight cat start_spotlight; \\
            eval "nodes=($CLUSTER_NODES)"; \\
            HOME=$ROACHPROD_HOME/.. \\
			$ROACHPROD run $CLUSTER:${nodes[$i]} --tag run -- \\
			"tail -F logs/cockroach.log | stdbuf -oL grep -v 'failed to connect'"
  signal   warning event at ^W(?P<ts_log>)\s+\d+\s+\S+\s+(?P<event>.*)$
  signal   error event at  ^E(?P<ts_log>)\s+\d+\s+\S+\s+(?P<event>.*)$
  signal   fatal event at  ^F(?P<ts_log>)\s+\d+\s+\S+\s+(?P<event>.*)$
end

# cluster_size should be set from roachenv's $CLUSTER_SIZE.
parameter cluster_size defaults to 0
title ~cluster_size~ nodes
cast
  n* play ~cluster_size~ nodes
end
script
  scene l entails for every node: start
end
# this audience generates an event plot for the log files.
audience
  server_events    watches every node warning
  server_events    watches every node error
  server_events    watches every node fatal
  server_events    measures occurrences (y value is source index)
end

# The worker role is responsible for starting the TPC-C workload
# and monitoring the "external" tp/lat metrics.
role worker
  cleanup   test -e start_spotlight || mkfifo start_spotlight
  :start    echo start>start_spotlight; cat start_spotlight
  spotlight cat start_spotlight; echo done>start_spotlight; \\
            HOME=$ROACHPROD_HOME/.. \\
			start-tpcc.sh $workload_size --display-format=incremental-json --tolerate-errors
  signal	errors delta at      .*"time":"(?P<ts_rfc3339>)".*"errs":(?P<delta>[^,]*),.*newOrder.*
  signal	throughput scalar at .*"time":"(?P<ts_rfc3339>)".*"avgt":(?P<scalar>[^,]*),.*newOrder.*
  signal	lat50 scalar at      .*"time":"(?P<ts_rfc3339>)".*"p50l":(?P<scalar>[^,]*),.*newOrder.*
  signal	lat95 scalar at      .*"time":"(?P<ts_rfc3339>)".*"p95l":(?P<scalar>[^,]*),.*newOrder.*
  signal	lat99 scalar at      .*"time":"(?P<ts_rfc3339>)".*"p99l":(?P<scalar>[^,]*),.*newOrder.*
  signal error event at (?P<ts_now>)(?P<event>[eE]rror:.*)
end

parameter workload_size defaults to light
title ~workload_size~ workload
cast
  tpcc plays worker with workload_size=~workload_size~
end

# The internal_observer role is responsible for observing
# the "internal" tp/lat metrics.
role internal_observer
  cleanup   test -e start_spotlight || mkfifo start_spotlight
  :start    echo start>start_spotlight; cat start_spotlight
  spotlight cat start_spotlight; echo done>start_spotlight; \\
            HOME=$ROACHPROD_HOME/.. \\
			stdbuf -oL $ROACHPROD sql $CLUSTER:1 -- --format=csv --watch=1s -e "WITH a AS ( \\
    SELECT flags, count, service_lat_avg*count::FLOAT AS latsum \\
      FROM crdb_internal.node_statement_statistics \\
     WHERE application_name NOT LIKE '\$%' \\
    ) \\
    SELECT sum(count) FILTER (WHERE flags LIKE '%!%') AS err_cnt, \\
           sum(latsum) FILTER (WHERE flags LIKE '%!%') AS err_latsum, \\
           sum(count) FILTER (WHERE flags NOT LIKE '%!%') AS cnt, \\
           sum(latsum) FILTER (WHERE flags NOT LIKE '%!%') AS latsum \\
      FROM a"
   signal cnt scalar at (?P<ts_now>)(\d+(\.\d+)?,){2}(?P<scalar>[^,]*).*
   signal latsum scalar at (?P<ts_now>)(\d+(\.\d+)?,){3}(?P<scalar>[^,]*).*
end

cast
  bob plays internal_observer
end

# The script for the workload:
# skip the first two acts (these were to start up the cluster)
# start the workload,
# and define a 10s ramp-up period with blue mood.
script
  scene w entails for tpcc: start
  scene w entails for bob: start
  scene w mood starts blue
  scene c mood ends clear
  storyline . . w ....................c
end
attention the blue mood is the calibration period for the reference latency/throughput values

# This audience verifies the "good" behavior.
audience
  int_helper collects prev2_c as last 2 [bob cnt]
  int_helper computes itp as last(prev2_c) - first(prev2_c)
  int_helper collects prev2_l as last 2 [bob latsum]
  int_helper computes latsum as last(prev2_l) - first(prev2_l)
  int_helper computes ilat as 1000. * (latsum / itp)
  int_helper only helps

  tref audits only while mood == 'blue'
  tref collects itp_ref_values as last 5 itp
  tref computes itp_ref as avg(itp_ref_values)
  tref expects eventually: itp_ref > 0
  tref only helps

  lref audits only while mood == 'blue'
  lref collects ilat_ref_values as last 5 ilat
  lref computes ilat_ref as avg(ilat_ref_values)
  lref expects eventually: ilat_ref > 0
  lref only helps

  itp audits only while mood != 'blue' && itp_ref > 0
  itp watches itp
  itp expects always: itp >= (itp_ref / 2)
  itp measures statements/s (internal measure)

  ilat audits only while mood != 'blue' && ilat_ref > 0
  ilat watches ilat
  ilat expects always: ilat <= max(1000, ilat_ref * 2)
  ilat measures avg service latency ms (internal measure)

  xtp watches tpcc throughput
  xtp measures newOrder ops/s (external measure)
  xp99l watches tpcc lat99
  xp99l measures newOrder p99 latency ms (external measure)
end
