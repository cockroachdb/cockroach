# This test ensures that database and cluster backups properly
# backup and restore an IMPORT INTO of an empty table (foo) and a non empty table (foofoo).
#
# On a fully upgraded cluster: the table should get rollback to its pre-import state after RESTORE
# On an unfinalized cluster:
#  - a backed up import should not get restored
# TODO: ALLOW BACKUP/RESTORE TABLE of an in progress import
#
new-server name=s1
----

exec-sql
CREATE DATABASE d;
USE d;
CREATE TABLE foo (i INT PRIMARY KEY, s STRING);
CREATE TABLE foofoo (i INT PRIMARY KEY, s STRING);
INSERT INTO foofoo VALUES (10, 'x0');
CREATE TABLE baz (i INT PRIMARY KEY, s STRING);
INSERT INTO baz VALUES (1, 'x'),(2,'y'),(3,'z');
----

exec-sql
SET CLUSTER SETTING jobs.debug.pausepoints = 'import.after_ingest';
----

exec-sql
EXPORT INTO CSV 'nodelocal://0/export1/' FROM SELECT * FROM baz WHERE i = 1;
----

# Pause the import job, in order to back up the importing data.
import expect-pausepoint tag=a
IMPORT INTO foo (i,s) CSV DATA ('nodelocal://0/export1/export*-n*.0.csv')
----
job paused at pausepoint

import expect-pausepoint tag=aa
IMPORT INTO foofoo (i,s) CSV DATA ('nodelocal://0/export1/export*-n*.0.csv')
----
job paused at pausepoint

# Ensure Database, and cluster full backups capture importing rows.
exec-sql
BACKUP INTO 'nodelocal://0/cluster/' WITH revision_history;
----

exec-sql
BACKUP DATABASE d INTO 'nodelocal://0/database/' WITH revision_history;
----


# Ensure incremental backups do NOT re-capture the importing rows while the tables are offline
exec-sql
BACKUP INTO LATEST IN 'nodelocal://0/cluster/' WITH revision_history;
----

exec-sql
BACKUP DATABASE d INTO LATEST IN 'nodelocal://0/database/' WITH revision_history;
----

save-cluster-ts tag=t0
----

exec-sql
SET CLUSTER SETTING jobs.debug.pausepoints = '';
----


# Resume the job so the next set of incremental backups observes that tables are back online
job resume=a
----

job resume=aa
----

job tag=a wait-for-state=succeeded
----


job tag=aa wait-for-state=succeeded
----


# NOTE: currently the backups below re-capture the _1_ version of each imported key:
#  1. when the import job resumes, the data is re-ingested, hence a second version of the imported
#     data gets ingested into the backing up cluster, and consequently, the incremental backup.
#
# TODO (msbutler): add test coverage for a case when incremental backup only captures when the
# descriptor goes back online.

exec-sql
BACKUP INTO LATEST IN 'nodelocal://0/cluster/' WITH revision_history;
----

exec-sql
BACKUP DATABASE d INTO LATEST IN 'nodelocal://0/database/' WITH revision_history;
----



query-sql
SELECT
  database_name, object_name, object_type, rows, backup_type
FROM
  [SHOW BACKUP FROM LATEST IN 'nodelocal://0/cluster/']
WHERE
  object_name = 'foo' or object_name = 'foofoo';
----
d foo table 1 full
d foofoo table 2 full
d foo table 0 incremental
d foofoo table 0 incremental
d foo table 1 incremental
d foofoo table 1 incremental

query-sql
SELECT
  database_name, object_name, object_type, rows, backup_type
FROM
  [SHOW BACKUP FROM LATEST IN 'nodelocal://0/database/']
WHERE
  object_name = 'foo' or object_name = 'foofoo';
----
d foo table 1 full
d foofoo table 2 full
d foo table 0 incremental
d foofoo table 0 incremental
d foo table 1 incremental
d foofoo table 1 incremental

# Ensure all the RESTOREs contain foo (no data) and foofoo (1 row) as of system time t0
new-server name=s2 share-io-dir=s1 allow-implicit-access
----

restore aost=t0
RESTORE FROM LATEST IN 'nodelocal://0/cluster/' AS OF SYSTEM TIME t0;
----

query-sql
SELECT * FROM d.foo;
----

query-sql
SELECT * FROM d.foofoo;
----
10 x0

exec-sql
DROP DATABASE d;
----


restore aost=t0
RESTORE DATABASE d FROM LATEST IN 'nodelocal://0/database/' AS OF SYSTEM TIME t0;
----

query-sql
SELECT * FROM d.foo;
----

query-sql
SELECT * FROM d.foofoo;
----
10 x0

exec-sql
DROP TABLE d.foo;
----


# Ensure the imported data exists as of latest time
new-server name=s3 share-io-dir=s1 allow-implicit-access
----

exec-sql
RESTORE FROM LATEST IN 'nodelocal://0/cluster/';
----

query-sql
SELECT * FROM d.foo;
----
1 x

query-sql
SELECT * FROM d.foofoo;
----
1 x
10 x0

exec-sql
DROP DATABASE d;
----


exec-sql
RESTORE DATABASE d FROM LATEST IN 'nodelocal://0/database/';
----

query-sql
SELECT * FROM d.foo;
----
1 x

query-sql
SELECT * FROM d.foofoo;
----
1 x
10 x0

exec-sql
DROP TABLE d.foo;
----

#######################
# Version Gate Testing
#######################

# In an unfinalized cluster, back up some in-progress imports, and ensure that once the tables come
# back online we fully back them again.
#
# Note that during IMPORT planning on an unfinalized cluster, the
# ImportStartTime is not bound to the table's descriptor, therefore during
# RESTORE AOST in-progress IMPORT, these tables should get thrown out.

# TODO (msbutler): Currently, we still incrementally back up data from these
# "old" jobs. Once the schema changer gets out of the business setting
# descriptors to offline, consider preventing incremental backups of old
# jobs, as we always have to re-back up the span once it comes online.

new-server name=s4 share-io-dir=s1 allow-implicit-access beforeVersion=Start22_2
----

exec-sql
CREATE DATABASE d;
USE d;
CREATE TABLE foo (i INT PRIMARY KEY, s STRING);
CREATE TABLE foofoo (i INT PRIMARY KEY, s STRING);
INSERT INTO foofoo VALUES (10, 'x0');
CREATE TABLE baz (i INT PRIMARY KEY, s STRING);
INSERT INTO baz VALUES (1, 'x'),(2,'y'),(3,'z');
----


exec-sql
SET CLUSTER SETTING jobs.debug.pausepoints = 'import.after_ingest';
----


exec-sql
EXPORT INTO CSV 'nodelocal://0/export1/' FROM SELECT * FROM baz WHERE i = 1;
----


# Pause the import job, in order to back up the importing data.
import expect-pausepoint tag=b
IMPORT INTO foo (i,s) CSV DATA ('nodelocal://0/export1/export*-n*.0.csv')
----
job paused at pausepoint


import expect-pausepoint tag=bb
IMPORT INTO foofoo (i,s) CSV DATA ('nodelocal://0/export1/export*-n*.0.csv')
----
job paused at pausepoint


# The first backup in the chain will capture data from offline tables
# NOTE: in a mixed version setting, we don't need to do this. (see todo above)
exec-sql
BACKUP INTO 'nodelocal://0/cluster/' with revision_history;
----


exec-sql
BACKUP DATABASE d INTO 'nodelocal://0/database/' with revision_history;
----


exec-sql
BACKUP DATABASE d INTO 'nodelocal://0/database_upgrade/' with revision_history;
----


save-cluster-ts tag=m0
----


# Conduct another set of incremental backups to ensure that _offline_ tables do _not_ fully backup
# in a cluster in an unfinalized state
exec-sql
BACKUP INTO LATEST IN 'nodelocal://0/cluster/' with revision_history;
----


exec-sql
BACKUP DATABASE d INTO LATEST IN 'nodelocal://0/database/' with revision_history;
----


exec-sql
CREATE VIEW show_cluster_backup AS
SELECT
  database_name, object_name, object_type, rows, backup_type
FROM
  [SHOW BACKUP FROM LATEST IN 'nodelocal://0/cluster/']
WHERE
  object_name = 'foo' or object_name = 'foofoo';
----


exec-sql
CREATE VIEW show_database_backup AS
SELECT
  database_name, object_name, object_type, rows, backup_type
FROM
  [SHOW BACKUP FROM LATEST IN 'nodelocal://0/database/']
WHERE
  object_name = 'foo' or object_name = 'foofoo';
----


query-sql
SELECT * FROM show_cluster_backup;
----
d foo table 1 full
d foofoo table 2 full
d foo table 0 incremental
d foofoo table 0 incremental


query-sql
SELECT * FROM show_database_backup;
----
d foo table 1 full
d foofoo table 2 full
d foo table 0 incremental
d foofoo table 0 incremental


exec-sql
SET CLUSTER SETTING jobs.debug.pausepoints = '';
----


# Resume the job so the next set of incremental backups observes that tables are back online
job resume=b
----


job tag=b wait-for-state=succeeded
----


job resume=bb
----


job tag=bb wait-for-state=succeeded
----


# Key Test: Ensure that once the tables come back online, everything gets backed
# up again, as these imports may have non-mvcc ops in them. Ensure this in the
# unfinalized cluster and in the finalized cluster.
exec-sql
BACKUP INTO LATEST IN 'nodelocal://0/cluster/' with revision_history;
----


exec-sql
BACKUP DATABASE d INTO LATEST IN 'nodelocal://0/database/' with revision_history;
----


query-sql
SELECT * FROM show_cluster_backup;
----
d foo table 1 full
d foofoo table 2 full
d foo table 0 incremental
d foofoo table 0 incremental
d foo table 2 incremental
d foofoo table 3 incremental


query-sql
SELECT * FROM show_database_backup;
----
d foo table 1 full
d foofoo table 2 full
d foo table 0 incremental
d foofoo table 0 incremental
d foo table 2 incremental
d foofoo table 3 incremental


upgrade-server version=Start22_2
----

exec-sql
BACKUP DATABASE d INTO LATEST IN 'nodelocal://0/database_upgrade/' with revision_history;
----

query-sql
SELECT
  database_name, object_name, object_type, rows, backup_type
FROM
  [SHOW BACKUP FROM LATEST IN 'nodelocal://0/database_upgrade/']
WHERE
  object_name = 'foo' or object_name = 'foofoo';
----
d foo table 1 full
d foofoo table 2 full
d foo table 2 incremental
d foofoo table 3 incremental

# A restore on mixed version cluster should always fail on a backup that contains import times
new-server name=s5 share-io-dir=s1 allow-implicit-access
----

restore
RESTORE FROM LATEST IN 'nodelocal://0/cluster/';
----

# Now actually Restore the backups taken from a mixed version chain
new-server name=s6 share-io-dir=s1 allow-implicit-access
----

# Ensure the RESTOREs omit the tables with in progress imports (foo and foofoo)
# as their descriptor will not have the import start time.
restore aost=m0
RESTORE FROM LATEST IN 'nodelocal://0/cluster/' AS OF SYSTEM TIME m0;
----


query-sql
SELECT table_name FROM [SHOW TABLES FROM d];
----
baz


exec-sql
DROP DATABASE d;
----


# Restore AOST after the table comes back online
restore
RESTORE DATABASE d FROM LATEST IN 'nodelocal://0/database/';
----


query-sql
SELECT table_name FROM [SHOW TABLES FROM d];
----
foo
foofoo
baz
show_cluster_backup
show_database_backup

