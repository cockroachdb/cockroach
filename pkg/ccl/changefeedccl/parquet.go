// Copyright 2022 The Cockroach Authors.
//
// Licensed as a CockroachDB Enterprise file under the Cockroach Community
// License (the "License"); you may not use this file except in compliance with
// the License. You may obtain a copy of the License at
//
//     https://github.com/cockroachdb/cockroach/blob/master/licenses/CCL.txt

package changefeedccl

import (
	"bytes"
	"io"
	"strconv"
	"strings"

	"github.com/cockroachdb/cockroach/pkg/ccl/changefeedccl/cdcevent"
	"github.com/cockroachdb/cockroach/pkg/sql/sem/tree"
	"github.com/cockroachdb/cockroach/pkg/sql/types"
	"github.com/cockroachdb/cockroach/pkg/util/buildutil"
	"github.com/cockroachdb/cockroach/pkg/util/parquet"
	"github.com/cockroachdb/errors"
)

// includeParquestTestMetadata configures the parquet writer to write
// metadata required for reading parquet files in tests.
var includeParquestTestMetadata = false

type parquetWriter struct {
	inner      *parquet.Writer
	datumAlloc []tree.Datum
}

// newParquetSchemaDefintion returns a parquet schema definition based on the
// cdcevent.Row and the number of cols in the schema.
func newParquetSchemaDefintion(row cdcevent.Row) (*parquet.SchemaDefinition, int, error) {
	var columnNames []string
	var columnTypes []*types.T

	numCols := 0
	if err := row.ForAllColumns().Col(func(col cdcevent.ResultColumn) error {
		columnNames = append(columnNames, col.Name)
		columnTypes = append(columnTypes, col.Typ)
		numCols += 1
		return nil
	}); err != nil {
		return nil, 0, err
	}

	columnNames = append(columnNames, parquetCrdbEventTypeColName)
	columnTypes = append(columnTypes, types.String)
	numCols += 1

	schemaDef, err := parquet.NewSchema(columnNames, columnTypes)
	if err != nil {
		return nil, 0, err
	}
	return schemaDef, numCols, nil
}

// newParquetWriterFromRow constructs a new parquet writer which outputs to
// the given sink. This function interprets the schema from the supplied row.
func newParquetWriterFromRow(
	row cdcevent.Row, sink io.Writer, opts ...parquet.Option,
) (*parquetWriter, error) {
	schemaDef, numCols, err := newParquetSchemaDefintion(row)
	if err != nil {
		return nil, err
	}

	if includeParquestTestMetadata {
		if opts, err = addParquetTestMetadata(row, opts); err != nil {
			return nil, err
		}
	}
	writer, err := newParquetWriter(schemaDef, sink, opts...)
	if err != nil {
		return nil, err
	}
	return &parquetWriter{inner: writer, datumAlloc: make([]tree.Datum, numCols)}, nil
}

// addData writes the updatedRow, adding the row's event type. There is no guarantee
// that data will be flushed after this function returns.
func (w *parquetWriter) addData(updatedRow cdcevent.Row, prevRow cdcevent.Row) error {
	if err := populateDatums(updatedRow, prevRow, w.datumAlloc); err != nil {
		return err
	}

	return w.inner.AddRow(w.datumAlloc)
}

// close closes the writer and flushes any buffered data to the sink.
func (w *parquetWriter) close() error {
	return w.inner.Close()
}

// populateDatums writes the appropriate datums into the datumAlloc slice.
func populateDatums(updatedRow cdcevent.Row, prevRow cdcevent.Row, datumAlloc []tree.Datum) error {
	datums := datumAlloc[:0]

	if err := updatedRow.ForAllColumns().Datum(func(d tree.Datum, _ cdcevent.ResultColumn) error {
		datums = append(datums, d)
		return nil
	}); err != nil {
		return err
	}
	datums = append(datums, getEventTypeDatum(updatedRow, prevRow).DString())
	return nil
}

// addParquetTestMetadata appends options to the provided options to configure the
// parquet writer to write metadata required by cdc test feed factories.
//
// Generally, cdc tests will convert the row to JSON loosely in the form:
// `[0]->{"b": "b", "c": "c"}` with the key columns in square brackets and value
// columns in a JSON object. The metadata generated by this function contains
// key and value column names along with their offsets in the parquet file.
func addParquetTestMetadata(row cdcevent.Row, opts []parquet.Option) ([]parquet.Option, error) {
	// NB: Order matters. When iterating using ForAllColumns, which is used when
	// writing datums and defining the schema, the order of columns usually
	// matches the underlying table. If a composite keys defined, the order in
	// ForEachKeyColumn may not match. In tests, we want to use the latter
	// order when printing the keys.
	keyCols := map[string]int{}
	var keysInOrder []string
	if err := row.ForEachKeyColumn().Col(func(col cdcevent.ResultColumn) error {
		keyCols[col.Name] = -1
		keysInOrder = append(keysInOrder, col.Name)
		return nil
	}); err != nil {
		return opts, err
	}

	// NB: We do not use ForAllColumns here because it will always contain the
	// key. In tests where we target a column family without a key in it,
	// ForEachColumn will exclude the primary key of the table, which is what
	// we want.
	valueCols := map[string]int{}
	var valuesInOrder []string
	if err := row.ForEachColumn().Col(func(col cdcevent.ResultColumn) error {
		valueCols[col.Name] = -1
		valuesInOrder = append(valuesInOrder, col.Name)
		return nil
	}); err != nil {
		return opts, err
	}

	// Iterate over ForAllColumns to determine the offets of each column
	// in a parquet row (ie. the slice of datums provided to addData). We don't
	// do this above because there is no way to determine it from
	// cdcevent.ResultColumn. The Ordinal() method may return an invalid
	// number for virtual columns.
	idx := 0
	if err := row.ForAllColumns().Col(func(col cdcevent.ResultColumn) error {
		if _, colIsInKey := keyCols[col.Name]; colIsInKey {
			keyCols[col.Name] = idx
		}
		if _, colIsInValue := valueCols[col.Name]; colIsInValue {
			valueCols[col.Name] = idx
		}
		idx += 1
		return nil
	}); err != nil {
		return opts, err
	}
	valuesInOrder = append(valuesInOrder, parquetCrdbEventTypeColName)
	valueCols[parquetCrdbEventTypeColName] = idx
	idx += 1

	opts = append(opts, parquet.WithMetadata(map[string]string{"keyCols": serializeMap(keysInOrder, keyCols)}))
	opts = append(opts, parquet.WithMetadata(map[string]string{"allCols": serializeMap(valuesInOrder, valueCols)}))
	return opts, nil
}

// serializeMap serializes a map to a string. For example, orderedKeys=["b",
// "a"] m={"a": 1", "b": 2, "c":3} will return the string "b,2,a,1".
func serializeMap(orderedKeys []string, m map[string]int) string {
	var buf bytes.Buffer
	for i, k := range orderedKeys {
		if i > 0 {
			buf.WriteString(",")
		}
		buf.WriteString(k)
		buf.WriteString(",")
		buf.WriteString(strconv.Itoa(m[k]))
	}
	return buf.String()
}

// deserializeMap deserializes a string in the form "b,2,a,1" and returns a map
// representation along with the keys in order: m={"a": 1", "b": 2}
// orderedKeys=["b", "a"].
func deserializeMap(s string) (orderedKeys []string, m map[string]int, err error) {
	keyValues := strings.Split(s, ",")
	if len(keyValues)%2 != 0 {
		return nil, nil,
			errors.AssertionFailedf("list of elements %s should have an even length", s)
	}
	for i := 0; i < len(keyValues); i += 2 {
		key := keyValues[i]
		value, err := strconv.Atoi(keyValues[i+1])
		if err != nil {
			return nil, nil, err
		}
		orderedKeys = append(orderedKeys, key)
		if i == 0 {
			m = map[string]int{}
		}
		m[key] = value
	}
	return orderedKeys, m, nil
}

// newParquetWriter allocates a new parquet writer using the provided
// schema definition.
func newParquetWriter(
	sch *parquet.SchemaDefinition, sink io.Writer, opts ...parquet.Option,
) (*parquet.Writer, error) {
	if includeParquestTestMetadata {
		// To use parquet test utils for reading datums, the writer needs to be
		// configured with additional metadata.
		return parquet.NewWriterWithReaderMeta(sch, sink, opts...)
	}

	return parquet.NewWriter(sch, sink, opts...)
}

func init() {
	if buildutil.CrdbTestBuild {
		includeParquestTestMetadata = true
	}
}
