# Understanding the "IO overload" Log Message

The "IO overload" log message in CockroachDB provides a dense but detailed snapshot of the I/O subsystem's state, particularly when it's under pressure. It's triggered when certain thresholds are crossed, indicating that the storage layer is struggling to keep up with the write load. The message is composed of two main parts, described below.

The log message is generated by this line of code in `pkg/util/admission/io_load_listener.go`:

```go
log.Infof(ctx, "IO overload: %s; %s", io.adjustTokensResult, io.diskBandwidthLimiter)
```

## `adjustTokensResult`

This is the primary component, detailing the state of the LSM (Log-Structured Merge-tree) and the admission control system's response.

**Format:**
```
l0=.../.../... f=.../s=... c=.../... mb-w-stall-since-start=...
add-bytes=... compact-bytes=...
tok-kind=... used-comp-tok-bound=...
[flush-util=... flush-tok=... flush-d=...] [wal-failover] [unflushed-memtable-too-large]
total-tok=... alloc-tok=... used-tok=...
total-elastic-tok=... alloc-elastic-tok=... used-elastic-tok=...
```

### Breakdown of Fields

-   **`l0={L0-files}/{L0-sublevels}/{L0-bytes}`**: This shows the state of Level 0 (L0) in the LSM tree, which is often the first bottleneck for write-heavy workloads.
    -   **`L0-files`**: The number of files in L0. Pay attention if this number is growing rapidly, especially if it approaches `admission.l0_file_count_overload_threshold` (default: 4000). A high file count can slow down reads due to the need to check many files for a key.
    -   **`L0-sublevels`**: The number of sub-levels in L0. Pebble creates sub-levels to handle overlapping SSTables. A high number (approaching `admission.l0_sub_level_count_overload_threshold`, default: 20) indicates high write amplification and compaction pressure.
    -   **`L0-bytes`**: The total size of L0. This provides context for the file and sub-level counts. A large size might be expected, but a large size with a high file/sub-level count is a sign of trouble.

-   **`f={files-score}/s={sublevels-score}`**: These are the calculated "scores" for L0 files and sub-levels, contributing to the overall IO threshold. A score of 1.0 or higher indicates that the resource is considered overloaded.

-   **`c={out-going-bytes}/{out-levels}`**: This shows compaction statistics.
    -   **`out-going-bytes`**: The number of bytes being written from non-L0 levels to lower levels. This indicates how much work compactions are doing in the rest of the LSM tree.
    -   **`out-levels`**: The number of levels (other than L0) from which compactions are actively moving data.

-   **`mb-w-stall-since-start={megabytes-written-before-stall}`**: The cumulative throughput of writes to flushing memtables before a write stall occurs. It's a measure of how much data can be written before the system has to pause writes to let flushes catch up. A low number here is a strong indicator of I/O overload.

-   **`add-bytes={added-bytes}`**: The number of bytes added to L0 in the last measurement interval. This shows the current incoming write rate to the LSM.

-   **`compact-bytes={compacted-bytes}`**: The number of bytes compacted out of L0 in the last measurement interval. Ideally, `compact-bytes` should keep up with `add-bytes`. If `add-bytes` is consistently higher, L0 will grow, and overload is imminent.

-   **`tok-kind={kind}`**: Indicates the primary reason for the current token allocation decision. It can be `compaction` (L0 overload is the main concern) or `flush` (memtable flushing is the main concern). This tells you where the bottleneck is.

-   **`used-comp-tok-bound={true/false}`**: This is true if the system is using a lower-bound estimate for compaction-based tokens, which happens when there are no compactions to measure. This can indicate that the system is either idle or so backed up that compactions aren't running.

-   **`[flush-util={utilization} flush-tok={tokens} flush-d={duration}]`**: This section appears when the `tok-kind` is `flush`.
    -   **`flush-util`**: The target flush utilization fraction. This is dynamically adjusted to control write stalls. It's usually between 0.5 and 1.0. If it's at its minimum (0.5), it means the system is aggressively trying to reduce write stalls.
    -   **`flush-tok`**: The number of tokens generated based on memtable flush performance.
    -   **`flush-d`**: The duration over which flush performance was measured.

-   **`[wal-failover]`**: This flag appears if the system has detected a WAL (Write-Ahead Log) failover, which can happen if the disk is overloaded. This is a serious condition.

-   **`[unflushed-memtable-too-large]`**: This flag appears if there's a memtable that has grown too large without being flushed. This is another indicator of a flush bottleneck.

-   **`total-tok`, `alloc-tok`, `used-tok`**: These show the total number of I/O tokens available for regular priority work, how many have been allocated, and how many have been used in the last interval.

-   **`total-elastic-tok`, `alloc-elastic-tok`, `used-elastic-tok`**: Same as above, but for elastic (low-priority) work.

## `diskBandwidthLimiter`

This part of the log focuses on whether the physical disk bandwidth is a bottleneck.

**Format:**
```
disk-bw-limiter: util=... elastic-util=... elastic-tok=...
```

### Breakdown of Fields

-   **`util={utilization}`**: The estimated utilization of the disk's provisioned bandwidth by all traffic (reads and writes). Pay attention if this approaches 1.0 (100%), as it means the disk is saturated.
-   **`elastic-util={utilization}`**: The portion of the disk bandwidth utilization attributed to elastic (low-priority) traffic.
-   **`elastic-tok={tokens}`**: The number of tokens being made available for elastic work based on the available disk bandwidth. If this is low, it means the system is throttling low-priority work to reserve disk bandwidth for more critical operations.

## When to Pay Attention

You should pay attention to this log message when you are investigating performance issues related to high I/O, such as slow writes, high latencies, or write stalls. It provides a very detailed breakdown of where the I/O pressure is coming from, allowing you to distinguish between LSM-level bottlenecks (L0, compactions, flushes) and physical disk saturation.

## Reasons for Throttling

When the admission controller decides to throttle, the log message will contain the phrase `admitting ... (rate ... MiB/s)`. If it is not throttling, it will say `admitting all`. The reason for the decision is stated near the admission rate.

### 1. L0 Compaction Overload

-   **Reason in Log:** The admission decision will state `due to high L0 file count` or `due to high L0 sub-level count`.
-   **What it means:** The system is writing data into Level 0 (L0) of the LSM faster than it can be compacted into lower levels. This is the most common reason for throttling. A bloated L0 increases read amplification, meaning queries have to do more work (and more disk I/O) to find data, which can significantly increase read latencies.
-   **What to look for in the log:**
    -   At the beginning of the message, find `compaction score X.XXX (Y ssts, Z sub-levels)`. A high score (`X`), a high file count (`Y`), or a high sub-level count (`Z`) are the primary indicators.
    -   Compare the `L0 growth X MiB` value with the `compacted Y MiB` value. If incoming data (`X`) is consistently higher than what can be compacted (`Y`), L0 will continue to grow and throttling will continue.
-   **Affected Work & Throttling Thresholds:** This condition uses a tiered throttling approach based on an internal "badness" score for L0 health (where 0 is healthy and 1.0 is overloaded).
    -   **Score >= 0.5:** When L0 health is moderately degraded, the system throttles **only elastic work**. This is a preventative measure to slow down background jobs and free up resources for compaction.
    -   **Score >= 1.0:** When L0 health is seriously degraded, the system throttles **all work**, including regular user-facing traffic.

### 2. Memtable Flush Overload

-   **Reason in Log:** The admission decision will explicitly state `due to memtable flush`.
-   **What it means:** The system is struggling to flush in-memory write buffers (memtables) to durable storage (SSTables in L0). This is a more urgent problem than L0 compaction overload. If the system can't flush memtables, it will eventually have to stop accepting new writes altogether, leading to a **write stall**. The admission controller throttles writes proactively to prevent this from happening.
-   **What to look for in the log:**
    -   The explicit `due to memtable flush` reason is the clearest indicator.
    -   Check the end of the message for `write stalls X`. A non-zero value for `X` is a severe symptom that indicates the proactive throttling was not enough to prevent a stall.
    -   The log may also contain the flag `unflushed-memtable-too-large`.
-   **Affected Work & Throttling Thresholds:** This condition affects **all work** immediately. Throttling due to memtable flushing is considered urgent to prevent write stalls, so there is no separate, lower threshold for elastic work. Both regular and elastic traffic are rate-limited as soon as this becomes the bottleneck.

### 3. Disk Bandwidth Overload

-   **Reason in Log:** The admission decision will state `due to disk bandwidth`.
-   **What it means:** The physical disk has reached its provisioned bandwidth limit. All I/O operations, including reads and writes for compactions, are competing for a saturated resource. This will cause high latencies for all disk-touching operations on the node.
-   **What to look for in the log:**
    -   This feature requires disk bandwidth to be configured on the node. If it is not, the `diskBandwidthLimiter` section will show `provisioned 0 B/s`.
    -   Examine the `diskBandwidthLimiter (...)` section at the very end of the log. The key indicator is `tokenUtilization`, which will approach `1.0` if disk bandwidth is the bottleneck.
    -   When this is the bottleneck, the admission rate for `elastic` work will likely be throttled to zero or a very low value.
-   **Affected Work & Throttling Thresholds:** This condition primarily affects **only elastic work**. The system is designed to throttle lower-priority tasks that are heavy on disk I/O (like backups and bulk ingestions) to reserve physical bandwidth for higher-priority, latency-sensitive work. Regular traffic is generally not throttled for this reason. There isn't a separate "badness" threshold; it simply throttles elastic work when the disk is nearing its configured capacity. 

# Improving the IO overload metrics


### Example 1: Healthy System

> IO overload: compaction score 0.000 (0 ssts, 0 sub-levels), L0 growth 10 MiB (write 10 MiB (ignored 0 B) ingest 0 B (ignored 0 B)): requests 84706 (4012 bypassed) with 47 MiB acc-write (4.4 MiB bypassed) + 0 B acc-ingest (0 B bypassed) + 10 MiB adjusted-LSM-writes + 957 MiB adjusted-disk-writes + write-model 0.50x+1 B (smoothed 0.52x+1 B) + l0-ingest-model 0.00x+0 B (smoothed 0.75x+1 B) + ingest-model 0.00x+0 B (smoothed 1.00x+1 B) + write-amp-model 92.88x+1 B (smoothed 68.07x+1 B) + at-admission-tokens 159 B, compacted 31 MiB [≈21 MiB], flushed 941 MiB [≈841 MiB] (mult 1.00); admitting all (used total: 13 MiB elastic 1.1 MiB); write stalls 0; diskBandwidthLimiter‹×› (tokenUtilization 0.00, tokensUsed (elastic 0 B, snapshot 0 B, regular 0 B) tokens (write 0 B (prev 0 B), read 0 B (prev 0 B)), writeBW 0 B/s, readBW 0 B/s, provisioned 0 B/s)


-   **What we're seeing:** This log shows a healthy system that is not under I/O pressure. The `compaction score` is zero, there are `0 write stalls`, and crucially, the system is `admitting all` traffic. The amount of data being compacted out of L0 (`31 MiB`) is greater than the amount being added (`L0 growth 10 MiB`), which means the system is easily keeping up with the write load.

-   **Crucial bits:**
    -   `compaction score 0.000`
    -   `L0 growth 10 MiB` vs. `compacted 31 MiB`
    -   `admitting all`
    -   `write stalls 0`

-   **Irrelevant information:** In this healthy state, almost all of the detailed statistics are noise.
    -   The various estimation models (`write-model`, `ingest-model`, `write-amp-model`, etc.) are implementation details of the controller and irrelevant when it's not throttling.
    -   The `diskBandwidthLimiter` section is all zeros, indicating it's not configured or active, and can be ignored.
    -   The detailed breakdown of requests and accumulated writes provides more detail than necessary for a high-level status check.

-   **Improved message could look like:**
    ```
    IO Admission Controller:
      - Status:             OK (admitting all traffic)
      - Reason:             L0 compaction score (0.0) is below overload threshold.
      - Health:             0 write stalls in the last interval.
      - L0 State:           score=0.0, files=0, sub-levels=0
      - L0 Rate (In/Out):   +10 MiB / -31 MiB (Net: -21 MiB)
    ```

-   **Why it's better:** The proposed format is scannable and immediately communicates the system's health. It places the most important information (status and reason) at the top and computes the net L0 rate so an operator doesn't have to do the math. It hides the verbose, low-level details that are not actionable in this healthy state.

### Example 2: Throttling due to Memtable Flush

> IO overload: compaction score 0.100 (11 ssts, 2 sub-levels), L0 growth 66 MiB (write 66 MiB (ignored 0 B) ingest 0 B (ignored 0 B)): requests 126009 (64002 bypassed) with 138 MiB acc-write (98 MiB bypassed) + 0 B acc-ingest (0 B bypassed) + 66 MiB adjusted-LSM-writes + 1.1 GiB adjusted-disk-writes + write-model 0.50x+1 B (smoothed 0.50x+1 B) + ingested-model 0.00x+0 B (smoothed 0.75x+1 B) + write-amp-model 16.42x+1 B (smoothed 19.46x+1 B) + at-admission-tokens 400 B, compacted 54 MiB [≈52 MiB], flushed 553 MiB [≈517 MiB] (mult 1.00); admitting 517 MiB (rate 34 MiB/s) (elastic 65 MiB rate 4.3 MiB/s) due to memtable flush (multiplier 1.000) (used total: 69 MiB elastic 8.9 MiB); write stalls 0; diskBandwidthLimiter (tokenUtilization 0.00, tokensUsed (elastic 0 B, snapshot 0 B, regular 0 B) tokens (write 0 B (prev 0 B)), writeBW 0 B/s, readBW 0 B/s, provisioned 0 B/s)


-   **What we're seeing:** This log shows a system that has begun to throttle incoming work. The key phrases are `admitting 517 MiB (rate 34 MiB/s)` and `due to memtable flush`. This means the system is not limiting work because of L0's size, but as a preventative measure to allow in-memory memtables to be flushed to disk, thereby avoiding future write stalls. The L0 is growing slightly (`66 MiB` in vs. `54 MiB` out), but this is not the primary reason for throttling.

-   **Crucial bits:**
    -   `admitting 517 MiB (rate 34 MiB/s)`: This is the rate limit being applied.
    -   `due to memtable flush`: This is the specific reason for the rate limit.
    -   `compaction score 0.100`: Important to note that this is low, confirming L0 is not the problem.
    -   `write stalls 0`: Shows the proactive throttling is successfully preventing stalls.

-   **Irrelevant information:**
    -   The linear models (`write-model`, etc.) remain irrelevant for operators.
    -   The inactive `diskBandwidthLimiter` section is still just noise.
    -   While the various byte counters are related, the most important information is the final decision and the reason why.

-   **Improved message could look like:**
    ```
    IO Admission Controller:
      - Status:             Throttling (admitting at 34 MiB/s regular, 4.3 MiB/s elastic)
      - Reason:             Memtable flushing is the current bottleneck.
      - Health:             0 write stalls in the last interval.
      - L0 State:           score=0.1, files=11, sub-levels=2
      - L0 Rate (In/Out):   +66 MiB / -54 MiB (Net: +12 MiB)
    ```

-   **Why it's better:** It leads with the most critical information: the system is throttling and at what rate. It clearly distinguishes the reason (memtable flushes) from other potential causes (like L0 health), guiding an operator to the correct area of investigation.

### Example 3: Throttling due to L0 Growth under high stress

> IO overload: compaction score 0.050 (53 ssts, 1 sub-levels), L0 growth 345 MiB (flush-backlog)  (write 345 MiB (ignored 0 B) ingest 0 B (ignored 0 B)): requests 31956 (28000 bypassed) with 1.4 GiB acc-write (1.1 GiB bypassed) + 0 B acc-ingest (0 B bypassed) + 345 MiB adjusted-LSM-writes + 4.0 GiB adjusted-disk-writes + write-model 0.00x+0 B (smoothed 1.76x+15 B) + ingested-model 0.00x+0 B (smoothed 0.28x+1 B) + write-amp-model 11.97x+1 B (smoothed 7.61x+68 B) + at-admission-tokens 12 KiB, compacted 420 MiB [≈281 MiB], flushed 897 MiB [≈1.1 GiB] (mult 0.52); admitting (WAL failover) elastic 420 MiB (rate 28 MiB/s) due to L0 growth (used total: 2.5 GiB elastic 0 B); write stalls 0; diskBandwidthLimiter (tokenUtilization 0.00, tokensUsed (elastic 0 B, snapshot 0 B, regular 0 B) tokens (write 0 B (prev 0 B)), writeBW 0 B/s, readBW 0 B/s, provisioned 0 B/s)

-   **What we're seeing:** This is the most complex case, showing a system under extreme duress. It has two critical, simultaneous events: `(flush-backlog)` and `(WAL failover)`. The interaction between these two events triggers a special admission policy. Normally, a `flush-backlog` would pause regular traffic. However, the `(WAL failover)` takes precedence. To avoid deadlocks during storage recovery, the system **admits all regular work without throttling**. The log message is confusing as it only specifies the admission rate for elastic work.

-   **Crucial bits:**
    -   The combination of `(flush-backlog)` and `(WAL failover)`. This is the key to understanding the unusual admission policy.
    -   `admitting ... elastic 420 MiB`: The admission decision, showing a rate limit for elastic work. The *absence* of a rate for regular work, combined with the WAL failover, means regular work is not shaped.
    -   The net L0 rate is `+345 MiB` in vs. `420 MiB` out, so compactions are currently winning the race, which is a positive sign for recovery.

-   **Irrelevant information:** The signal-to-noise ratio is very low here. The linear models and detailed byte counters are secondary to the high-level status flags.

-   **Improved message could look like:**
    ```
    IO Admission Controller:
      - Status:             Throttling Elastic Work Only (admitting at 28 MiB/s)
      - Reason:             Flush backlog with WAL failover. Regular work is un-throttled to prevent deadlock.
      - Health:             0 write stalls in the last interval.
      - L0 State:           score=0.05, files=53, sub-levels=1
      - L0 Rate (In/Out):   +345 MiB / -420 MiB (Net: -75 MiB)
      - Critical Events:    WAL failover detected; Flush backlog present.
    ```

-   **Why it's better:** This format explicitly states the unique admission policy and the reason for it. It clarifies that regular work is exempt from throttling and elevates the critical events (`WAL failover`) so they cannot be missed. This removes the ambiguity of the current log format in this specific, high-stress scenario.