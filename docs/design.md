# ABOUT

THIS DOCUMENT IS AN UPDATED VERSION OF THE ORIGINAL DESIGN DOCUMENTS
BY SPENCER KIMBALL FROM EARLY 2014. IT MAY NOT ALWAYS BE COMPLETELY UP TO DATE.
FOR A MORE APPROACHABLE EXPLANATION OF HOW COCKROACHDB WORKS, CONSIDER READING
THE [ARCHITECTURE DOCS](HTTPS://WWW.COCKROACHLABS.COM/DOCS/STABLE/ARCHITECTURE/OVERVIEW.HTML).

# OVERVIEW

COCKROACHDB IS A DISTRIBUTED SQL DATABASE. THE PRIMARY DESIGN GOALS
ARE **SCALABILITY**, **STRONG CONSISTENCY** AND **SURVIVABILITY**
(HENCE THE NAME). COCKROACHDB AIMS TO TOLERATE DISK, MACHINE, RACK, AND
EVEN **DATACENTER FAILURES** WITH MINIMAL LATENCY DISRUPTION AND **NO
MANUAL INTERVENTION**. COCKROACHDB NODES ARE SYMMETRIC; A DESIGN GOAL IS
**HOMOGENEOUS DEPLOYMENT** (ONE BINARY) WITH MINIMAL CONFIGURATION AND
NO REQUIRED EXTERNAL DEPENDENCIES.

THE ENTRY POINT FOR DATABASE CLIENTS IS THE SQL INTERFACE. EVERY NODE
IN A COCKROACHDB CLUSTER CAN ACT AS A CLIENT SQL GATEWAY. A SQL
GATEWAY TRANSFORMS AND EXECUTES CLIENT SQL STATEMENTS TO KEY-VALUE
(KV) OPERATIONS, WHICH THE GATEWAY DISTRIBUTES ACROSS THE CLUSTER AS
NECESSARY AND RETURNS RESULTS TO THE CLIENT. COCKROACHDB IMPLEMENTS A
**SINGLE, MONOLITHIC SORTED MAP** FROM KEY TO VALUE WHERE BOTH KEYS
AND VALUES ARE BYTE STRINGS.

THE KV MAP IS LOGICALLY COMPOSED OF SMALLER SEGMENTS OF THE KEYSPACE CALLED
RANGES. EACH RANGE IS BACKED BY DATA STORED IN A LOCAL KV STORAGE ENGINE (WE
USE [ROCKSDB](HTTP://ROCKSDB.ORG/), A VARIANT OF
[LEVELDB](HTTPS://GITHUB.COM/GOOGLE/LEVELDB)). RANGE DATA IS REPLICATED TO A
CONFIGURABLE NUMBER OF ADDITIONAL COCKROACHDB NODES. RANGES ARE MERGED AND
SPLIT TO MAINTAIN A TARGET SIZE, BY DEFAULT `64M`. THE RELATIVELY SMALL SIZE
FACILITATES QUICK REPAIR AND REBALANCING TO ADDRESS NODE FAILURES, NEW CAPACITY
AND EVEN READ/WRITE LOAD. HOWEVER, THE SIZE MUST BE BALANCED AGAINST THE
PRESSURE ON THE SYSTEM FROM HAVING MORE RANGES TO MANAGE.

COCKROACHDB ACHIEVES HORIZONTALLY SCALABILITY:
- ADDING MORE NODES INCREASES THE CAPACITY OF THE CLUSTER BY THE
  AMOUNT OF STORAGE ON EACH NODE (DIVIDED BY A CONFIGURABLE
  REPLICATION FACTOR), THEORETICALLY UP TO 4 EXABYTES (4E) OF LOGICAL
  DATA;
- CLIENT QUERIES CAN BE SENT TO ANY NODE IN THE CLUSTER, AND QUERIES
  CAN OPERATE INDEPENDENTLY (W/O CONFLICTS), MEANING THAT OVERALL
  THROUGHPUT IS A LINEAR FACTOR OF THE NUMBER OF NODES IN THE CLUSTER.
- QUERIES ARE DISTRIBUTED (REF: DISTRIBUTED SQL) SO THAT THE OVERALL
  THROUGHPUT OF SINGLE QUERIES CAN BE INCREASED BY ADDING MORE NODES.

COCKROACHDB ACHIEVES STRONG CONSISTENCY:
- USES A DISTRIBUTED CONSENSUS PROTOCOL FOR SYNCHRONOUS REPLICATION OF
  DATA IN EACH KEY VALUE RANGE. WE’VE CHOSEN TO USE THE [RAFT
  CONSENSUS ALGORITHM](HTTPS://RAFTCONSENSUS.GITHUB.IO); ALL CONSENSUS
  STATE IS STORED IN ROCKSDB.
- SINGLE OR BATCHED MUTATIONS TO A SINGLE RANGE ARE MEDIATED VIA THE
  RANGE'S RAFT INSTANCE. RAFT GUARANTEES ACID SEMANTICS.
- LOGICAL MUTATIONS WHICH AFFECT MULTIPLE RANGES EMPLOY DISTRIBUTED
  TRANSACTIONS FOR ACID SEMANTICS. COCKROACHDB USES AN EFFICIENT
  **NON-LOCKING DISTRIBUTED COMMIT** PROTOCOL.

COCKROACHDB ACHIEVES SURVIVABILITY:
- RANGE REPLICAS CAN BE CO-LOCATED WITHIN A SINGLE DATACENTER FOR LOW
  LATENCY REPLICATION AND SURVIVE DISK OR MACHINE FAILURES. THEY CAN
  BE DISTRIBUTED ACROSS RACKS TO SURVIVE SOME NETWORK SWITCH FAILURES.
- RANGE REPLICAS CAN BE LOCATED IN DATACENTERS SPANNING INCREASINGLY
  DISPARATE GEOGRAPHIES TO SURVIVE EVER-GREATER FAILURE SCENARIOS FROM
  DATACENTER POWER OR NETWORKING LOSS TO REGIONAL POWER FAILURES
  (E.G. `{ US-EAST-1A, US-EAST-1B, US-EAST-1C }`, `{ US-EAST, US-WEST,
  JAPAN }`, `{ IRELAND, US-EAST, US-WEST}`, `{ IRELAND, US-EAST,
  US-WEST, JAPAN, AUSTRALIA }`).

COCKROACHDB PROVIDES [SNAPSHOT
ISOLATION](HTTP://EN.WIKIPEDIA.ORG/WIKI/SNAPSHOT_ISOLATION) (SI) AND
SERIALIZABLE SNAPSHOT ISOLATION (SSI) SEMANTICS, ALLOWING **EXTERNALLY
CONSISTENT, LOCK-FREE READS AND WRITES**--BOTH FROM A HISTORICAL SNAPSHOT
TIMESTAMP AND FROM THE CURRENT WALL CLOCK TIME. SI PROVIDES LOCK-FREE READS
AND WRITES BUT STILL ALLOWS WRITE SKEW. SSI ELIMINATES WRITE SKEW, BUT
INTRODUCES A PERFORMANCE HIT IN THE CASE OF A CONTENTIOUS SYSTEM. SSI IS THE
DEFAULT ISOLATION; CLIENTS MUST CONSCIOUSLY DECIDE TO TRADE CORRECTNESS FOR
PERFORMANCE. COCKROACHDB IMPLEMENTS [A LIMITED FORM OF LINEARIZABILITY
](#STRICT-SERIALIZABILITY-LINEARIZABILITY), PROVIDING ORDERING FOR ANY
OBSERVER OR CHAIN OF OBSERVERS.

SIMILAR TO
[SPANNER](HTTP://STATIC.GOOGLEUSERCONTENT.COM/MEDIA/RESEARCH.GOOGLE.COM/EN/US/ARCHIVE/SPANNER-OSDI2012.PDF)
DIRECTORIES, COCKROACHDB ALLOWS CONFIGURATION OF ARBITRARY ZONES OF DATA.
THIS ALLOWS REPLICATION FACTOR, STORAGE DEVICE TYPE, AND/OR DATACENTER
LOCATION TO BE CHOSEN TO OPTIMIZE PERFORMANCE AND/OR AVAILABILITY.
UNLIKE SPANNER, ZONES ARE MONOLITHIC AND DON’T ALLOW MOVEMENT OF FINE
GRAINED DATA ON THE LEVEL OF ENTITY GROUPS.

# ARCHITECTURE

COCKROACHDB IMPLEMENTS A LAYERED ARCHITECTURE. THE HIGHEST LEVEL OF
ABSTRACTION IS THE SQL LAYER (CURRENTLY UNSPECIFIED IN THIS DOCUMENT).
IT DEPENDS DIRECTLY ON THE [*SQL LAYER*](#SQL),
WHICH PROVIDES FAMILIAR RELATIONAL CONCEPTS
SUCH AS SCHEMAS, TABLES, COLUMNS, AND INDEXES. THE SQL LAYER
IN TURN DEPENDS ON THE [DISTRIBUTED KEY VALUE STORE](#KEY-VALUE-API),
WHICH HANDLES THE DETAILS OF RANGE ADDRESSING TO PROVIDE THE ABSTRACTION
OF A SINGLE, MONOLITHIC KEY VALUE STORE. THE DISTRIBUTED KV STORE
COMMUNICATES WITH ANY NUMBER OF PHYSICAL COCKROACH NODES. EACH NODE
CONTAINS ONE OR MORE STORES, ONE PER PHYSICAL DEVICE.

![ARCHITECTURE](MEDIA/ARCHITECTURE.PNG)

EACH STORE CONTAINS POTENTIALLY MANY RANGES, THE LOWEST-LEVEL UNIT OF
KEY-VALUE DATA. RANGES ARE REPLICATED USING THE RAFT CONSENSUS PROTOCOL.
THE DIAGRAM BELOW IS A BLOWN UP VERSION OF STORES FROM FOUR OF THE FIVE
NODES IN THE PREVIOUS DIAGRAM. EACH RANGE IS REPLICATED THREE WAYS USING
RAFT. THE COLOR CODING SHOWS ASSOCIATED RANGE REPLICAS.

![RANGES](MEDIA/RANGES.PNG)

EACH PHYSICAL NODE EXPORTS TWO RPC-BASED KEY VALUE APIS: ONE FOR
EXTERNAL CLIENTS AND ONE FOR INTERNAL CLIENTS (EXPOSING SENSITIVE
OPERATIONAL FEATURES). BOTH SERVICES ACCEPT BATCHES OF REQUESTS AND
RETURN BATCHES OF RESPONSES. NODES ARE SYMMETRIC IN CAPABILITIES AND
EXPORTED INTERFACES; EACH HAS THE SAME BINARY AND MAY ASSUME ANY
ROLE.

NODES AND THE RANGES THEY PROVIDE ACCESS TO CAN BE ARRANGED WITH VARIOUS
PHYSICAL NETWORK TOPOLOGIES TO MAKE TRADE OFFS BETWEEN RELIABILITY AND
PERFORMANCE. FOR EXAMPLE, A TRIPLICATED (3-WAY REPLICA) RANGE COULD HAVE
EACH REPLICA LOCATED ON DIFFERENT:

-   DISKS WITHIN A SERVER TO TOLERATE DISK FAILURES.
-   SERVERS WITHIN A RACK TO TOLERATE SERVER FAILURES.
-   SERVERS ON DIFFERENT RACKS WITHIN A DATACENTER TO TOLERATE RACK POWER/NETWORK FAILURES.
-   SERVERS IN DIFFERENT DATACENTERS TO TOLERATE LARGE SCALE NETWORK OR POWER OUTAGES.

UP TO `F` FAILURES CAN BE TOLERATED, WHERE THE TOTAL NUMBER OF REPLICAS `N = 2F + 1` (E.G. WITH 3X REPLICATION, ONE FAILURE CAN BE TOLERATED; WITH 5X REPLICATION, TWO FAILURES, AND SO ON).

# KEYS

COCKROACH KEYS ARE ARBITRARY BYTE ARRAYS. KEYS COME IN TWO FLAVORS:
SYSTEM KEYS AND TABLE DATA KEYS. SYSTEM KEYS ARE USED BY COCKROACH FOR
INTERNAL DATA STRUCTURES AND METADATA. TABLE DATA KEYS CONTAIN SQL
TABLE DATA (AS WELL AS INDEX DATA). SYSTEM AND TABLE DATA KEYS ARE
PREFIXED IN SUCH A WAY THAT ALL SYSTEM KEYS SORT BEFORE ANY TABLE DATA
KEYS.

SYSTEM KEYS COME IN SEVERAL SUBTYPES:

- **GLOBAL** KEYS STORE CLUSTER-WIDE DATA SUCH AS THE "META1" AND
    "META2" KEYS AS WELL AS VARIOUS OTHER SYSTEM-WIDE KEYS SUCH AS THE
    NODE AND STORE ID ALLOCATORS.
- **STORE LOCAL** KEYS ARE USED FOR UNREPLICATED STORE METADATA
    (E.G. THE `STOREIDENT` STRUCTURE). "UNREPLICATED" INDICATES THAT
    THESE VALUES ARE NOT REPLICATED ACROSS MULTIPLE STORES BECAUSE THE
    DATA THEY HOLD IS TIED TO THE LIFETIME OF THE STORE THEY ARE
    PRESENT ON.
- **RANGE LOCAL** KEYS STORE RANGE METADATA THAT IS ASSOCIATED WITH A
    GLOBAL KEY. RANGE LOCAL KEYS HAVE A SPECIAL PREFIX FOLLOWED BY A
    GLOBAL KEY AND A SPECIAL SUFFIX. FOR EXAMPLE, TRANSACTION RECORDS
    ARE RANGE LOCAL KEYS WHICH LOOK LIKE:
    `\X01K<GLOBAL-KEY>TXN-<TXNID>`.
- **REPLICATED RANGE ID LOCAL** KEYS STORE RANGE METADATA THAT IS
    PRESENT ON ALL OF THE REPLICAS FOR A RANGE. THESE KEYS ARE UPDATED
    VIA RAFT OPERATIONS. EXAMPLES INCLUDE THE RANGE LEASE STATE AND
    ABORT SPAN ENTRIES.
- **UNREPLICATED RANGE ID LOCAL** KEYS STORE RANGE METADATA THAT IS
    LOCAL TO A REPLICA. THE PRIMARY EXAMPLES OF SUCH KEYS ARE THE RAFT
    STATE AND RAFT LOG.

TABLE DATA KEYS ARE USED TO STORE ALL SQL DATA. TABLE DATA KEYS
CONTAIN INTERNAL STRUCTURE AS DESCRIBED IN THE SECTION ON [MAPPING
DATA BETWEEN THE SQL MODEL AND
KV](#DATA-MAPPING-BETWEEN-THE-SQL-MODEL-AND-KV).

# VERSIONED VALUES

COCKROACH MAINTAINS HISTORICAL VERSIONS OF VALUES BY STORING THEM WITH
ASSOCIATED COMMIT TIMESTAMPS. READS AND SCANS CAN SPECIFY A SNAPSHOT
TIME TO RETURN THE MOST RECENT WRITES PRIOR TO THE SNAPSHOT TIMESTAMP.
OLDER VERSIONS OF VALUES ARE GARBAGE COLLECTED BY THE SYSTEM DURING
COMPACTION ACCORDING TO A USER-SPECIFIED EXPIRATION INTERVAL. IN ORDER
TO SUPPORT LONG-RUNNING SCANS (E.G. FOR MAPREDUCE), ALL VERSIONS HAVE A
MINIMUM EXPIRATION.

VERSIONED VALUES ARE SUPPORTED VIA MODIFICATIONS TO ROCKSDB TO RECORD
COMMIT TIMESTAMPS AND GC EXPIRATIONS PER KEY.

# LOCK-FREE DISTRIBUTED TRANSACTIONS

COCKROACH PROVIDES DISTRIBUTED TRANSACTIONS WITHOUT LOCKS. COCKROACH
TRANSACTIONS SUPPORT TWO ISOLATION LEVELS:

- SNAPSHOT ISOLATION (SI) AND
- *SERIALIZABLE* SNAPSHOT ISOLATION (SSI).

*SI* IS SIMPLE TO IMPLEMENT, HIGHLY PERFORMANT, AND CORRECT FOR ALL BUT A
HANDFUL OF ANOMALOUS CONDITIONS (E.G. WRITE SKEW). *SSI* REQUIRES JUST A TOUCH
MORE COMPLEXITY, IS STILL HIGHLY PERFORMANT (LESS SO WITH CONTENTION), AND HAS
NO ANOMALOUS CONDITIONS. COCKROACH’S SSI IMPLEMENTATION IS BASED ON IDEAS FROM
THE LITERATURE AND SOME POSSIBLY NOVEL INSIGHTS.

SSI IS THE DEFAULT LEVEL, WITH SI PROVIDED FOR APPLICATION DEVELOPERS
WHO ARE CERTAIN ENOUGH OF THEIR NEED FOR PERFORMANCE AND THE ABSENCE OF
WRITE SKEW CONDITIONS TO CONSCIOUSLY ELECT TO USE IT. IN A LIGHTLY
CONTENDED SYSTEM, OUR IMPLEMENTATION OF SSI IS JUST AS PERFORMANT AS SI,
REQUIRING NO LOCKING OR ADDITIONAL WRITES. WITH CONTENTION, OUR
IMPLEMENTATION OF SSI STILL REQUIRES NO LOCKING, BUT WILL END UP
ABORTING MORE TRANSACTIONS. COCKROACH’S SI AND SSI IMPLEMENTATIONS
PREVENT STARVATION SCENARIOS EVEN FOR ARBITRARILY LONG TRANSACTIONS.

SEE THE [CAHILL PAPER](HTTPS://SES.LIBRARY.USYD.EDU.AU/BITSTREAM/HANDLE/2123/5353/MICHAEL-CAHILL-2009-THESIS.PDF)
FOR ONE POSSIBLE IMPLEMENTATION OF SSI. THIS IS ANOTHER [GREAT PAPER](HTTP://CS.YALE.EDU/HOMES/THOMSON/PUBLICATIONS/CALVIN-SIGMOD12.PDF).
FOR A DISCUSSION OF SSI IMPLEMENTED BY PREVENTING READ-WRITE CONFLICTS
(IN CONTRAST TO DETECTING THEM, CALLED WRITE-SNAPSHOT ISOLATION), SEE
THE [YABANDEH PAPER](HTTPS://COURSES.CS.WASHINGTON.EDU/COURSES/CSE444/10SP/544M/READING-LIST/FEKETE-SIGMOD2008.PDF),
WHICH IS THE SOURCE OF MUCH INSPIRATION FOR COCKROACH’S SSI.

BOTH SI AND SSI REQUIRE THAT THE OUTCOME OF READS MUST BE PRESERVED, I.E.
A WRITE OF A KEY AT A LOWER TIMESTAMP THAN A PREVIOUS READ MUST NOT SUCCEED. TO
THIS END, EACH RANGE MAINTAINS A BOUNDED *IN-MEMORY* CACHE FROM KEY RANGE TO
THE LATEST TIMESTAMP AT WHICH IT WAS READ.

MOST UPDATES TO THIS *TIMESTAMP CACHE* CORRESPOND TO KEYS BEING READ, THOUGH
THE TIMESTAMP CACHE ALSO PROTECTS THE OUTCOME OF SOME WRITES (NOTABLY RANGE
DELETIONS) WHICH CONSEQUENTLY MUST ALSO POPULATE THE CACHE. THE CACHE’S ENTRIES
ARE EVICTED OLDEST TIMESTAMP FIRST, UPDATING THE LOW WATER MARK OF THE CACHE
APPROPRIATELY.

EACH COCKROACH TRANSACTION IS ASSIGNED A RANDOM PRIORITY AND A
"CANDIDATE TIMESTAMP" AT START. THE CANDIDATE TIMESTAMP IS THE
PROVISIONAL TIMESTAMP AT WHICH THE TRANSACTION WILL COMMIT, AND IS
CHOSEN AS THE CURRENT CLOCK TIME OF THE NODE COORDINATING THE
TRANSACTION. THIS MEANS THAT A TRANSACTION WITHOUT CONFLICTS WILL
USUALLY COMMIT WITH A TIMESTAMP THAT, IN ABSOLUTE TIME, PRECEDES THE
ACTUAL WORK DONE BY THAT TRANSACTION.

IN THE COURSE OF COORDINATING A TRANSACTION BETWEEN ONE OR MORE
DISTRIBUTED NODES, THE CANDIDATE TIMESTAMP MAY BE INCREASED, BUT WILL
NEVER BE DECREASED. THE CORE DIFFERENCE BETWEEN THE TWO ISOLATION LEVELS
SI AND SSI IS THAT THE FORMER ALLOWS THE TRANSACTION'S CANDIDATE
TIMESTAMP TO INCREASE AND THE LATTER DOES NOT.

**HYBRID LOGICAL CLOCK**

EACH COCKROACH NODE MAINTAINS A HYBRID LOGICAL CLOCK (HLC) AS DISCUSSED
IN THE [HYBRID LOGICAL CLOCK PAPER](HTTP://WWW.CSE.BUFFALO.EDU/TECH-REPORTS/2014-04.PDF).
HLC TIME USES TIMESTAMPS WHICH ARE COMPOSED OF A PHYSICAL COMPONENT (THOUGHT OF
AS AND ALWAYS CLOSE TO LOCAL WALL TIME) AND A LOGICAL COMPONENT (USED TO
DISTINGUISH BETWEEN EVENTS WITH THE SAME PHYSICAL COMPONENT). IT ALLOWS US TO
TRACK CAUSALITY FOR RELATED EVENTS SIMILAR TO VECTOR CLOCKS, BUT WITH LESS
OVERHEAD. IN PRACTICE, IT WORKS MUCH LIKE OTHER LOGICAL CLOCKS: WHEN EVENTS
ARE RECEIVED BY A NODE, IT INFORMS THE LOCAL HLC ABOUT THE TIMESTAMP SUPPLIED
WITH THE EVENT BY THE SENDER, AND WHEN EVENTS ARE SENT A TIMESTAMP GENERATED BY
THE LOCAL HLC IS ATTACHED.

FOR A MORE IN DEPTH DESCRIPTION OF HLC PLEASE READ THE PAPER. OUR
IMPLEMENTATION IS [HERE](HTTPS://GITHUB.COM/COCKROACHDB/COCKROACH/BLOB/MASTER/PKG/UTIL/HLC/HLC.GO).

COCKROACH PICKS A TIMESTAMP FOR A TRANSACTION USING HLC TIME. THROUGHOUT THIS
DOCUMENT, *TIMESTAMP* ALWAYS REFERS TO THE HLC TIME WHICH IS A SINGLETON
ON EACH NODE. THE HLC IS UPDATED BY EVERY READ/WRITE EVENT ON THE NODE, AND
THE HLC TIME >= WALL TIME. A READ/WRITE TIMESTAMP RECEIVED IN A COCKROACH REQUEST
FROM ANOTHER NODE IS NOT ONLY USED TO VERSION THE OPERATION, BUT ALSO UPDATES
THE HLC ON THE NODE. THIS IS USEFUL IN GUARANTEEING THAT ALL DATA READ/WRITTEN
ON A NODE IS AT A TIMESTAMP < NEXT HLC TIME.

**TRANSACTION EXECUTION FLOW**

TRANSACTIONS ARE EXECUTED IN TWO PHASES:

1. START THE TRANSACTION BY SELECTING A RANGE WHICH IS LIKELY TO BE
   HEAVILY INVOLVED IN THE TRANSACTION AND WRITING A NEW TRANSACTION
   RECORD TO A RESERVED AREA OF THAT RANGE WITH STATE "PENDING". IN
   PARALLEL WRITE AN "INTENT" VALUE FOR EACH DATUM BEING WRITTEN AS PART
   OF THE TRANSACTION. THESE ARE NORMAL MVCC VALUES, WITH THE ADDITION OF
   A SPECIAL FLAG (I.E. “INTENT”) INDICATING THAT THE VALUE MAY BE
   COMMITTED AFTER THE TRANSACTION ITSELF COMMITS. IN ADDITION,
   THE TRANSACTION ID (UNIQUE AND CHOSEN AT TXN START TIME BY CLIENT)
   IS STORED WITH INTENT VALUES. THE TXN ID IS USED TO REFER TO THE
   TRANSACTION RECORD WHEN THERE ARE CONFLICTS AND TO MAKE
   TIE-BREAKING DECISIONS ON ORDERING BETWEEN IDENTICAL TIMESTAMPS.
   EACH NODE RETURNS THE TIMESTAMP USED FOR THE WRITE (WHICH IS THE
   ORIGINAL CANDIDATE TIMESTAMP IN THE ABSENCE OF READ/WRITE CONFLICTS);
   THE CLIENT SELECTS THE MAXIMUM FROM AMONGST ALL WRITE TIMESTAMPS AS THE
   FINAL COMMIT TIMESTAMP.

2. COMMIT THE TRANSACTION BY UPDATING ITS TRANSACTION RECORD. THE VALUE
   OF THE COMMIT ENTRY CONTAINS THE CANDIDATE TIMESTAMP (INCREASED AS
   NECESSARY TO ACCOMMODATE ANY LATEST READ TIMESTAMPS). NOTE THAT THE
   TRANSACTION IS CONSIDERED FULLY COMMITTED AT THIS POINT AND CONTROL
   MAY BE RETURNED TO THE CLIENT.

   IN THE CASE OF AN SI TRANSACTION, A COMMIT TIMESTAMP WHICH WAS
   INCREASED TO ACCOMMODATE CONCURRENT READERS IS PERFECTLY
   ACCEPTABLE AND THE COMMIT MAY CONTINUE. FOR SSI TRANSACTIONS,
   HOWEVER, A GAP BETWEEN CANDIDATE AND COMMIT TIMESTAMPS
   NECESSITATES TRANSACTION RESTART (NOTE: RESTART IS DIFFERENT THAN
   ABORT--SEE BELOW).

   AFTER THE TRANSACTION IS COMMITTED, ALL WRITTEN INTENTS ARE UPGRADED
   IN PARALLEL BY REMOVING THE “INTENT” FLAG. THE TRANSACTION IS
   CONSIDERED FULLY COMMITTED BEFORE THIS STEP AND DOES NOT WAIT FOR
   IT TO RETURN CONTROL TO THE TRANSACTION COORDINATOR.

IN THE ABSENCE OF CONFLICTS, THIS IS THE END. NOTHING ELSE IS NECESSARY
TO ENSURE THE CORRECTNESS OF THE SYSTEM.

**CONFLICT RESOLUTION**

THINGS GET MORE INTERESTING WHEN A READER OR WRITER ENCOUNTERS AN INTENT
RECORD OR NEWLY-COMMITTED VALUE IN A LOCATION THAT IT NEEDS TO READ OR
WRITE. THIS IS A CONFLICT, USUALLY CAUSING EITHER OF THE TRANSACTIONS TO
ABORT OR RESTART DEPENDING ON THE TYPE OF CONFLICT.

***TRANSACTION RESTART:***

THIS IS THE USUAL (AND MORE EFFICIENT) TYPE OF BEHAVIOUR AND IS USED
EXCEPT WHEN THE TRANSACTION WAS ABORTED (FOR INSTANCE BY ANOTHER
TRANSACTION).
IN EFFECT, THAT REDUCES TO TWO CASES; THE FIRST BEING THE ONE OUTLINED
ABOVE: AN SSI TRANSACTION THAT FINDS UPON ATTEMPTING TO COMMIT THAT
ITS COMMIT TIMESTAMP HAS BEEN PUSHED. THE SECOND CASE INVOLVES A TRANSACTION
ACTIVELY ENCOUNTERING A CONFLICT, THAT IS, ONE OF ITS READERS OR WRITERS
ENCOUNTER DATA THAT NECESSITATE CONFLICT RESOLUTION
(SEE TRANSACTION INTERACTIONS BELOW).

WHEN A TRANSACTION RESTARTS, IT CHANGES ITS PRIORITY AND/OR MOVES ITS
TIMESTAMP FORWARD DEPENDING ON DATA TIED TO THE CONFLICT, AND
BEGINS ANEW REUSING THE SAME TXN ID. THE PRIOR RUN OF THE TRANSACTION MIGHT
HAVE WRITTEN SOME WRITE INTENTS, WHICH NEED TO BE DELETED BEFORE THE
TRANSACTION COMMITS, SO AS TO NOT BE INCLUDED AS PART OF THE TRANSACTION.
THESE STALE WRITE INTENT DELETIONS ARE DONE DURING THE REEXECUTION OF THE
TRANSACTION, EITHER IMPLICITLY, THROUGH WRITING NEW INTENTS TO
THE SAME KEYS AS PART OF THE REEXECUTION OF THE TRANSACTION, OR EXPLICITLY,
BY CLEANING UP STALE INTENTS THAT ARE NOT PART OF THE REEXECUTION OF THE
TRANSACTION. SINCE MOST TRANSACTIONS WILL END UP WRITING TO THE SAME KEYS,
THE EXPLICIT CLEANUP RUN JUST BEFORE COMMITTING THE TRANSACTION IS USUALLY
A NOOP.

***TRANSACTION ABORT:***

THIS IS THE CASE IN WHICH A TRANSACTION, UPON READING ITS TRANSACTION
RECORD, FINDS THAT IT HAS BEEN ABORTED. IN THIS CASE, THE TRANSACTION
CAN NOT REUSE ITS INTENTS; IT RETURNS CONTROL TO THE CLIENT BEFORE
CLEANING THEM UP (OTHER READERS AND WRITERS WOULD CLEAN UP DANGLING
INTENTS AS THEY ENCOUNTER THEM) BUT WILL MAKE AN EFFORT TO CLEAN UP
AFTER ITSELF. THE NEXT ATTEMPT (IF APPLICABLE) THEN RUNS AS A NEW
TRANSACTION WITH **A NEW TXN ID**.

***TRANSACTION INTERACTIONS:***

THERE ARE SEVERAL SCENARIOS IN WHICH TRANSACTIONS INTERACT:

- **READER ENCOUNTERS WRITE INTENT OR VALUE WITH NEWER TIMESTAMP FAR
  ENOUGH IN THE FUTURE**: THIS IS NOT A CONFLICT. THE READER IS FREE
  TO PROCEED; AFTER ALL, IT WILL BE READING AN OLDER VERSION OF THE
  VALUE AND SO DOES NOT CONFLICT. RECALL THAT THE WRITE INTENT MAY
  BE COMMITTED WITH A LATER TIMESTAMP THAN ITS CANDIDATE; IT WILL
  NEVER COMMIT WITH AN EARLIER ONE. 

- **READER ENCOUNTERS WRITE INTENT OR VALUE WITH NEWER TIMESTAMP IN THE
  NEAR FUTURE:** IN THIS CASE, WE HAVE TO BE CAREFUL. THE NEWER
  INTENT MAY, IN ABSOLUTE TERMS, HAVE HAPPENED IN OUR READ'S PAST IF
  THE CLOCK OF THE WRITER IS AHEAD OF THE NODE SERVING THE VALUES.
  IN THAT CASE, WE WOULD NEED TO TAKE THIS VALUE INTO ACCOUNT, BUT
  WE JUST DON'T KNOW. HENCE THE TRANSACTION RESTARTS, USING INSTEAD
  A FUTURE TIMESTAMP (BUT REMEMBERING A MAXIMUM TIMESTAMP USED TO
  LIMIT THE UNCERTAINTY WINDOW TO THE MAXIMUM CLOCK OFFSET). IN FACT,
  THIS IS OPTIMIZED FURTHER; SEE THE DETAILS UNDER "CHOOSING A TIME
  STAMP" BELOW.

- **READER ENCOUNTERS WRITE INTENT WITH OLDER TIMESTAMP**: THE READER
  MUST FOLLOW THE INTENT’S TRANSACTION ID TO THE TRANSACTION RECORD.
  IF THE TRANSACTION HAS ALREADY BEEN COMMITTED, THEN THE READER CAN
  JUST READ THE VALUE. IF THE WRITE TRANSACTION HAS NOT YET BEEN
  COMMITTED, THEN THE READER HAS TWO OPTIONS. IF THE WRITE CONFLICT
  IS FROM AN SI TRANSACTION, THE READER CAN *PUSH THAT TRANSACTION'S
  COMMIT TIMESTAMP INTO THE FUTURE* (AND CONSEQUENTLY NOT HAVE TO
  READ IT). THIS IS SIMPLE TO DO: THE READER JUST UPDATES THE
  TRANSACTION’S COMMIT TIMESTAMP TO INDICATE THAT WHEN/IF THE
  TRANSACTION DOES COMMIT, IT SHOULD USE A TIMESTAMP *AT LEAST* AS
  HIGH. HOWEVER, IF THE WRITE CONFLICT IS FROM AN SSI TRANSACTION,
  THE READER MUST COMPARE PRIORITIES. IF THE READER HAS THE HIGHER PRIORITY,
  IT PUSHES THE TRANSACTION’S COMMIT TIMESTAMP (THAT
  TRANSACTION WILL THEN NOTICE ITS TIMESTAMP HAS BEEN PUSHED, AND
  RESTART). IF IT HAS THE LOWER OR SAME PRIORITY, IT RETRIES ITSELF USING AS
  A NEW PRIORITY `MAX(NEW RANDOM PRIORITY, CONFLICTING TXN’S
  PRIORITY - 1)`.

- **WRITER ENCOUNTERS UNCOMMITTED WRITE INTENT**:
  IF THE OTHER WRITE INTENT HAS BEEN WRITTEN BY A TRANSACTION WITH A LOWER
  PRIORITY, THE WRITER ABORTS THE CONFLICTING TRANSACTION. IF THE WRITE
  INTENT HAS A HIGHER OR EQUAL PRIORITY THE TRANSACTION RETRIES, USING AS A NEW
  PRIORITY *MAX(NEW RANDOM PRIORITY, CONFLICTING TXN’S PRIORITY - 1)*;
  THE RETRY OCCURS AFTER A SHORT, RANDOMIZED BACKOFF INTERVAL.

- **WRITER ENCOUNTERS NEWER COMMITTED VALUE**:
  THE COMMITTED VALUE COULD ALSO BE AN UNRESOLVED WRITE INTENT MADE BY A
  TRANSACTION THAT HAS ALREADY COMMITTED. THE TRANSACTION RESTARTS. ON RESTART,
  THE SAME PRIORITY IS REUSED, BUT THE CANDIDATE TIMESTAMP IS MOVED FORWARD
  TO THE ENCOUNTERED VALUE'S TIMESTAMP.

- **WRITER ENCOUNTERS MORE RECENTLY READ KEY**:
  THE *READ TIMESTAMP CACHE* IS CONSULTED ON EACH WRITE AT A NODE. IF THE WRITE’S
  CANDIDATE TIMESTAMP IS EARLIER THAN THE LOW WATER MARK ON THE CACHE ITSELF
  (I.E. ITS LAST EVICTED TIMESTAMP) OR IF THE KEY BEING WRITTEN HAS A READ
  TIMESTAMP LATER THAN THE WRITE’S CANDIDATE TIMESTAMP, THIS LATER TIMESTAMP
  VALUE IS RETURNED WITH THE WRITE. A NEW TIMESTAMP FORCES A TRANSACTION
  RESTART ONLY IF IT IS SERIALIZABLE.

**TRANSACTION MANAGEMENT**

TRANSACTIONS ARE MANAGED BY THE CLIENT PROXY (OR GATEWAY IN SQL AZURE
PARLANCE). UNLIKE IN SPANNER, WRITES ARE NOT BUFFERED BUT ARE SENT
DIRECTLY TO ALL IMPLICATED RANGES. THIS ALLOWS THE TRANSACTION TO ABORT
QUICKLY IF IT ENCOUNTERS A WRITE CONFLICT. THE CLIENT PROXY KEEPS TRACK
OF ALL WRITTEN KEYS IN ORDER TO RESOLVE WRITE INTENTS ASYNCHRONOUSLY UPON
TRANSACTION COMPLETION. IF A TRANSACTION COMMITS SUCCESSFULLY, ALL INTENTS
ARE UPGRADED TO COMMITTED. IN THE EVENT A TRANSACTION IS ABORTED, ALL WRITTEN
INTENTS ARE DELETED. THE CLIENT PROXY DOESN’T GUARANTEE IT WILL RESOLVE INTENTS.

IN THE EVENT THE CLIENT PROXY RESTARTS BEFORE THE PENDING TRANSACTION IS
COMMITTED, THE DANGLING TRANSACTION WOULD CONTINUE TO "LIVE" UNTIL
ABORTED BY ANOTHER TRANSACTION. TRANSACTIONS PERIODICALLY HEARTBEAT
THEIR TRANSACTION RECORD TO MAINTAIN LIVENESS.
TRANSACTIONS ENCOUNTERED BY READERS OR WRITERS WITH DANGLING INTENTS
WHICH HAVEN’T BEEN HEARTBEAT WITHIN THE REQUIRED INTERVAL ARE ABORTED.
IN THE EVENT THE PROXY RESTARTS AFTER A TRANSACTION COMMITS BUT BEFORE
THE ASYNCHRONOUS RESOLUTION IS COMPLETE, THE DANGLING INTENTS ARE UPGRADED
WHEN ENCOUNTERED BY FUTURE READERS AND WRITERS AND THE SYSTEM DOES
NOT DEPEND ON THEIR TIMELY RESOLUTION FOR CORRECTNESS.

AN EXPLORATION OF RETRIES WITH CONTENTION AND ABORT TIMES WITH ABANDONED
TRANSACTION IS
[HERE](HTTPS://DOCS.GOOGLE.COM/DOCUMENT/D/1KBCU4SDGANVLQPT-_2VATBOMNMX3_SAAYWEGYU1J7MQ/EDIT?USP=SHARING).

**TRANSACTION RECORDS**

PLEASE SEE [PKG/ROACHPB/DATA.PROTO](HTTPS://GITHUB.COM/COCKROACHDB/COCKROACH/BLOB/MASTER/PKG/ROACHPB/DATA.PROTO) FOR THE UP-TO-DATE STRUCTURES, THE BEST ENTRY POINT BEING `MESSAGE TRANSACTION`.

**PROS**

- NO REQUIREMENT FOR RELIABLE CODE EXECUTION TO PREVENT STALLED 2PC
  PROTOCOL.
- READERS NEVER BLOCK WITH SI SEMANTICS; WITH SSI SEMANTICS, THEY MAY
  ABORT.
- LOWER LATENCY THAN TRADITIONAL 2PC COMMIT PROTOCOL (W/O CONTENTION)
  BECAUSE SECOND PHASE REQUIRES ONLY A SINGLE WRITE TO THE
  TRANSACTION RECORD INSTEAD OF A SYNCHRONOUS ROUND TO ALL
  TRANSACTION PARTICIPANTS.
- PRIORITIES AVOID STARVATION FOR ARBITRARILY LONG TRANSACTIONS AND
  ALWAYS PICK A WINNER FROM BETWEEN CONTENDING TRANSACTIONS (NO
  MUTUAL ABORTS).
- WRITES NOT BUFFERED AT CLIENT; WRITES FAIL FAST.
- NO READ-LOCKING OVERHEAD REQUIRED FOR *SERIALIZABLE* SI (IN CONTRAST
  TO OTHER SSI IMPLEMENTATIONS).
- WELL-CHOSEN (I.E. LESS RANDOM) PRIORITIES CAN FLEXIBLY GIVE
  PROBABILISTIC GUARANTEES ON LATENCY FOR ARBITRARY TRANSACTIONS
  (FOR EXAMPLE: MAKE OLTP TRANSACTIONS 10X LESS LIKELY TO ABORT THAN
  LOW PRIORITY TRANSACTIONS, SUCH AS ASYNCHRONOUSLY SCHEDULED JOBS).

**CONS**

- READS FROM NON-LEASE HOLDER REPLICAS STILL REQUIRE A PING TO THE LEASE HOLDER
  TO UPDATE THE *READ TIMESTAMP CACHE*.
- ABANDONED TRANSACTIONS MAY BLOCK CONTENDING WRITERS FOR UP TO THE
  HEARTBEAT INTERVAL, THOUGH AVERAGE WAIT IS LIKELY TO BE
  CONSIDERABLY SHORTER (SEE [GRAPH IN LINK](HTTPS://DOCS.GOOGLE.COM/DOCUMENT/D/1KBCU4SDGANVLQPT-_2VATBOMNMX3_SAAYWEGYU1J7MQ/EDIT?USP=SHARING)).
  THIS IS LIKELY CONSIDERABLY MORE PERFORMANT THAN DETECTING AND
  RESTARTING 2PC IN ORDER TO RELEASE READ AND WRITE LOCKS.
- BEHAVIOR DIFFERENT THAN OTHER SI IMPLEMENTATIONS: NO FIRST WRITER
  WINS, AND SHORTER TRANSACTIONS DO NOT ALWAYS FINISH QUICKLY.
  ELEMENT OF SURPRISE FOR OLTP SYSTEMS MAY BE A PROBLEMATIC FACTOR.
- ABORTS CAN DECREASE THROUGHPUT IN A CONTENDED SYSTEM COMPARED WITH
  TWO PHASE LOCKING. ABORTS AND RETRIES INCREASE READ AND WRITE
  TRAFFIC, INCREASE LATENCY AND DECREASE THROUGHPUT.

**CHOOSING A TIMESTAMP**

A KEY CHALLENGE OF READING DATA IN A DISTRIBUTED SYSTEM WITH CLOCK OFFSET
IS CHOOSING A TIMESTAMP GUARANTEED TO BE GREATER THAN THE LATEST
TIMESTAMP OF ANY COMMITTED TRANSACTION (IN ABSOLUTE TIME). NO SYSTEM CAN
CLAIM CONSISTENCY AND FAIL TO READ ALREADY-COMMITTED DATA.

ACCOMPLISHING CONSISTENCY FOR TRANSACTIONS (OR JUST SINGLE OPERATIONS)
ACCESSING A SINGLE NODE IS EASY. THE TIMESTAMP IS ASSIGNED BY THE NODE
ITSELF, SO IT IS GUARANTEED TO BE AT A GREATER TIMESTAMP THAN ALL THE
EXISTING TIMESTAMPED DATA ON THE NODE.

FOR MULTIPLE NODES, THE TIMESTAMP OF THE NODE COORDINATING THE
TRANSACTION `T` IS USED. IN ADDITION, A MAXIMUM TIMESTAMP `T+Ε` IS
SUPPLIED TO PROVIDE AN UPPER BOUND ON TIMESTAMPS FOR ALREADY-COMMITTED
DATA (`Ε` IS THE MAXIMUM CLOCK OFFSET). AS THE TRANSACTION PROGRESSES, ANY
DATA READ WHICH HAVE TIMESTAMPS GREATER THAN `T` BUT LESS THAN `T+Ε`
CAUSE THE TRANSACTION TO ABORT AND RETRY WITH THE CONFLICTING TIMESTAMP
T<SUB>C</SUB>, WHERE T<SUB>C</SUB> \> T. THE MAXIMUM TIMESTAMP `T+Ε` REMAINS
THE SAME. THIS IMPLIES THAT TRANSACTION RESTARTS DUE TO CLOCK UNCERTAINTY
CAN ONLY HAPPEN ON A TIME INTERVAL OF LENGTH `Ε`.

WE APPLY ANOTHER OPTIMIZATION TO REDUCE THE RESTARTS CAUSED
BY UNCERTAINTY. UPON RESTARTING, THE TRANSACTION NOT ONLY TAKES
INTO ACCOUNT T<SUB>C</SUB>, BUT THE TIMESTAMP OF THE NODE AT THE TIME
OF THE UNCERTAIN READ T<SUB>NODE</SUB>. THE LARGER OF THOSE TWO TIMESTAMPS
T<SUB>C</SUB> AND T<SUB>NODE</SUB> (LIKELY EQUAL TO THE LATTER) IS USED
TO INCREASE THE READ TIMESTAMP. ADDITIONALLY, THE CONFLICTING NODE IS
MARKED AS “CERTAIN”. THEN, FOR FUTURE READS TO THAT NODE WITHIN THE
TRANSACTION, WE SET `MAXTIMESTAMP = READ TIMESTAMP`, PREVENTING FURTHER
UNCERTAINTY RESTARTS.

CORRECTNESS FOLLOWS FROM THE FACT THAT WE KNOW THAT AT THE TIME OF THE READ,
THERE EXISTS NO VERSION OF ANY KEY ON THAT NODE WITH A HIGHER TIMESTAMP THAN
T<SUB>NODE</SUB>. UPON A RESTART CAUSED BY THE NODE, IF THE TRANSACTION
ENCOUNTERS A KEY WITH A HIGHER TIMESTAMP, IT KNOWS THAT IN ABSOLUTE TIME,
THE VALUE WAS WRITTEN AFTER T<SUB>NODE</SUB> WAS OBTAINED, I.E. AFTER THE
UNCERTAIN READ. HENCE THE TRANSACTION CAN MOVE FORWARD READING AN OLDER VERSION
OF THE DATA (AT THE TRANSACTION'S TIMESTAMP). THIS LIMITS THE TIME UNCERTAINTY
RESTARTS ATTRIBUTED TO A NODE TO AT MOST ONE. THE TRADEOFF IS THAT WE MIGHT
PICK A TIMESTAMP LARGER THAN THE OPTIMAL ONE (> HIGHEST CONFLICTING TIMESTAMP),
RESULTING IN THE POSSIBILITY OF A FEW MORE CONFLICTS.

WE EXPECT RETRIES WILL BE RARE, BUT THIS ASSUMPTION MAY NEED TO BE
REVISITED IF RETRIES BECOME PROBLEMATIC. NOTE THAT THIS PROBLEM DOES NOT
APPLY TO HISTORICAL READS. AN ALTERNATE APPROACH WHICH DOES NOT REQUIRE
RETRIES MAKES A ROUND TO ALL NODE PARTICIPANTS IN ADVANCE AND
CHOOSES THE HIGHEST REPORTED NODE WALL TIME AS THE TIMESTAMP. HOWEVER,
KNOWING WHICH NODES WILL BE ACCESSED IN ADVANCE IS DIFFICULT AND
POTENTIALLY LIMITING. COCKROACH COULD ALSO POTENTIALLY USE A GLOBAL
CLOCK (GOOGLE DID THIS WITH [PERCOLATOR](HTTPS://WWW.USENIX.ORG/LEGACY/EVENT/OSDI10/TECH/FULL_PAPERS/PENG.PDF)),
WHICH WOULD BE FEASIBLE FOR SMALLER, GEOGRAPHICALLY-PROXIMATE CLUSTERS.

# STRICT SERIALIZABILITY (LINEARIZABILITY)

ROUGHLY SPEAKING, THE GAP BETWEEN <I>STRICT SERIALIZABILITY</I> (WHICH WE USE
INTERCHANGEABLY WITH <I>LINEARIZABILITY</I>) AND COCKROACHDB'S DEFAULT
ISOLATION LEVEL (<I>SERIALIZABLE</I>) IS THAT WITH LINEARIZABLE TRANSACTIONS,
CAUSALITY IS PRESERVED. THAT IS, IF ONE TRANSACTION (SAY, CREATING A POSTING
FOR A USER) WAITS FOR ITS PREDECESSOR (CREATING THE USER IN THE FIRST PLACE)
TO COMPLETE, ONE WOULD HOPE THAT THE LOGICAL TIMESTAMP ASSIGNED TO THE FORMER
IS LARGER THAN THAT OF THE LATTER.
IN PRACTICE, IN DISTRIBUTED DATABASES THIS MAY NOT HOLD, THE REASON TYPICALLY
BEING THAT CLOCKS ACROSS A DISTRIBUTED SYSTEM ARE NOT PERFECTLY SYNCHRONIZED
AND THE "LATER" TRANSACTION TOUCHES A PART DISJOINT FROM THAT ON WHICH THE
FIRST TRANSACTION RAN, RESULTING IN CLOCKS WITH DISJOINT INFORMATION TO DECIDE
ON THE COMMIT TIMESTAMPS.

IN PRACTICE, IN COCKROACHDB MANY TRANSACTIONAL WORKLOADS ARE ACTUALLY
LINEARIZABLE, THOUGH THE PRECISE CONDITIONS ARE TOO INVOLVED TO OUTLINE THEM
HERE.

CAUSALITY IS TYPICALLY NOT REQUIRED FOR MANY TRANSACTIONS, AND SO IT IS
ADVANTAGEOUS TO PAY FOR IT ONLY WHEN IT *IS* NEEDED. COCKROACHDB IMPLEMENTS
THIS VIA <I>CAUSALITY TOKENS</I>: WHEN COMMITTING A TRANSACTION, A CAUSALITY
TOKEN CAN BE RETRIEVED AND PASSED TO THE NEXT TRANSACTION, ENSURING THAT THESE
TWO TRANSACTIONS GET ASSIGNED INCREASING LOGICAL TIMESTAMPS.

ADDITIONALLY, AS BETTER SYNCHRONIZED CLOCKS BECOME A STANDARD COMMODITY OFFERED
BY CLOUD PROVIDERS, COCKROACHDB CAN PROVIDE GLOBAL LINEARIZABILITY BY DOING
MUCH THE SAME THAT [GOOGLE'S
SPANNER](HTTP://RESEARCH.GOOGLE.COM/ARCHIVE/SPANNER.HTML) DOES: WAIT OUT THE
MAXIMUM CLOCK OFFSET AFTER COMMITTING, BUT BEFORE RETURNING TO THE CLIENT.

SEE THE BLOG POST BELOW FOR MUCH MORE IN-DEPTH INFORMATION.

HTTPS://WWW.COCKROACHLABS.COM/BLOG/LIVING-WITHOUT-ATOMIC-CLOCKS/

# LOGICAL MAP CONTENT

LOGICALLY, THE MAP CONTAINS A SERIES OF RESERVED SYSTEM KEY/VALUE
PAIRS PRECEDING THE ACTUAL USER DATA (WHICH IS MANAGED BY THE SQL
SUBSYSTEM).

- `\X02<KEY1>`: RANGE METADATA FOR RANGE ENDING `\X03<KEY1>`. THIS A "META1" KEY.
- ...
- `\X02<KEYN>`: RANGE METADATA FOR RANGE ENDING `\X03<KEYN>`. THIS A "META1" KEY.
- `\X03<KEY1>`: RANGE METADATA FOR RANGE ENDING `<KEY1>`. THIS A "META2" KEY.
- ...
- `\X03<KEYN>`: RANGE METADATA FOR RANGE ENDING `<KEYN>`. THIS A "META2" KEY.
- `\X04{DESC,NODE,RANGE,STORE}-IDEGEN`: ID GENERATION ORACLES FOR VARIOUS COMPONENT TYPES.
- `\X04STATUS-NODE-<VARINT ENCODED STORE ID>`: STORE RUNTIME METADATA.
- `\X04TSD<KEY>`: TIME-SERIES DATA KEY.
- `<KEY>`: A USER KEY. IN PRACTICE, THESE KEYS ARE MANAGED BY THE SQL
  SUBSYSTEM, WHICH EMPLOYS ITS OWN KEY ANATOMY.

# STORES AND STORAGE

NODES CONTAIN ONE OR MORE STORES. EACH STORE SHOULD BE PLACED ON A UNIQUE DISK.
INTERNALLY, EACH STORE CONTAINS A SINGLE INSTANCE OF ROCKSDB WITH A BLOCK CACHE
SHARED AMONGST ALL OF THE STORES IN A NODE. AND THESE STORES IN TURN HAVE
A COLLECTION OF RANGE REPLICAS. MORE THAN ONE REPLICA FOR A RANGE WILL NEVER
BE PLACED ON THE SAME STORE OR EVEN THE SAME NODE.

EARLY ON, WHEN A CLUSTER IS FIRST INITIALIZED, THE FEW DEFAULT STARTING RANGES
WILL ONLY HAVE A SINGLE REPLICA, BUT AS SOON AS OTHER NODES ARE AVAILABLE THEY
WILL REPLICATE TO THEM UNTIL THEY'VE REACHED THEIR DESIRED REPLICATION FACTOR,
THE DEFAULT BEING 3.

ZONE CONFIGS CAN BE USED TO CONTROL A RANGE'S REPLICATION FACTOR AND ADD
CONSTRAINTS AS TO WHERE THE RANGE'S REPLICAS CAN BE LOCATED. WHEN THERE IS A
CHANGE IN A RANGE'S ZONE CONFIG, THE RANGE WILL UP OR DOWN REPLICATE TO THE
APPROPRIATE NUMBER OF REPLICAS AND MOVE ITS REPLICAS TO THE APPROPRIATE STORES
BASED ON ZONE CONFIG'S CONSTRAINTS.

# SELF REPAIR

IF A STORE HAS NOT BEEN HEARD FROM (GOSSIPED THEIR DESCRIPTORS) IN SOME TIME,
THE DEFAULT SETTING BEING 5 MINUTES, THE CLUSTER WILL CONSIDER THIS STORE TO BE
DEAD. WHEN THIS HAPPENS, ALL RANGES THAT HAVE REPLICAS ON THAT STORE ARE
DETERMINED TO BE UNAVAILABLE AND REMOVED. THESE RANGES WILL THEN UPREPLICATE
THEMSELVES TO OTHER AVAILABLE STORES UNTIL THEIR DESIRED REPLICATION FACTOR IS
AGAIN MET. IF 50% OR MORE OF THE REPLICAS ARE UNAVAILABLE AT THE SAME TIME,
THERE IS NO QUORUM AND THE WHOLE RANGE WILL BE CONSIDERED UNAVAILABLE UNTIL AT
LEAST GREATER THAN 50% OF THE REPLICAS ARE AGAIN AVAILABLE.

# REBALANCING

AS MORE DATA ARE ADDED TO THE SYSTEM, SOME STORES MAY GROW FASTER THAN OTHERS.
TO COMBAT THIS AND TO SPREAD THE OVERALL LOAD ACROSS THE FULL CLUSTER, REPLICAS
WILL BE MOVED BETWEEN STORES MAINTAINING THE DESIRED REPLICATION FACTOR. THE
HEURISTICS USED TO PERFORM THIS REBALANCING INCLUDE:

- THE NUMBER OF REPLICAS PER STORE
- THE TOTAL SIZE OF THE DATA USED PER STORE
- FREE SPACE AVAILABLE PER STORE

IN THE FUTURE, SOME OTHER FACTORS THAT MIGHT BE CONSIDERED INCLUDE:

- CPU/NETWORK LOAD PER STORE
- RANGES THAT ARE USED TOGETHER OFTEN IN QUERIES
- NUMBER OF ACTIVE RANGES PER STORE
- NUMBER OF RANGE LEASES HELD PER STORE

# RANGE METADATA

THE DEFAULT APPROXIMATE SIZE OF A RANGE IS 64M (2\^26 B). IN ORDER TO
SUPPORT 1P (2\^50 B) OF LOGICAL DATA, METADATA IS NEEDED FOR ROUGHLY
2\^(50 - 26) = 2\^24 RANGES. A REASONABLE UPPER BOUND ON RANGE METADATA
SIZE IS ROUGHLY 256 BYTES (3\*12 BYTES FOR THE TRIPLICATED NODE
LOCATIONS AND 220 BYTES FOR THE RANGE KEY ITSELF). 2\^24 RANGES \* 2\^8
B WOULD REQUIRE ROUGHLY 4G (2\^32 B) TO STORE--TOO MUCH TO DUPLICATE
BETWEEN MACHINES. OUR CONCLUSION IS THAT RANGE METADATA MUST BE
DISTRIBUTED FOR LARGE INSTALLATIONS.

TO KEEP KEY LOOKUPS RELATIVELY FAST IN THE PRESENCE OF DISTRIBUTED METADATA,
WE STORE ALL THE TOP-LEVEL METADATA IN A SINGLE RANGE (THE FIRST RANGE). THESE
TOP-LEVEL METADATA KEYS ARE KNOWN AS *META1* KEYS, AND ARE PREFIXED SUCH THAT
THEY SORT TO THE BEGINNING OF THE KEY SPACE. GIVEN THE METADATA SIZE OF 256
BYTES GIVEN ABOVE, A SINGLE 64M RANGE WOULD SUPPORT 64M/256B = 2\^18 RANGES,
WHICH GIVES A TOTAL STORAGE OF 64M \* 2\^18 = 16T. TO SUPPORT THE 1P QUOTED
ABOVE, WE NEED TWO LEVELS OF INDIRECTION, WHERE THE FIRST LEVEL ADDRESSES THE
SECOND, AND THE SECOND ADDRESSES USER DATA. WITH TWO LEVELS OF INDIRECTION, WE
CAN ADDRESS 2\^(18 + 18) = 2\^36 RANGES; EACH RANGE ADDRESSES 2\^26 B, AND
ALTOGETHER WE ADDRESS 2\^(36+26) B = 2\^62 B = 4E OF USER DATA.

FOR A GIVEN USER-ADDRESSABLE `KEY1`, THE ASSOCIATED *META1* RECORD IS FOUND
AT THE SUCCESSOR KEY TO `KEY1` IN THE *META1* SPACE. SINCE THE *META1* SPACE
IS SPARSE, THE SUCCESSOR KEY IS DEFINED AS THE NEXT KEY WHICH IS PRESENT. THE
*META1* RECORD IDENTIFIES THE RANGE CONTAINING THE *META2* RECORD, WHICH IS
FOUND USING THE SAME PROCESS. THE *META2* RECORD IDENTIFIES THE RANGE
CONTAINING `KEY1`, WHICH IS AGAIN FOUND THE SAME WAY (SEE EXAMPLES BELOW).

CONCRETELY, METADATA KEYS ARE PREFIXED BY `\X02` (META1) AND `\X03`
(META2); THE PREFIXES `\X02` AND `\X03` PROVIDE FOR THE DESIRED
SORTING BEHAVIOUR. THUS, `KEY1`'S *META1* RECORD WILL RESIDE AT THE
SUCCESSOR KEY TO `\X02<KEY1>`.

NOTE: WE APPEND THE END KEY OF EACH RANGE TO META{1,2} RECORDS BECAUSE
THE ROCKSDB ITERATOR ONLY SUPPORTS A SEEK() INTERFACE WHICH ACTS AS A
CEIL(). USING THE START KEY OF THE RANGE WOULD CAUSE SEEK() TO FIND THE
KEY *AFTER* THE META INDEXING RECORD WE’RE LOOKING FOR, WHICH WOULD
RESULT IN HAVING TO BACK THE ITERATOR UP, AN OPTION WHICH IS BOTH LESS
EFFICIENT AND NOT AVAILABLE IN ALL CASES.

THE FOLLOWING EXAMPLE SHOWS THE DIRECTORY STRUCTURE FOR A MAP WITH
THREE RANGES WORTH OF DATA. ELLIPSES INDICATE ADDITIONAL KEY/VALUE
PAIRS TO FILL AN ENTIRE RANGE OF DATA. FOR CLARITY, THE EXAMPLES USE
`META1` AND `META2` TO REFER TO THE PREFIXES `\X02` AND `\X03`. EXCEPT
FOR THE FACT THAT SPLITTING RANGES REQUIRES UPDATES TO THE RANGE
METADATA WITH KNOWLEDGE OF THE METADATA LAYOUT, THE RANGE METADATA
ITSELF REQUIRES NO SPECIAL TREATMENT OR BOOTSTRAPPING.

**RANGE 0** (LOCATED ON SERVERS `DCRAMA1:8000`, `DCRAMA2:8000`,
  `DCRAMA3:8000`)

- `META1\XFF`: `DCRAMA1:8000`, `DCRAMA2:8000`, `DCRAMA3:8000`
- `META2<LASTKEY0>`: `DCRAMA1:8000`, `DCRAMA2:8000`, `DCRAMA3:8000`
- `META2<LASTKEY1>`: `DCRAMA4:8000`, `DCRAMA5:8000`, `DCRAMA6:8000`
- `META2\XFF`: `DCRAMA7:8000`, `DCRAMA8:8000`, `DCRAMA9:8000`
- ...
- `<LASTKEY0>`: `<LASTVALUE0>`

**RANGE 1** (LOCATED ON SERVERS `DCRAMA4:8000`, `DCRAMA5:8000`,
`DCRAMA6:8000`)

- ...
- `<LASTKEY1>`: `<LASTVALUE1>`

**RANGE 2** (LOCATED ON SERVERS `DCRAMA7:8000`, `DCRAMA8:8000`,
`DCRAMA9:8000`)

- ...
- `<LASTKEY2>`: `<LASTVALUE2>`

CONSIDER A SIMPLER EXAMPLE OF A MAP CONTAINING LESS THAN A SINGLE
RANGE OF DATA. IN THIS CASE, ALL RANGE METADATA AND ALL DATA ARE
LOCATED IN THE SAME RANGE:

**RANGE 0** (LOCATED ON SERVERS `DCRAMA1:8000`, `DCRAMA2:8000`,
`DCRAMA3:8000`)*

- `META1\XFF`: `DCRAMA1:8000`, `DCRAMA2:8000`, `DCRAMA3:8000`
- `META2\XFF`: `DCRAMA1:8000`, `DCRAMA2:8000`, `DCRAMA3:8000`
- `<KEY0>`: `<VALUE0>`
- `...`

FINALLY, A MAP LARGE ENOUGH TO NEED BOTH LEVELS OF INDIRECTION WOULD
LOOK LIKE (NOTE THAT INSTEAD OF SHOWING RANGE REPLICAS, THIS
EXAMPLE IS SIMPLIFIED TO JUST SHOW RANGE INDEXES):

**RANGE 0**

- `META1<LASTKEYN-1>`: RANGE 0
- `META1\XFF`: RANGE 1
- `META2<LASTKEY1>`:  RANGE 1
- `META2<LASTKEY2>`:  RANGE 2
- `META2<LASTKEY3>`:  RANGE 3
- ...
- `META2<LASTKEYN-1>`: RANGE 262143

**RANGE 1**

- `META2<LASTKEYN>`: RANGE 262144
- `META2<LASTKEYN+1>`: RANGE 262145
- ...
- `META2\XFF`: RANGE 500,000
- ...
- `<LASTKEY1>`: `<LASTVALUE1>`

**RANGE 2**

- ...
- `<LASTKEY2>`: `<LASTVALUE2>`

**RANGE 3**

- ...
- `<LASTKEY3>`: `<LASTVALUE3>`

**RANGE 262144**

- ...
- `<LASTKEYN>`: `<LASTVALUEN>`

**RANGE 262145**

- ...
- `<LASTKEYN+1>`: `<LASTVALUEN+1>`

NOTE THAT THE CHOICE OF RANGE `262144` IS JUST AN APPROXIMATION. THE
ACTUAL NUMBER OF RANGES ADDRESSABLE VIA A SINGLE METADATA RANGE IS
DEPENDENT ON THE SIZE OF THE KEYS. IF EFFORTS ARE MADE TO KEEP KEY SIZES
SMALL, THE TOTAL NUMBER OF ADDRESSABLE RANGES WOULD INCREASE AND VICE
VERSA.

FROM THE EXAMPLES ABOVE IT’S CLEAR THAT KEY LOCATION LOOKUPS REQUIRE AT
MOST THREE READS TO GET THE VALUE FOR `<KEY>`:

1. LOWER BOUND OF `META1<KEY>`
2. LOWER BOUND OF `META2<KEY>`,
3. `<KEY>`.

FOR SMALL MAPS, THE ENTIRE LOOKUP IS SATISFIED IN A SINGLE RPC TO RANGE 0. MAPS
CONTAINING LESS THAN 16T OF DATA WOULD REQUIRE TWO LOOKUPS. CLIENTS CACHE BOTH
LEVELS OF RANGE METADATA, AND WE EXPECT THAT DATA LOCALITY FOR INDIVIDUAL
CLIENTS WILL BE HIGH. CLIENTS MAY END UP WITH STALE CACHE ENTRIES. IF ON A
LOOKUP, THE RANGE CONSULTED DOES NOT MATCH THE CLIENT’S EXPECTATIONS, THE
CLIENT EVICTS THE STALE ENTRIES AND POSSIBLY DOES A NEW LOOKUP.

# RAFT - CONSISTENCY OF RANGE REPLICAS

EACH RANGE IS CONFIGURED TO CONSIST OF THREE OR MORE REPLICAS, AS SPECIFIED BY
THEIR ZONECONFIG. THE REPLICAS IN A RANGE MAINTAIN THEIR OWN INSTANCE OF A
DISTRIBUTED CONSENSUS ALGORITHM. WE USE THE [*RAFT CONSENSUS ALGORITHM*](HTTPS://RAFTCONSENSUS.GITHUB.IO)
AS IT IS SIMPLER TO REASON ABOUT AND INCLUDES A REFERENCE IMPLEMENTATION
COVERING IMPORTANT DETAILS.
[EPAXOS](HTTPS://WWW.CS.CMU.EDU/~DGA/PAPERS/EPAXOS-SOSP2013.PDF) HAS
PROMISING PERFORMANCE CHARACTERISTICS FOR WAN-DISTRIBUTED REPLICAS, BUT
IT DOES NOT GUARANTEE A CONSISTENT ORDERING BETWEEN REPLICAS.

RAFT ELECTS A RELATIVELY LONG-LIVED LEADER WHICH MUST BE INVOLVED TO
PROPOSE COMMANDS. IT HEARTBEATS FOLLOWERS PERIODICALLY AND KEEPS THEIR LOGS
REPLICATED. IN THE ABSENCE OF HEARTBEATS, FOLLOWERS BECOME CANDIDATES
AFTER RANDOMIZED ELECTION TIMEOUTS AND PROCEED TO HOLD NEW LEADER
ELECTIONS. COCKROACH WEIGHTS RANDOM TIMEOUTS SUCH THAT THE REPLICAS WITH
SHORTER ROUND TRIP TIMES TO PEERS ARE MORE LIKELY TO HOLD ELECTIONS
FIRST (NOT IMPLEMENTED YET). ONLY THE RAFT LEADER MAY PROPOSE COMMANDS;
FOLLOWERS WILL SIMPLY RELAY COMMANDS TO THE LAST KNOWN LEADER.

OUR RAFT IMPLEMENTATION WAS DEVELOPED TOGETHER WITH COREOS, BUT ADDS AN EXTRA
LAYER OF OPTIMIZATION TO ACCOUNT FOR THE FACT THAT A SINGLE NODE MAY HAVE
MILLIONS OF CONSENSUS GROUPS (ONE FOR EACH RANGE). AREAS OF OPTIMIZATION
ARE CHIEFLY COALESCED HEARTBEATS (SO THAT THE NUMBER OF NODES DICTATES THE
NUMBER OF HEARTBEATS AS OPPOSED TO THE MUCH LARGER NUMBER OF RANGES) AND
BATCH PROCESSING OF REQUESTS.
FUTURE OPTIMIZATIONS MAY INCLUDE TWO-PHASE ELECTIONS AND QUIESCENT RANGES
(I.E. STOPPING TRAFFIC COMPLETELY FOR INACTIVE RANGES).

# RANGE LEASES

AS OUTLINED IN THE RAFT SECTION, THE REPLICAS OF A RANGE ARE ORGANIZED AS A
RAFT GROUP AND EXECUTE COMMANDS FROM THEIR SHARED COMMIT LOG. GOING THROUGH
RAFT IS AN EXPENSIVE OPERATION THOUGH, AND THERE ARE TASKS WHICH SHOULD ONLY BE
CARRIED OUT BY A SINGLE REPLICA AT A TIME (AS OPPOSED TO ALL OF THEM).
IN PARTICULAR, IT IS DESIRABLE TO SERVE AUTHORITATIVE READS FROM A SINGLE
REPLICA (IDEALLY FROM MORE THAN ONE, BUT THAT IS FAR MORE DIFFICULT).

FOR THESE REASONS, COCKROACH INTRODUCES THE CONCEPT OF **RANGE LEASES**:
THIS IS A LEASE HELD FOR A SLICE OF (DATABASE, I.E. HYBRID LOGICAL) TIME.
A REPLICA ESTABLISHES ITSELF AS OWNING THE LEASE ON A RANGE BY COMMITTING
A SPECIAL LEASE ACQUISITION LOG ENTRY THROUGH RAFT. THE LOG ENTRY CONTAINS
THE REPLICA NODE'S EPOCH FROM THE NODE LIVENESS TABLE--A SYSTEM
TABLE CONTAINING AN EPOCH AND AN EXPIRATION TIME FOR EACH NODE. A NODE IS
RESPONSIBLE FOR CONTINUOUSLY UPDATING THE EXPIRATION TIME FOR ITS ENTRY
IN THE LIVENESS TABLE. ONCE THE LEASE HAS BEEN COMMITTED THROUGH RAFT
THE REPLICA BECOMES THE LEASE HOLDER AS SOON AS IT APPLIES THE LEASE
ACQUISITION COMMAND, GUARANTEEING THAT WHEN IT USES THE LEASE IT HAS
ALREADY APPLIED ALL PRIOR WRITES ON THE REPLICA AND CAN SEE THEM LOCALLY.

TO PREVENT TWO NODES FROM ACQUIRING THE LEASE, THE REQUESTOR INCLUDES A COPY
OF THE LEASE THAT IT BELIEVES TO BE VALID AT THE TIME IT REQUESTS THE LEASE.
IF THAT LEASE IS STILL VALID WHEN THE NEW LEASE IS APPLIED, IT IS GRANTED,
OR ANOTHER LEASE IS GRANTED IN THE INTERIM AND THE REQUESTED LEASE IS
IGNORED. A LEASE CAN MOVE FROM NODE A TO NODE B ONLY AFTER NODE A'S
LIVENESS RECORD HAS EXPIRED AND ITS EPOCH HAS BEEN INCREMENTED.

NOTE: RANGE LEASES FOR RANGES WITHIN THE NODE LIVENESS TABLE KEYSPACE AND
ALL RANGES THAT PRECEDE IT, INCLUDING META1 AND META2, ARE NOT MANAGED USING
THE ABOVE MECHANISM TO PREVENT CIRCULAR DEPENDENCIES.

A REPLICA HOLDING A LEASE AT A SPECIFIC EPOCH CAN USE THE LEASE AS LONG AS
THE NODE EPOCH HASN'T CHANGED AND THE EXPIRATION TIME HASN'T PASSED.
THE REPLICA HOLDING THE LEASE MAY SATISFY READS LOCALLY, WITHOUT INCURRING THE
OVERHEAD OF GOING THROUGH RAFT, AND IS IN CHARGE OR INVOLVED IN HANDLING
RANGE-SPECIFIC MAINTENANCE TASKS SUCH AS SPLITTING, MERGING AND REBALANCING

ALL READS AND WRITES ARE GENERALLY ADDRESSED TO THE REPLICA HOLDING
THE LEASE; IF NONE DOES, ANY REPLICA MAY BE ADDRESSED, CAUSING IT TO TRY
TO OBTAIN THE LEASE SYNCHRONOUSLY. REQUESTS RECEIVED BY A NON-LEASE HOLDER
(FOR THE HLC TIMESTAMP SPECIFIED IN THE REQUEST'S HEADER) FAIL WITH AN
ERROR POINTING AT THE REPLICA'S LAST KNOWN LEASE HOLDER. THESE REQUESTS
ARE RETRIED TRANSPARENTLY WITH THE UPDATED LEASE BY THE GATEWAY NODE AND
NEVER REACH THE CLIENT.

## COLOCATION WITH RAFT LEADERSHIP

THE RANGE LEASE IS COMPLETELY SEPARATE FROM RAFT LEADERSHIP, AND SO WITHOUT
FURTHER EFFORTS, RAFT LEADERSHIP AND THE RANGE LEASE MIGHT NOT BE HELD BY THE
SAME REPLICA. SINCE IT'S EXPENSIVE TO NOT HAVE THESE TWO ROLES COLOCATED (THE
LEASE HOLDER HAS TO FORWARD EACH PROPOSAL TO THE LEADER, ADDING COSTLY RPC
ROUND-TRIPS), EACH LEASE RENEWAL OR TRANSFER ALSO ATTEMPTS TO COLOCATE THEM.
IN PRACTICE, THAT MEANS THAT THE MISMATCH IS RARE AND SELF-CORRECTS QUICKLY.

## COMMAND EXECUTION FLOW

THIS SUBSECTION DESCRIBES HOW A LEASE HOLDER REPLICA PROCESSES A
READ/WRITE COMMAND IN MORE DETAILS. EACH COMMAND SPECIFIES (1) A KEY
(OR A RANGE OF KEYS) THAT THE COMMAND ACCESSES AND (2) THE ID OF A
RANGE WHICH THE KEY(S) BELONGS TO. WHEN RECEIVING A COMMAND, A NODE
LOOKS UP A RANGE BY THE SPECIFIED RANGE ID AND CHECKS IF THE RANGE IS
STILL RESPONSIBLE FOR THE SUPPLIED KEYS. IF ANY OF THE KEYS DO NOT
BELONG TO THE RANGE, THE NODE RETURNS AN ERROR SO THAT THE CLIENT WILL
RETRY AND SEND A REQUEST TO A CORRECT RANGE.

WHEN ALL THE KEYS BELONG TO THE RANGE, THE NODE ATTEMPTS TO
PROCESS THE COMMAND. IF THE COMMAND IS AN INCONSISTENT READ-ONLY
COMMAND, IT IS PROCESSED IMMEDIATELY. IF THE COMMAND IS A CONSISTENT
READ OR A WRITE, THE COMMAND IS EXECUTED WHEN BOTH OF THE FOLLOWING
CONDITIONS HOLD:

- THE RANGE REPLICA HAS A RANGE LEASE.
- THERE ARE NO OTHER RUNNING COMMANDS WHOSE KEYS OVERLAP WITH
THE SUBMITTED COMMAND AND CAUSE READ/WRITE CONFLICT.

WHEN THE FIRST CONDITION IS NOT MET, THE REPLICA ATTEMPTS TO ACQUIRE
A LEASE OR RETURNS AN ERROR SO THAT THE CLIENT WILL REDIRECT THE
COMMAND TO THE CURRENT LEASE HOLDER. THE SECOND CONDITION GUARANTEES THAT
CONSISTENT READ/WRITE COMMANDS FOR A GIVEN KEY ARE SEQUENTIALLY
EXECUTED.

WHEN THE ABOVE TWO CONDITIONS ARE MET, THE LEASE HOLDER REPLICA PROCESSES THE
COMMAND. CONSISTENT READS ARE PROCESSED ON THE LEASE HOLDER IMMEDIATELY.
WRITE COMMANDS ARE COMMITTED INTO THE RAFT LOG SO THAT EVERY REPLICA
WILL EXECUTE THE SAME COMMANDS. ALL COMMANDS PRODUCE DETERMINISTIC
RESULTS SO THAT THE RANGE REPLICAS KEEP CONSISTENT STATES AMONG THEM.

WHEN A WRITE COMMAND COMPLETES, ALL THE REPLICA UPDATES THEIR RESPONSE
CACHE TO ENSURE IDEMPOTENCY. WHEN A READ COMMAND COMPLETES, THE LEASE HOLDER
REPLICA UPDATES ITS TIMESTAMP CACHE TO KEEP TRACK OF THE LATEST READ
FOR A GIVEN KEY.

THERE IS A CHANCE THAT A RANGE LEASE GETS EXPIRED WHILE A COMMAND IS
EXECUTED. BEFORE EXECUTING A COMMAND, EACH REPLICA CHECKS IF A REPLICA
PROPOSING THE COMMAND HAS A STILL LEASE. WHEN THE LEASE HAS BEEN
EXPIRED, THE COMMAND WILL BE REJECTED BY THE REPLICA.


# SPLITTING / MERGING RANGES

NODES SPLIT OR MERGE RANGES BASED ON WHETHER THEY EXCEED MAXIMUM OR
MINIMUM THRESHOLDS FOR CAPACITY OR LOAD. RANGES EXCEEDING MAXIMUMS FOR
EITHER CAPACITY OR LOAD ARE SPLIT; RANGES BELOW MINIMUMS FOR *BOTH*
CAPACITY AND LOAD ARE MERGED.

RANGES MAINTAIN THE SAME ACCOUNTING STATISTICS AS ACCOUNTING KEY
PREFIXES. THESE BOIL DOWN TO A TIME SERIES OF DATA POINTS WITH MINUTE
GRANULARITY. EVERYTHING FROM NUMBER OF BYTES TO READ/WRITE QUEUE SIZES.
ARBITRARY DISTILLATIONS OF THE ACCOUNTING STATS CAN BE DETERMINED AS THE
BASIS FOR SPLITTING / MERGING. TWO SENSIBLE METRICS FOR USE WITH
SPLIT/MERGE ARE RANGE SIZE IN BYTES AND IOPS. A GOOD METRIC FOR
REBALANCING A REPLICA FROM ONE NODE TO ANOTHER WOULD BE TOTAL READ/WRITE
QUEUE WAIT TIMES. THESE METRICS ARE GOSSIPPED, WITH EACH RANGE / NODE
PASSING ALONG RELEVANT METRICS IF THEY’RE IN THE BOTTOM OR TOP OF THE
RANGE IT’S AWARE OF.

A RANGE FINDING ITSELF EXCEEDING EITHER CAPACITY OR LOAD THRESHOLD
SPLITS. TO THIS END, THE RANGE LEASE HOLDER COMPUTES AN APPROPRIATE SPLIT KEY
CANDIDATE AND ISSUES THE SPLIT THROUGH RAFT. IN CONTRAST TO SPLITTING,
MERGING REQUIRES A RANGE TO BE BELOW THE MINIMUM THRESHOLD FOR BOTH
CAPACITY *AND* LOAD. A RANGE BEING MERGED CHOOSES THE SMALLER OF THE
RANGES IMMEDIATELY PRECEDING AND SUCCEEDING IT.

SPLITTING, MERGING, REBALANCING AND RECOVERING ALL FOLLOW THE SAME BASIC
ALGORITHM FOR MOVING DATA BETWEEN ROACH NODES. NEW TARGET REPLICAS ARE
CREATED AND ADDED TO THE REPLICA SET OF SOURCE RANGE. THEN EACH NEW
REPLICA IS BROUGHT UP TO DATE BY EITHER REPLAYING THE LOG IN FULL OR
COPYING A SNAPSHOT OF THE SOURCE REPLICA DATA AND THEN REPLAYING THE LOG
FROM THE TIMESTAMP OF THE SNAPSHOT TO CATCH UP FULLY. ONCE THE NEW
REPLICAS ARE FULLY UP TO DATE, THE RANGE METADATA IS UPDATED AND OLD,
SOURCE REPLICA(S) DELETED IF APPLICABLE.

**COORDINATOR** (LEASE HOLDER REPLICA)

```
IF SPLITTING
  SPLITRANGE(SPLIT_KEY): SPLITS HAPPEN LOCALLY ON RANGE REPLICAS AND
  ONLY AFTER BEING COMPLETED LOCALLY, ARE MOVED TO NEW TARGET REPLICAS.
ELSE IF MERGING
  CHOOSE NEW REPLICAS ON SAME SERVERS AS TARGET RANGE REPLICAS;
  ADD TO REPLICA SET.
ELSE IF REBALANCING || RECOVERING
  CHOOSE NEW REPLICA(S) ON LEAST LOADED SERVERS; ADD TO REPLICA SET.
```

**NEW REPLICA**

*BRING REPLICA UP TO DATE:*

```
IF ALL INFO CAN BE READ FROM REPLICATED LOG
  COPY REPLICATED LOG
ELSE
  SNAPSHOT SOURCE REPLICA
  SEND SUCCESSIVE READRANGE REQUESTS TO SOURCE REPLICA
  REFERENCING SNAPSHOT

IF MERGING
  COMBINE RANGES ON ALL REPLICAS
ELSE IF REBALANCING || RECOVERING
  REMOVE OLD RANGE REPLICA(S)
```

NODES SPLIT RANGES WHEN THE TOTAL DATA IN A RANGE EXCEEDS A
CONFIGURABLE MAXIMUM THRESHOLD. SIMILARLY, RANGES ARE MERGED WHEN THE
TOTAL DATA FALLS BELOW A CONFIGURABLE MINIMUM THRESHOLD.

**TBD: FLESH THIS OUT**: ESPECIALLY FOR MERGES (BUT ALSO REBALANCING) WE HAVE A
RANGE DISAPPEARING FROM THE LOCAL NODE; THAT RANGE NEEDS TO DISAPPEAR
GRACEFULLY, WITH A SMOOTH HANDOFF OF OPERATION TO THE NEW OWNER OF ITS DATA.

RANGES ARE REBALANCED IF A NODE DETERMINES ITS LOAD OR CAPACITY IS ONE
OF THE WORST IN THE CLUSTER BASED ON GOSSIPPED LOAD STATS. A NODE WITH
SPARE CAPACITY IS CHOSEN IN THE SAME DATACENTER AND A SPECIAL-CASE SPLIT
IS DONE WHICH SIMPLY DUPLICATES THE DATA 1:1 AND RESETS THE RANGE
CONFIGURATION METADATA.

# NODE ALLOCATION (VIA GOSSIP)

NEW NODES MUST BE ALLOCATED WHEN A RANGE IS SPLIT. INSTEAD OF REQUIRING
EVERY NODE TO KNOW ABOUT THE STATUS OF ALL OR EVEN A LARGE NUMBER
OF PEER NODES --OR-- ALTERNATIVELY REQUIRING A SPECIALIZED CURATOR OR
MASTER WITH SUFFICIENTLY GLOBAL KNOWLEDGE, WE USE A GOSSIP PROTOCOL TO
EFFICIENTLY COMMUNICATE ONLY INTERESTING INFORMATION BETWEEN ALL OF THE
NODES IN THE CLUSTER. WHAT’S INTERESTING INFORMATION? ONE EXAMPLE WOULD
BE WHETHER A PARTICULAR NODE HAS A LOT OF SPARE CAPACITY. EACH NODE,
WHEN GOSSIPING, COMPARES EACH TOPIC OF GOSSIP TO ITS OWN STATE. IF ITS
OWN STATE IS SOMEHOW “MORE INTERESTING” THAN THE LEAST INTERESTING ITEM
IN THE TOPIC IT’S SEEN RECENTLY, IT INCLUDES ITS OWN STATE AS PART OF
THE NEXT GOSSIP SESSION WITH A PEER NODE. IN THIS WAY, A NODE WITH
CAPACITY SUFFICIENTLY IN EXCESS OF THE MEAN QUICKLY BECOMES DISCOVERED
BY THE ENTIRE CLUSTER. TO AVOID PILING ONTO OUTLIERS, NODES FROM THE
HIGH CAPACITY SET ARE SELECTED AT RANDOM FOR ALLOCATION.

THE GOSSIP PROTOCOL ITSELF CONTAINS TWO PRIMARY COMPONENTS:

- **PEER SELECTION**: EACH NODE MAINTAINS UP TO N PEERS WITH WHICH IT
  REGULARLY COMMUNICATES. IT SELECTS PEERS WITH AN EYE TOWARDS
  MAXIMIZING FANOUT. A PEER NODE WHICH ITSELF COMMUNICATES WITH AN
  ARRAY OF OTHERWISE UNKNOWN NODES WILL BE SELECTED OVER ONE WHICH
  COMMUNICATES WITH A SET CONTAINING SIGNIFICANT OVERLAP. EACH TIME
  GOSSIP IS INITIATED, EACH NODES’ SET OF PEERS IS EXCHANGED. EACH
  NODE IS THEN FREE TO INCORPORATE THE OTHER’S PEERS AS IT SEES FIT.
  TO AVOID ANY NODE SUFFERING FROM EXCESS INCOMING REQUESTS, A NODE
  MAY REFUSE TO ANSWER A GOSSIP EXCHANGE. EACH NODE IS BIASED
  TOWARDS ANSWERING REQUESTS FROM NODES WITHOUT SIGNIFICANT OVERLAP
  AND REFUSING REQUESTS OTHERWISE.

  PEERS ARE EFFICIENTLY SELECTED USING A HEURISTIC AS DESCRIBED IN
  [AGARWAL & TRACHTENBERG (2006)](HTTPS://DRIVE.GOOGLE.COM/FILE/D/0B9GCVTP_FHJISMFRTTHKOEZSM1U/EDIT?USP=SHARING).

  **TBD**: HOW TO AVOID PARTITIONS? NEED TO WORK OUT A SIMULATION OF
  THE PROTOCOL TO TUNE THE BEHAVIOR AND SEE EMPIRICALLY HOW WELL IT
  WORKS.

- **GOSSIP SELECTION**: WHAT TO COMMUNICATE. GOSSIP IS DIVIDED INTO
  TOPICS. LOAD CHARACTERISTICS (CAPACITY PER DISK, CPU LOAD, AND
  STATE [E.G. DRAINING, OK, FAILURE]) ARE USED TO DRIVE NODE
  ALLOCATION. RANGE STATISTICS (RANGE READ/WRITE LOAD, MISSING
  REPLICAS, UNAVAILABLE RANGES) AND NETWORK TOPOLOGY (INTER-RACK
  BANDWIDTH/LATENCY, INTER-DATACENTER BANDWIDTH/LATENCY, SUBNET
  OUTAGES) ARE USED FOR DETERMINING WHEN TO SPLIT RANGES, WHEN TO
  RECOVER REPLICAS VS. WAIT FOR NETWORK CONNECTIVITY, AND FOR
  DEBUGGING / SYSOPS. IN ALL CASES, A SET OF MINIMUMS AND A SET OF
  MAXIMUMS IS PROPAGATED; EACH NODE APPLIES ITS OWN VIEW OF THE
  WORLD TO AUGMENT THE VALUES. EACH MINIMUM AND MAXIMUM VALUE IS
  TAGGED WITH THE REPORTING NODE AND OTHER ACCOMPANYING CONTEXTUAL
  INFORMATION. EACH TOPIC OF GOSSIP HAS ITS OWN PROTOBUF TO HOLD THE
  STRUCTURED DATA. THE NUMBER OF ITEMS OF GOSSIP IN EACH TOPIC IS
  LIMITED BY A CONFIGURABLE BOUND.

  FOR EFFICIENCY, NODES ASSIGN EACH NEW ITEM OF GOSSIP A SEQUENCE
  NUMBER AND KEEP TRACK OF THE HIGHEST SEQUENCE NUMBER EACH PEER
  NODE HAS SEEN. EACH ROUND OF GOSSIP COMMUNICATES ONLY THE DELTA
  CONTAINING NEW ITEMS.

# NODE AND CLUSTER METRICS

EVERY COMPONENT OF THE SYSTEM IS RESPONSIBLE FOR EXPORTING INTERESTING
METRICS ABOUT ITSELF. THESE COULD BE HISTOGRAMS, THROUGHPUT COUNTERS, OR
GAUGES.

THESE METRICS ARE EXPORTED FOR EXTERNAL MONITORING SYSTEMS (SUCH AS PROMETHEUS)
VIA A HTTP ENDPOINT, BUT COCKROACHDB ALSO IMPLEMENTS AN INTERNAL TIMESERIES
DATABASE WHICH IS STORED IN THE REPLICATED KEY-VALUE MAP.

TIME SERIES ARE STORED AT STORE GRANULARITY AND ALLOW THE ADMIN DASHBOARD
TO EFFICIENTLY GAIN VISIBILITY INTO A UNIVERSE OF INFORMATION AT THE CLUSTER,
NODE OR STORE LEVEL. A [PERIODIC BACKGROUND PROCESS](RFCS/20160901_TIME_SERIES_CULLING.MD)
CULLS OLDER TIMESERIES DATA, DOWNSAMPLING AND EVENTUALLY DISCARDING IT.

# ZONES

ZONES PROVIDE A METHOD FOR CONFIGURING THE REPLICATION OF PORTIONS OF THE
KEYSPACE. ZONE VALUES SPECIFY A PROTOBUF CONTAINING
THE DATACENTERS FROM WHICH REPLICAS FOR RANGES WHICH FALL UNDER
THE ZONE MUST BE CHOSEN.

PLEASE SEE
[PKG/CONFIG/ZONE.PROTO](HTTPS://GITHUB.COM/COCKROACHDB/COCKROACH/BLOB/MASTER/PKG/CONFIG/ZONE.PROTO)
FOR UP-TO-DATE DATA STRUCTURES USED, THE BEST ENTRY POINT BEING
`MESSAGE ZONECONFIG`.

IF ZONES ARE MODIFIED IN SITU, EACH NODE VERIFIES THE
EXISTING ZONES FOR ITS RANGES AGAINST THE ZONE CONFIGURATION. IF
IT DISCOVERS DIFFERENCES, IT RECONFIGURES RANGES IN THE SAME WAY
THAT IT REBALANCES AWAY FROM BUSY NODES, VIA SPECIAL-CASE 1:1
SPLIT TO A DUPLICATE RANGE COMPRISING THE NEW CONFIGURATION.

# SQL

EACH NODE IN A CLUSTER CAN ACCEPT SQL CLIENT CONNECTIONS. COCKROACHDB
SUPPORTS THE POSTGRESQL WIRE PROTOCOL, TO ENABLE REUSE OF NATIVE
POSTGRESQL CLIENT DRIVERS. CONNECTIONS USING SSL AND AUTHENTICATED
USING CLIENT CERTIFICATES ARE SUPPORTED AND EVEN ENCOURAGED OVER
UNENCRYPTED (INSECURE) AND PASSWORD-BASED CONNECTIONS.

EACH CONNECTION IS ASSOCIATED WITH A SQL SESSION WHICH HOLDS THE
SERVER-SIDE STATE OF THE CONNECTION. OVER THE LIFESPAN OF A SESSION
THE CLIENT CAN SEND SQL TO OPEN/CLOSE TRANSACTIONS, ISSUE STATEMENTS
OR QUERIES OR CONFIGURE SESSION PARAMETERS, MUCH LIKE WITH ANY OTHER
SQL DATABASE.

## LANGUAGE SUPPORT

COCKROACHDB ALSO ATTEMPTS TO EMULATE THE FLAVOR OF SQL SUPPORTED BY
POSTGRESQL, ALTHOUGH IT ALSO DIVERGES IN SIGNIFICANT WAYS:

- COCKROACHDB EXCLUSIVELY IMPLEMENTS MVCC-BASED CONSISTENCY FOR
  TRANSACTIONS, AND THUS ONLY SUPPORTS SQL'S ISOLATION LEVELS SNAPSHOT
  AND SERIALIZABLE.  THE OTHER TRADITIONAL SQL ISOLATION LEVELS ARE
  INTERNALLY MAPPED TO EITHER SNAPSHOT OR SERIALIZABLE.

- COCKROACHDB IMPLEMENTS ITS OWN [SQL TYPE SYSTEM](RFCS/20160203_TYPING.MD)
  WHICH ONLY SUPPORTS A LIMITED FORM OF IMPLICIT COERCIONS BETWEEN
  TYPES COMPARED TO POSTGRESQL. THE RATIONALE IS TO KEEP THE
  IMPLEMENTATION SIMPLE AND EFFICIENT, CAPITALIZING ON THE OBSERVATION
  THAT 1) MOST SQL CODE IN CLIENTS IS AUTOMATICALLY GENERATED WITH
  COHERENT TYPING ALREADY AND 2) EXISTING SQL CODE FOR OTHER DATABASES
  WILL NEED TO BE MASSAGED FOR COCKROACHDB ANYWAY.

## SQL ARCHITECTURE

CLIENT CONNECTIONS OVER THE NETWORK ARE HANDLED IN EACH NODE BY A
PGWIRE SERVER PROCESS (GOROUTINE). THIS HANDLES THE STREAM OF INCOMING
COMMANDS AND SENDS BACK RESPONSES INCLUDING QUERY/STATEMENT RESULTS.
THE PGWIRE SERVER ALSO HANDLES PGWIRE-LEVEL PREPARED STATEMENTS,
BINDING PREPARED STATEMENTS TO ARGUMENTS AND LOOKING UP PREPARED
STATEMENTS FOR EXECUTION.

MEANWHILE THE STATE OF A SQL CONNECTION IS MAINTAINED BY A SESSION
OBJECT AND A MONOLITHIC `PLANNER` OBJECT (ONE PER CONNECTION) WHICH
COORDINATES EXECUTION BETWEEN THE SESSION, THE CURRENT SQL TRANSACTION
STATE AND THE UNDERLYING KV STORE.

UPON RECEIVING A QUERY/STATEMENT (EITHER DIRECTLY OR VIA AN EXECUTE
COMMAND FOR A PREVIOUSLY PREPARED STATEMENT) THE PGWIRE SERVER FORWARDS
THE SQL TEXT TO THE `PLANNER` ASSOCIATED WITH THE CONNECTION. THE SQL
CODE IS THEN TRANSFORMED INTO A SQL QUERY PLAN.
THE QUERY PLAN IS IMPLEMENTED AS A TREE OF OBJECTS WHICH DESCRIBE THE
HIGH-LEVEL DATA OPERATIONS NEEDED TO RESOLVE THE QUERY, FOR EXAMPLE
"JOIN", "INDEX JOIN", "SCAN", "GROUP", ETC.

THE QUERY PLAN OBJECTS CURRENTLY ALSO EMBED THE RUN-TIME STATE NEEDED
FOR THE EXECUTION OF THE QUERY PLAN. ONCE THE SQL QUERY PLAN IS READY,
METHODS ON THESE OBJECTS THEN CARRY THE EXECUTION OUT IN THE FASHION
OF "GENERATORS" IN OTHER PROGRAMMING LANGUAGES: EACH NODE *STARTS* ITS
CHILDREN NODES AND FROM THAT POINT FORWARD EACH CHILD NODE SERVES AS A
*GENERATOR* FOR A STREAM OF RESULT ROWS, WHICH THE PARENT NODE CAN
CONSUME AND TRANSFORM INCREMENTALLY AND PRESENT TO ITS OWN PARENT NODE
ALSO AS A GENERATOR.

THE TOP-LEVEL PLANNER CONSUMES THE DATA PRODUCED BY THE TOP NODE OF
THE QUERY PLAN AND RETURNS IT TO THE CLIENT VIA PGWIRE.

## DATA MAPPING BETWEEN THE SQL MODEL AND KV

EVERY SQL TABLE HAS A PRIMARY KEY IN COCKROACHDB. (IF A TABLE IS CREATED
WITHOUT ONE, AN IMPLICIT PRIMARY KEY IS PROVIDED AUTOMATICALLY.)
THE TABLE IDENTIFIER, FOLLOWED BY THE VALUE OF THE PRIMARY KEY FOR
EACH ROW, ARE ENCODED AS THE *PREFIX* OF A KEY IN THE UNDERLYING KV
STORE.

EACH REMAINING COLUMN OR *COLUMN FAMILY* IN THE TABLE IS THEN ENCODED
AS A VALUE IN THE UNDERLYING KV STORE, AND THE COLUMN/FAMILY IDENTIFIER
IS APPENDED AS *SUFFIX* TO THE KV KEY.

FOR EXAMPLE:

- AFTER TABLE `CUSTOMERS` IS CREATED IN A DATABASE `MYDB` WITH A
PRIMARY KEY COLUMN `NAME` AND NORMAL COLUMNS `ADDRESS` AND `URL`, THE KV PAIRS
TO STORE THE SCHEMA WOULD BE:

| KEY                          | VALUES |
| ---------------------------- | ------ |
| `/SYSTEM/DATABASES/MYDB/ID`  | 51     |
| `/SYSTEM/TABLES/CUSTOMER/ID` | 42     |
| `/SYSTEM/DESC/51/42/ADDRESS` | 69     |
| `/SYSTEM/DESC/51/42/URL`     | 66     |

(THE NUMERIC VALUES ON THE RIGHT ARE CHOSEN ARBITRARILY FOR THE
EXAMPLE; THE STRUCTURE OF THE SCHEMA KEYS ON THE LEFT IS SIMPLIFIED
FOR THE EXAMPLE AND SUBJECT TO CHANGE.)  EACH DATABASE/TABLE/COLUMN
NAME IS MAPPED TO A SPONTANEOUSLY GENERATED IDENTIFIER, SO AS TO
SIMPLIFY RENAMES.

THEN FOR A SINGLE ROW IN THIS TABLE:

| KEY               | VALUES                           |
| ----------------- | -------------------------------- |
| `/51/42/APPLE/69` | `1 INFINITE LOOP, CUPERTINO, CA` |
| `/51/42/APPLE/66` | `HTTP://APPLE.COM/`              |

EACH KEY HAS THE TABLE PREFIX `/51/42` FOLLOWED BY THE PRIMARY KEY
PREFIX `/APPLE` FOLLOWED BY THE COLUMN/FAMILY SUFFIX (`/66`,
`/69`). THE KV VALUE IS DIRECTLY ENCODED FROM THE SQL VALUE.

EFFICIENT STORAGE FOR THE KEYS IS GUARANTEED BY THE UNDERLYING ROCKSDB ENGINE
BY MEANS OF PREFIX COMPRESSION.

FINALLY, FOR SQL INDEXES, THE KV KEY IS FORMED USING THE SQL VALUE OF THE
INDEXED COLUMNS, AND THE KV VALUE IS THE KV KEY PREFIX OF THE REST OF
THE INDEXED ROW.

## DISTRIBUTED SQL

DIST-SQL IS A NEW EXECUTION FRAMEWORK BEING DEVELOPED AS OF Q3 2016 WITH THE
GOAL OF DISTRIBUTING THE PROCESSING OF SQL QUERIES.
SEE THE [DISTRIBUTED SQL
RFC](RFCS/20160421_DISTRIBUTED_SQL.MD)
FOR A DETAILED DESIGN OF THE SUBSYSTEM; THIS SECTION WILL SERVE AS A SUMMARY.

DISTRIBUTING THE PROCESSING IS DESIRABLE FOR MULTIPLE REASONS:
- REMOTE-SIDE FILTERING: WHEN QUERYING FOR A SET OF ROWS THAT MATCH A FILTERING
  EXPRESSION, INSTEAD OF QUERYING ALL THE KEYS IN CERTAIN RANGES AND PROCESSING
  THE FILTERS AFTER RECEIVING THE DATA ON THE GATEWAY NODE OVER THE NETWORK,
  WE'D LIKE THE FILTERING EXPRESSION TO BE PROCESSED BY THE LEASE HOLDER OR
  REMOTE NODE, SAVING ON NETWORK TRAFFIC AND RELATED PROCESSING.
- FOR STATEMENTS LIKE `UPDATE .. WHERE` AND `DELETE .. WHERE` WE WANT TO
  PERFORM THE QUERY AND THE UPDATES ON THE NODE WHICH HAS THE DATA (AS OPPOSED
  TO RECEIVING RESULTS AT THE GATEWAY OVER THE NETWORK, AND THEN PERFORMING THE
  UPDATE OR DELETION THERE, WHICH INVOLVES ADDITIONAL ROUND-TRIPS).
- PARALLELIZE SQL COMPUTATION: WHEN SIGNIFICANT COMPUTATION IS REQUIRED, WE
  WANT TO DISTRIBUTE IT TO MULTIPLE NODE, SO THAT IT SCALES WITH THE AMOUNT OF
  DATA INVOLVED. THIS APPLIES TO `JOIN`S, AGGREGATION, SORTING.

THE APPROACH WE TOOK  WAS ORIGINALLY INSPIRED BY
[SAWZALL](HTTPS://CLOUD.GOOGLE.COM/DATAFLOW/MODEL/PROGRAMMING-MODEL) - A
PROJECT BY ROB PIKE ET AL. AT GOOGLE THAT PROPOSES A "SHELL" (HIGH-LEVEL
LANGUAGE INTERPRETER) TO EASE THE EXPLOITATION OF MAPREDUCE. IT PROVIDES A
CLEAR SEPARATION BETWEEN "LOCAL" PROCESSES WHICH PROCESS A LIMITED AMOUNT OF
DATA AND DISTRIBUTED COMPUTATIONS, WHICH ARE ABSTRACTED AWAY BEHIND A
RESTRICTED SET OF CONCEPTUAL CONSTRUCTS.

TO RUN SQL STATEMENTS IN A DISTRIBUTED FASHION, WE INTRODUCE A COUPLE OF CONCEPTS:
- _LOGICAL PLAN_ - SIMILAR ON THE SURFACE TO THE `PLANNODE` TREE DESCRIBED IN
  THE [SQL](#SQL) SECTION, IT REPRESENTS THE ABSTRACT (NON-DISTRIBUTED) DATA FLOW
  THROUGH COMPUTATION STAGES.
- _PHYSICAL PLAN_ - A PHYSICAL PLAN IS CONCEPTUALLY A MAPPING OF THE _LOGICAL
  PLAN_ NODES TO COCKROACHDB NODES. LOGICAL PLAN NODES ARE REPLICATED AND
  SPECIALIZED DEPENDING ON THE CLUSTER TOPOLOGY. THE COMPONENTS OF THE PHYSICAL
  PLAN ARE SCHEDULED AND RUN ON THE CLUSTER.

## LOGICAL PLANNING

THE LOGICAL PLAN IS MADE UP OF _AGGREGATORS_. EACH _AGGREGATOR_ CONSUMES AN
_INPUT STREAM_ OF ROWS (OR MULTIPLE STREAMS FOR JOINS) AND PRODUCES AN _OUTPUT
STREAM_ OF ROWS. BOTH THE INPUT AND THE OUTPUT STREAMS HAVE A SET SCHEMA. THE
STREAMS ARE A LOGICAL CONCEPT AND MIGHT NOT MAP TO A SINGLE DATA STREAM IN THE
ACTUAL COMPUTATION. AGGREGATORS WILL BE POTENTIALLY DISTRIBUTED WHEN CONVERTING
THE *LOGICAL PLAN* TO A *PHYSICAL PLAN*; TO EXPRESS WHAT DISTRIBUTION AND
PARALLELIZATION IS ALLOWED, AN AGGREGATOR DEFINES A _GROUPING_ ON THE DATA THAT
FLOWS THROUGH IT, EXPRESSING WHICH ROWS NEED TO BE PROCESSED ON THE SAME NODE
(THIS MECHANISM CONSTRAINTS ROWS MATCHING IN A SUBSET OF COLUMNS TO BE
PROCESSED ON THE SAME NODE). THIS CONCEPT IS USEFUL FOR AGGREGATORS THAT NEED
TO SEE SOME SET OF ROWS FOR PRODUCING OUTPUT - E.G. THE SQL AGGREGATION
FUNCTIONS. AN AGGREGATOR WITH NO GROUPING IS A SPECIAL BUT IMPORTANT CASE IN
WHICH WE ARE NOT AGGREGATING MULTIPLE PIECES OF DATA, BUT WE MAY BE FILTERING,
TRANSFORMING, OR REORDERING INDIVIDUAL PIECES OF DATA.

SPECIAL **TABLE READER** AGGREGATORS WITH NO INPUTS ARE USED AS DATA SOURCES; A
TABLE READER CAN BE CONFIGURED TO OUTPUT ONLY CERTAIN COLUMNS, AS NEEDED.
A SPECIAL **FINAL** AGGREGATOR WITH NO OUTPUTS IS USED FOR THE RESULTS OF THE
QUERY/STATEMENT.

TO REFLECT THE RESULT ORDERING THAT A QUERY HAS TO PRODUCE, SOME AGGREGATORS
(`FINAL`, `LIMIT`) ARE CONFIGURED WITH AN **ORDERING REQUIREMENT** ON THE INPUT
STREAM (A LIST OF COLUMNS WITH CORRESPONDING ASCENDING/DESCENDING
REQUIREMENTS). SOME AGGREGATORS (LIKE `TABLE READERS`) CAN GUARANTEE A CERTAIN
ORDERING ON THEIR OUTPUT STREAM, CALLED AN **ORDERING GUARANTEE**. ALL
AGGREGATORS HAVE AN ASSOCIATED **ORDERING CHARACTERIZATION** FUNCTION
`ORD(INPUT_ORDER) -> OUTPUT_ORDER` THAT MAPS `INPUT_ORDER` (AN ORDERING
GUARANTEE ON THE INPUT STREAM) INTO `OUTPUT_ORDER` (AN ORDERING GUARANTEE FOR
THE OUTPUT STREAM) - MEANING THAT IF THE ROWS IN THE INPUT STREAM ARE ORDERED
ACCORDING TO `INPUT_ORDER`, THEN THE ROWS IN THE OUTPUT STREAM WILL BE ORDERED
ACCORDING TO `OUTPUT_ORDER`.

THE ORDERING GUARANTEE OF THE TABLE READERS ALONG WITH THE CHARACTERIZATION
FUNCTIONS CAN BE USED TO PROPAGATE ORDERING INFORMATION ACROSS THE LOGICAL PLAN.
WHEN THERE IS A MISMATCH (AN AGGREGATOR HAS AN ORDERING REQUIREMENT THAT IS NOT
MATCHED BY A GUARANTEE), WE INSERT A **SORTING AGGREGATOR**.

### TYPES OF AGGREGATORS

- `TABLE READER` IS A SPECIAL AGGREGATOR, WITH NO INPUT STREAM. IT'S CONFIGURED
  WITH SPANS OF A TABLE OR INDEX AND THE SCHEMA THAT IT NEEDS TO READ.
  LIKE EVERY OTHER AGGREGATOR, IT CAN BE CONFIGURED WITH A PROGRAMMABLE OUTPUT
  FILTER.
- `JOIN` PERFORMS A JOIN ON TWO STREAMS, WITH EQUALITY CONSTRAINTS BETWEEN
  CERTAIN COLUMNS. THE AGGREGATOR IS GROUPED ON THE COLUMNS THAT ARE
  CONSTRAINED TO BE EQUAL.
- `JOIN READER` PERFORMS POINT-LOOKUPS FOR ROWS WITH THE KEYS INDICATED BY THE
  INPUT STREAM. IT CAN DO SO BY PERFORMING (POTENTIALLY REMOTE) KV READS, OR BY
  SETTING UP REMOTE FLOWS.
- `SET OPERATION` TAKES SEVERAL INPUTS AND PERFORMS SET ARITHMETIC ON THEM
  (UNION, DIFFERENCE).
- `AGGREGATOR` IS THE ONE THAT DOES "AGGREGATION" IN THE SQL SENSE. IT GROUPS
  ROWS AND COMPUTES AN AGGREGATE FOR EACH GROUP. THE GROUP IS CONFIGURED USING
  THE GROUP KEY. `AGGREGATOR` CAN BE CONFIGURED WITH ONE OR MORE AGGREGATION
  FUNCTIONS:
  - `SUM`
  - `COUNT`
  - `COUNT DISTINCT`
  - `DISTINCT`

  AN OPTIONAL OUTPUT FILTER HAS ACCESS TO THE GROUP KEY AND ALL THE
  AGGREGATED VALUES (I.E. IT CAN USE EVEN VALUES THAT ARE NOT ULTIMATELY
  OUTPUTTED).
- `SORT` SORTS THE INPUT ACCORDING TO A CONFIGURABLE SET OF COLUMNS.
  THIS IS A NO-GROUPING AGGREGATOR, HENCE IT CAN BE DISTRIBUTED ARBITRARILY TO
  THE DATA PRODUCERS. THIS MEANS THAT IT DOESN'T PRODUCE A GLOBAL ORDERING,
  INSTEAD IT JUST GUARANTEES AN INTRA-STREAM ORDERING ON EACH PHYSICAL OUTPUT
  STREAMS). THE GLOBAL ORDERING, WHEN NEEDED, IS ACHIEVED BY AN INPUT
  SYNCHRONIZER OF A GROUPED PROCESSOR (SUCH AS `LIMIT` OR `FINAL`).
- `LIMIT` IS A SINGLE-GROUP AGGREGATOR THAT STOPS AFTER READING SO MANY INPUT
  ROWS.
- `FINAL` IS A SINGLE-GROUP AGGREGATOR, SCHEDULED ON THE GATEWAY, THAT COLLECTS
  THE RESULTS OF THE QUERY. THIS AGGREGATOR WILL BE HOOKED UP TO THE PGWIRE
  CONNECTION TO THE CLIENT.

## PHYSICAL PLANNING

LOGICAL PLANS ARE TRANSFORMED INTO PHYSICAL PLANS IN A *PHYSICAL PLANNING
PHASE*. SEE THE [CORRESPONDING
SECTION](RFCS/20160421_DISTRIBUTED_SQL.MD#FROM-LOGICAL-TO-PHYSICAL) OF THE DISTRIBUTED SQL RFC
FOR DETAILS.  TO SUMMARIZE, EACH AGGREGATOR IS PLANNED AS ONE OR MORE
*PROCESSORS*, WHICH WE DISTRIBUTE STARTING FROM THE DATA LAYOUT - `TABLE
READER`S HAVE MULTIPLE INSTANCES, SPLIT ACCORDING TO THE RANGES - EACH INSTANCE
IS PLANNED ON THE LEASE HOLDER OF THE RELEVANT RANGE. FROM THAT POINT ON,
SUBSEQUENT PROCESSORS ARE GENERALLY EITHER COLOCATED WITH THEIR INPUTS, OR
PLANNED AS SINGLETONS, USUALLY ON THE FINAL DESTINATION NODE.

### PROCESSORS

WHEN TURNING A _LOGICAL PLAN_ INTO A _PHYSICAL PLAN_, ITS NODES ARE TURNED INTO
_PROCESSORS_. PROCESSORS ARE GENERALLY MADE UP OF THREE COMPONENTS:

![PROCESSOR](RFCS/IMAGES/DISTRIBUTED_SQL_PROCESSOR.PNG?RAW=TRUE "PROCESSOR")

1. THE *INPUT SYNCHRONIZER* MERGES THE INPUT STREAMS INTO A SINGLE STREAM OF
   DATA. TYPES:
   * SINGLE-INPUT (PASS-THROUGH)
   * UNSYNCHRONIZED: PASSES ROWS FROM ALL INPUT STREAMS, ARBITRARILY
     INTERLEAVED.
   * ORDERED: THE INPUT PHYSICAL STREAMS HAVE AN ORDERING GUARANTEE (NAMELY THE
     GUARANTEE OF THE CORRESPONDING LOGICAL STREAM); THE SYNCHRONIZER IS CAREFUL
     TO INTERLEAVE THE STREAMS SO THAT THE MERGED STREAM HAS THE SAME GUARANTEE.

2. THE *DATA PROCESSOR* CORE IMPLEMENTS THE DATA TRANSFORMATION OR AGGREGATION
   LOGIC (AND IN SOME CASES PERFORMS KV OPERATIONS).

3. THE *OUTPUT ROUTER* SPLITS THE DATA PROCESSOR'S OUTPUT TO MULTIPLE STREAMS;
   TYPES:
   * SINGLE-OUTPUT (PASS-THROUGH)
   * MIRROR: EVERY ROW IS SENT TO ALL OUTPUT STREAMS
   * HASHING: EACH ROW GOES TO A SINGLE OUTPUT STREAM, CHOSEN ACCORDING
     TO A HASH FUNCTION APPLIED ON CERTAIN ELEMENTS OF THE DATA TUPLES.
   * BY RANGE: THE ROUTER IS CONFIGURED WITH RANGE INFORMATION (RELATING TO A
     CERTAIN TABLE) AND IS ABLE TO SEND ROWS TO THE NODES THAT ARE LEASE HOLDERS FOR
     THE RESPECTIVE RANGES (USEFUL FOR `JOINREADER` NODES (TAKING INDEX VALUES
     TO THE NODE RESPONSIBLE FOR THE PK) AND `INSERT` (TAKING NEW ROWS TO THEIR
     LEASE HOLDER-TO-BE)).

TO ILLUSTRATE WITH AN EXAMPLE FROM THE DISTRIBUTED SQL RFC, THE QUERY:
```
TABLE ORDERS (OID INT PRIMARY KEY, CID INT, VALUE DECIMAL, DATE DATE)

SELECT CID, SUM(VALUE) FROM ORDERS
  WHERE DATE > 2015
  GROUP BY CID
  ORDER BY 1 - SUM(VALUE)
```

PRODUCES THE FOLLOWING LOGICAL PLAN:

![LOGICAL PLAN](RFCS/IMAGES/DISTRIBUTED_SQL_LOGICAL_PLAN.PNG?RAW=TRUE "LOGICAL PLAN")

THIS LOGICAL PLAN ABOVE COULD BE TRANSFORMED INTO EITHER ONE OF THE FOLLOWING
PHYSICAL PLANS:

![PHYSICAL PLAN](RFCS/IMAGES/DISTRIBUTED_SQL_PHYSICAL_PLAN.PNG?RAW=TRUE "PHYSICAL PLAN")

OR

![ALTERNATE PHYSICAL PLAN](RFCS/IMAGES/DISTRIBUTED_SQL_PHYSICAL_PLAN_2.PNG?RAW=TRUE "ALTERNATE PHYSICAL PLAN")


## EXECUTION INFRASTRUCTURE

ONCE A PHYSICAL PLAN HAS BEEN GENERATED, THE SYSTEM NEEDS TO DIVVY IT UP
BETWEEN THE NODES AND SEND IT AROUND FOR EXECUTION. EACH NODE IS RESPONSIBLE
FOR LOCALLY SCHEDULING DATA PROCESSORS AND INPUT SYNCHRONIZERS. NODES ALSO
COMMUNICATE WITH EACH OTHER FOR CONNECTING OUTPUT ROUTERS TO INPUT
SYNCHRONIZERS THROUGH A STREAMING INTERFACE.

### CREATING A LOCAL PLAN: THE `SCHEDULEFLOWS` RPC

DISTRIBUTED EXECUTION STARTS WITH THE GATEWAY MAKING A REQUEST TO EVERY NODE
THAT'S SUPPOSED TO EXECUTE PART OF THE PLAN ASKING THE NODE TO SCHEDULE THE
SUB-PLAN(S) IT'S RESPONSIBLE FOR (EXCEPT FOR "ON-THE-FLY" FLOWS, SEE DESIGN
DOC). A NODE MIGHT BE RESPONSIBLE FOR MULTIPLE DISPARATE PIECES OF THE OVERALL
DAG - LET'S CALL EACH OF THEM A *FLOW*. A FLOW IS DESCRIBED BY THE SEQUENCE OF
PHYSICAL PLAN NODES IN IT, THE CONNECTIONS BETWEEN THEM (INPUT SYNCHRONIZERS,
OUTPUT ROUTERS) PLUS IDENTIFIERS FOR THE INPUT STREAMS OF THE TOP NODE IN THE
PLAN AND THE OUTPUT STREAMS OF THE (POSSIBLY MULTIPLE) BOTTOM NODES. A NODE
MIGHT BE RESPONSIBLE FOR MULTIPLE HETEROGENEOUS FLOWS. MORE COMMONLY, WHEN A
NODE IS THE LEASE HOLDER FOR MULTIPLE RANGES FROM THE SAME TABLE INVOLVED IN
THE QUERY, IT WILL RUN A `TABLEREADER` CONFIGURED WITH ALL THE SPANS TO BE
READ ACROSS ALL THE RANGES LOCAL TO THE NODE.

A NODE THEREFORE IMPLEMENTS A `SCHEDULEFLOWS` RPC WHICH TAKES A SET OF FLOWS,
SETS UP THE INPUT AND OUTPUT [MAILBOXES](#MAILBOXES), CREATES THE LOCAL
PROCESSORS AND STARTS THEIR EXECUTION.

### LOCAL SCHEDULING OF FLOWS

THE SIMPLEST WAY TO SCHEDULE THE DIFFERENT PROCESSORS LOCALLY ON A NODE IS
CONCURRENTLY: EACH DATA PROCESSOR, SYNCHRONIZER AND ROUTER RUNS AS A GOROUTINE,
WITH CHANNELS BETWEEN THEM. THE CHANNELS ARE BUFFERED TO SYNCHRONIZE PRODUCERS
AND CONSUMERS TO A CONTROLLABLE DEGREE.

### MAILBOXES

FLOWS ON DIFFERENT NODES COMMUNICATE WITH EACH OTHER OVER GRPC STREAMS. TO
ALLOW THE PRODUCER AND THE CONSUMER TO START AT DIFFERENT TIMES,
`SCHEDULEFLOWS` CREATES NAMED MAILBOXES FOR ALL THE INPUT AND OUTPUT STREAMS.
THESE MESSAGE BOXES WILL HOLD SOME NUMBER OF TUPLES IN AN INTERNAL QUEUE UNTIL
A GRPC STREAM IS ESTABLISHED FOR TRANSPORTING THEM. FROM THAT MOMENT ON, GRPC
FLOW CONTROL IS USED TO SYNCHRONIZE THE PRODUCER AND CONSUMER. A GRPC STREAM IS
ESTABLISHED BY THE CONSUMER USING THE `STREAMMAILBOX` RPC, TAKING A MAILBOX ID
(THE SAME ONE THAT'S BEEN ALREADY USED IN THE FLOWS PASSED TO `SCHEDULEFLOWS`).

A DIAGRAM OF A SIMPLE QUERY USING MAILBOXES FOR ITS EXECUTION:
![MAILBOXES](RFCS/IMAGES/DISTRIBUTED_SQL_MAILBOXES.PNG?RAW=TRUE)

## A COMPLEX EXAMPLE: DAILY PROMOTION

TO GIVE A VISUAL INTUITION OF ALL THE CONCEPTS PRESENTED, WE DRAW THE PHYSICAL PLAN OF A RELATIVELY INVOLVED QUERY. THE
POINT OF THE QUERY IS TO HELP WITH A PROMOTION THAT GOES OUT DAILY, TARGETING
CUSTOMERS THAT HAVE SPENT OVER $1000 IN THE LAST YEAR. WE'LL INSERT INTO THE
`DAILYPROMOTION` TABLE ROWS REPRESENTING EACH SUCH CUSTOMER AND THE SUM OF HER
RECENT ORDERS.

```SQL
TABLE DAILYPROMOTION (
  EMAIL TEXT,
  NAME TEXT,
  ORDERCOUNT INT
)

TABLE CUSTOMERS (
  CUSTOMERID INT PRIMARY KEY,
  EMAIL TEXT,
  NAME TEXT
)

TABLE ORDERS (
  CUSTOMERID INT,
  DATE DATETIME,
  VALUE INT,

  PRIMARY KEY (CUSTOMERID, DATE),
  INDEX DATE (DATE)
)

INSERT INTO DAILYPROMOTION
(SELECT C.EMAIL, C.NAME, OS.ORDERCOUNT FROM
      CUSTOMERS AS C
    INNER JOIN
      (SELECT CUSTOMERID, COUNT(*) AS ORDERCOUNT FROM ORDERS
        WHERE DATE >= '2015-01-01'
        GROUP BY CUSTOMERID HAVING SUM(VALUE) >= 1000) AS OS
    ON C.CUSTOMERID = OS.CUSTOMERID)
```

A POSSIBLE PHYSICAL PLAN:
![PHYSICAL PLAN](RFCS/IMAGES/DISTRIBUTED_SQL_DAILY_PROMOTION_PHYSICAL_PLAN.PNG?RAW=TRUE)
