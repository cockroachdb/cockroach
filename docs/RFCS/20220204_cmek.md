- Feature Name: **Customer-Managed Encryption Keys**
- Status: in-progress
- Start Date: 2021-12-01 
- Last updated: 2022-02-18
- Authors: Adwit Tumuluri (adwittumuluri), David Ding (davidwding)
- RFC PR: [#76086](https://github.com/cockroachdb/cockroach/pull/76086)
- Jira Issue: [CC-3879](https://cockroachlabs.atlassian.net/browse/CC-3879)

This RFC was migrated from [this Google
Doc](https://docs.google.com/document/d/1_Srgo9cWo4aV3Dwjp7FHLrx5Nt0XIdN0BaJakE-WGUU/edit#heading=h.f2jjkj9qtz0i).

# Objective

This is the technical RFC that covers the workflows discussed in the [customer-facing workflows doc](https://docs.google.com/document/d/1bXtIT43-zEFZdMLHK-_FEvEIkuZ8s1y7VB97bG4np1c/edit#heading=h.b4dtrvivx5fe). The implementation is based on the initial [CMEK technical brief](https://docs.google.com/document/d/1VleJa-V6Fh-5xv9Rt7Qpy-LfXNqwH9uDu2pSWx0LNxk/edit#heading=h.bavnuhujjvfn).

# Background

## Overview

CMEK introduces the ability for customers to provide CockroachCloud their own set of keys that can be used to encrypt and decrypt data within CC-managed CRDB clusters. At a high level, the customer configures keys at a regional level, providing their keys' data via the CC API as part of the [April 1st MVP](https://docs.google.com/document/d/17Exh_0yoIUwdRtunAq3wAFJZX9spKY2J6tT62f46oVA/edit#heading=h.qkl77pen402r), and the UI after the initial release. This RFC fleshes out how CC plans to support AWS KMS and GCP Cloud KMS<sup>1</sup>. Note that CMEK will be supported on only dedicated clusters, with no plans for serverless at this point in time.

<sup>1</sup> Designing GCP Cloud KMS integration is a work in progress, but will fall in line with the AWS approach detailed in this doc.

## Terminology

CMEK introduces many overloaded concepts and terms. Often, entities can be referred to as different things depending on their context. To avoid ambiguity, let's expand on the terms in the glossaries given [here](https://docs.google.com/document/d/1xEb0wBgEfgYEjafzARiGqR-WhTT2DKHq9er0lrKTSxY/edit#heading=h.7uf0wkrjz2lh) and [here](https://docs.google.com/document/d/1Q9zxfsEPbYwib-lY8D19mAnG7FBq1jzjopnZYpQ2Kio/edit#heading=h.ii1hgiakkpqf):

| Term | Description |
| --- | --- |
| KMS | An abbreviation for key management system. Examples include [Amazon KMS](https://aws.amazon.com/kms/), [Google Cloud KMS](https://cloud.google.com/kms/docs), and [Azure Key Vault](https://azure.microsoft.com/en-us/services/key-vault/). In this RFC, KMS colloquially refers to Amazon KMS. |
| KMS Key | This is the encryption key that resides within the customer's KMS and is otherwise known as the customer-managed encryption key. This is technically a key encryption key (KEK; [Amazon docs](https://docs.aws.amazon.com/whitepapers/latest/kms-best-practices/using-aws-kms-at-scale.html)), but this doc will use "KMS key" to distinguish it from the CC store key. |
| CC Store Key | This is the CockroachCloud-managed key that's provided to each CockroachDB node/process via the key and old-key fields ([docs](https://www.cockroachlabs.com/docs/stable/encryption.html)). Technically, this is another KEK used to encrypt the CockroachDB-managed data keys. |
| enc/dec | Short for encryption/decryption. Usually used in the context of encryption/decryption permissions ([AWS docs](https://docs.aws.amazon.com/kms/latest/APIReference/API_Encrypt.html)). |
| customer | This refers to the CockroachCloud customer. This is important because "user" generally means the [AWS IAM concept](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html). |
| CC | Cockroach Cloud |
| EKS | Amazon’s Elastic Kubernetes Service |
| GKE | Google Kubernetes Engine |
| AKS | Azure Kubernetes Service |
| EAR | Encryption at Rest |

## High-Level Design Components

![High level design components](images/cmek-high-level-design-components.png?raw=true)

## Workflow Overviews

### Customer Workflows

#### [MVP] The customer wants to enable CMEK on a CC Dedicated cluster

The inception of a CMEK-enabled CC cluster begins with creating a vanilla CRDB cluster with CC. This creates a cluster within an AWS account or GCP project. The customer must then "trust" this account/project to access their KMS, then enable CMEK by providing CC the KMS data that must be used in each region of their CRDB cluster. This workflow is fleshed out in detail in the [permission flow](#permission-flow).

#### [MVP] The customer wants to revoke keys for a CMEK-enabled Dedicated cluster

The customer reserves the right to lock everyone, [including CC](https://docs.google.com/document/d/1VleJa-V6Fh-5xv9Rt7Qpy-LfXNqwH9uDu2pSWx0LNxk/edit#heading=h.865vmco1z371) and themselves, out of the data stored in their clusters. We will implement an [explicit endpoint](#revoking-all-cmek-keys) to revoke CMEK keys that will automatically terminate running CRDB nodes and mark a cluster as revoked.

#### [Post-MVP] The customer wants to rotate a KMS key for an existing CMEK-enabled CC Dedicated cluster

The customer will send us a request via the CC API indicating the keys for the regions they wish to rotate. Within each region, the kubernetes operator will decrypt the original CC-owned store key via the previous KMS, and encrypt it with the new one. The endpoint triggering this flow will be added post-MVP, and is described in detail in the [API section](#adding-or-rotating-cmek-keys-not-for-mvp).

Note that rotation of a KMS key does not necessarily correspond to rotation of the underlying CRDB store key. Rotation of a KMS key means that the store key is re-encrypted using the new KMS key, but the store key itself does not need to change. We may decide that we want to rotate the store key anyway whenever the KMS key is rotated, but this is a post-MVP discussion.

#### [Post-MVP] The customer wants to disable CMEK for a Dedicated cluster.

In the event that a customer wants to turn off encryption-at-rest for a CMEK-enabled cluster, they can use an [endpoint](#disabling-a-cmek-not-for-mvp) that will be added post-MVP.

### CRDB Workflows

#### [MVP] Store Key Generation

In this option, the operator / encryption application will generate the store key and encrypt it using the customer KMS. Encrypted store keys will be stored in a k8s secret. When a CRDB node starts up and decrypts the store key, it will also store a backup of the encrypted key on the CRDB node's persistent volume.

To start, store key generation may be done in Intrusion, but this is explicitly a temporary stopgap while the operator is developed.

##### Store Key Generation Sequence Diagram

![Store key generation sequence diagram](images/cmek-orchestration-key-generation-sequence.png?raw=true)

##### Store Key Usage Sequence Diagram

![Store key usage sequence diagram](images/cmek-orchestration-key-usage-sequence.png?raw=true)

#### [MVP] Rolling Restart

A rolling restart of a CRDB cluster is necessary to actually enable Encryption At Rest on an unencrypted cluster. This functionality will live within Intrusion to start, and eventually move to the operator.

##### Enabling CRDB EAR Post-MVP

Currently, CRDB relies on LSM compaction to lazily encrypt data added before EAR is enabled. As such, we'll need to advise customers to refrain from writing to their clusters before they've provided CC their KMS configuration. Post-MVP, CC will provide the customer a mechanism to signal their intent to enable CMEK on a new cluster. Given this intent, CC will hold off on starting nodes prior to the customer sending an [EnableCMEK](#enabling-new-cmek-keys) request. This "awaiting" state would be useful, if not required, when adding regions to a cluster already enabled for CMEK.

#### [Post-MVP] New Region Added to Cluster

Any new region for a CMEK-enabled cluster should also have a CMEK key associated with it. This will be addressed post-MVP via the [PUT API](#adding-or-rotating-cmek-keys-not-for-mvp).

#### [Post-MVP] CRDB Store Key Rotation

We should rotate the store keys occasionally, probably once every few months. More frequently is unnecessary, as noted on the CRDB encryption [documentation](https://www.cockroachlabs.com/docs/stable/encryption.html): "Since very little data is encrypted using this key, it can have a very long lifetime without risk of reuse."

The entity responsible for generating the initial store key will also be responsible for rotating the store key. Initially, this means Intrusion; eventually, this will be done on the operator.

# Detailed Design

## Permission Flow

The KMS key resides in an AWS account that the customer owns. The entity using the customer's KMS key resides in an AWS account that we, as CockroachCloud, own. Simply put, some principal that CockroachCloud owns needs to be granted encryption and decryption permissions using the customer's KMS key. At a very high level, these are the steps that accomplish this:

<ol start="0">
<li>The customer creates a KMS key in their AWS account and a KMS role that has enc/dec permissions on their KMS key.</li>
</ol>

1. The customer creates a CC cluster on AWS.

2. The CC cluster creation flow creates a per-region CC-owned AWS user.

3. The customer retrieves the AWS account number for their CC cluster via the CC API.

4. The customer enters a trust relationship between the customer-owned role with enc/dec permissions and AWS account  for their CC cluster, with a sts:ExternalId condition containing the customer's CC's org ID.

5. The customer provides CC the ARNs for their KMS key and their enc/dec role via the API.

![Permissions flow](images/cmek-permissions-flow.png?raw=true)

### Step 1: The customer creates a CC cluster on AWS.

The customer can trigger the creation of an AWS CC cluster via the existing management console UI or an API (if it exists). Note that, at this step, there's no further information required (e.g. ARN, AWS account ID).

### Step 2: The CC cluster creation flow creates a per-region CC-owned AWS user.

Here, we must create the entity that the cluster will authenticate as to query the customer's KMS. Note that though this principal will be created for every new cluster, it will not be used until CMEK is enabled with a KMS data (AWS KMS key ARN, GCP Cloud KMS key/keyring, etc.). The existence of a placeholder principal prevents the need to trigger separate Pulumi logic to create a new entity at CMEK-enablement time.

Given this new principal, we reach the first decision point regarding how its enc/dec privileges should propagate to the k8s clusters.

#### Option 1: Attach the principal as a service-linked role directly to the EC2s and/or EKS cluster

AWS provides an easy way to assume a role within EKS clusters using a service-linked role ([docs](https://docs.aws.amazon.com/eks/latest/userguide/using-service-linked-roles.html); [entrypoint](https://docs.aws.amazon.com/sdk-for-go/api/service/kms/#New)). This option is fairly straightforward to implement within [Pulumi](https://www.pulumi.com/registry/packages/aws/api-docs/iam/servicelinkedrole/), and the user can "trust" this role within their KMS's [key-policy.](https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html) 

##### Pros

- Has a well defined implementation path - everything's seamless within AWS.

- Avoids any reason to store role/user credentials within k8s secrets; credential management is not CC's problem.

##### Cons

- This route breaks parity with the public operator by coupling KMS too tightly with EKS. With service-linked roles, AWS is responsible for creating/rotating the role credentials and injecting them into the CRDB pods. This alienates GKE, Azure AKS, self-hosted operators, etc. who must somehow assume the responsibility of rotating and injecting credentials given a KMS. This argument is symmetric with any KMS/k8s combination, e.g. GCP Cloud KMS/EKS.

#### Option 2: Store principal's credentials in k8s secrets [Adwit's preference; selected]

We can use Pulumi to [create IAM users per-region](https://www.pulumi.com/registry/packages/aws/api-docs/iam/user/), retrieve their credentials, and store them in the regional k8s secrets for the orchestration layer to authenticate as and query the customer's KMS.

##### Why create a user vs a role?

Traditionally, we'd create an [AWS role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) to assume and perform an action on some resource. Existing workflows in the control plane query the Vault instance ([example](https://github.com/adwittumuluri/managed-service/blob/c7121c61ecf80ffdbe5581f90271e95c3f3c0c64/pkg/vault/vault_aws.go#L325)) to create and retrieve credentials for the role (via the [AWS Secrets Engine](https://www.vaultproject.io/docs/secrets/aws)). However, role credentials have a maximum TTL of [12 hours](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use.html). The orchestration layer runs in the data plane and cannot leverage Vault to retrieve refreshed role credentials. It follows that we need some sort of  "long-lived"  (see [cons](#cons-1)) credentials hosted within the data plane itself; [IAM users](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html), unlike roles, can authenticate via several methods, including long-lived access keys and SSL certs.

##### Why do we need an IAM user per region?

IAM Users allow us the ability to self-rotate credentials, thus making it easy to extend KMS functionality to the public operator and other clouds (we're not going the [service-linked role approach](#option-1-attach-the-principal-as-a-service-linked-role-directly-to-the-ec2s-andor-eks-cluster)). AWS IAM Users are allowed [2 sets of valid credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html) at any point in time (GCP service accounts similarly are allowed 10). This means, in a multi-region deployment, we'll need one CC-owned user per region, each of which the customer must add in their trust relationship with their KMS for the region.

##### Pros

- This is also fairly simple to do within Pulumi ([creating user](https://www.pulumi.com/registry/packages/aws/api-docs/iam/user/), [creating a secret](https://www.pulumi.com/registry/packages/kubernetes/api-docs/core/v1/secret/)).

- We've decoupled k8s from the vendor - now, provided that we can store the credentials for an AWS user within a k8s secret<sup>2</sup>, AWS KMS support should be trivial (as compared to [Option 1](#option-1-attach-the-principal-as-a-service-linked-role-directly-to-the-ec2s-andor-eks-cluster)) to extend to GCP GKE, Azure AKS, etc. Same goes with other cloud vendor KMS's on EKS clusters. The public operator would have no dependency on service-linked roles as well.

##### Cons

- We're storing and managing user credentials in a k8s secret ourselves<sup>3</sup>. However, there are [well-documented mechanisms](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_iam_credentials_console.html) to allow users to rotate these credentials, similar to what [Vault does with GCSE](https://www.vaultproject.io/docs/secrets/gcp#root-credential-rotation). This can boil down to something as simple as a k8s cronjob calling a [bash script](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#rotating_access_keys_cli) and writing a new set of credentials in a new k8s secret (though we'd likely go the AWS API route considering testability).

<sup>2</sup>This, however, is still a bit tricky. Where does the privileged AWS user live when using AWS KMS with GCP CC clusters? Further discussion may be needed around where to house these users, or we can consider [the Alternative Approach to Permissioning](#the-alternative-approach-to-permissioning-the-customer-owns-the-aws-user).

<sup>3</sup>This is largely a defense-at-depth argument - in both [Option 1](#option-1-attach-the-principal-as-a-service-linked-role-directly-to-the-ec2s-andor-eks-cluster) or [Option 2](#option-2-store-principals-credentials-in-k8s-secrets-adwits-preference-selected), anyone with kubectl access to the cluster (SRES, Cloud Foundations, Orchestration team (?)) can query KMS as the privileged role/user and decrypt the store key.

### Step 3: The customer retrieves the AWS account number for their CC cluster via the API.

This is with the [**Cluster GET API**](https://www.cockroachlabs.com/docs/api/cloud/v1#operation/CockroachCloud_GetCluster) that can be used for all CC clusters, regardless of cloud vendor. This is mentioned [in the customer-facing workflow](https://docs.google.com/document/d/1bXtIT43-zEFZdMLHK-_FEvEIkuZ8s1y7VB97bG4np1c/edit#heading=h.2hxmkvigmgf).

### Step 4: The customer grants the CC principal within the CC AWS account enc/dec permissions for their KMS key.

Given the AWS account ID of the user from step 3, the customer can declare a trust relationship between their KMS account and CC's AWS account. We can instruct the user to enter into a trust relationship with our per-region IAM users, described in the docs [here](https://docs.aws.amazon.com/IAM/latest/UserGuide/roles-managingrole-editing-cli.html#roles-managingrole_edit-trust-policy-cli):

`$ aws iam update-role {...}`

In order for the orchestration to acquire permissions, it would need to authenticate with the created user and assume the customer’s role. For example, this would be the customer’s trust policy for their KMS role assumed within their CC cluster:

```
{
  ...
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::[crl-account-id]:root"
      },
      "Action": "sts:AssumeRole"
      "Condition":{
        "StringEquals":{
          "sts:ExternalId":[
            "CUSTOMER-ORG-ID-WITHIN-CRL"
          ]
        }
      }
    }
  ...
  ]
}
```

Alternatively, if the customer prefers to enumerate the regional IAM users for a specific KMS, they can trust per-region users as follows:

```
{
      ...
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam::[crl-account-id]:user/CRLOwnedUserAccessingOurKMS-region1",
          "arn:aws:iam::[crl-account-id]:user/CRLOwnedUserAccessingOurKMS-region2"
        ]
      },
      ...
}
```

#### A note about key policies

Originally, we were considering using [KMS key policies](https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html) to allow CC's IAM users to access the customers KMS. We've decided against this as customers are more used to declaring well-defined trust relationships at the account level vs. the resource level.

### Step 5: The customer provides CC the ARNs for their KMS key and their enc/dec role via the API.

This can be done via the Cluster CMEK POST API call mentioned [here](#enabling-new-cmek-keys). At this point, once the customer's key and role ARNs propagate to the k8s cluster, the cluster itself should have permissions to use them.

## Creation of Encrypted Store Keys

For simplicity, we plan to create one store key per region, which will be shared across all CRDB nodes in the region. This store key will be encrypted using AWS KMS to start, and stored in a k8s secret with a backup in each CRDB volume alongside the data keys.

### Short-Term vs. Long-Term

The operator, for all practical purposes, does not yet exist as a way to manage CC dedicated clusters. Much of the logic that will eventually live inside the operator currently lives in Intrusion. The timeline of the operator is a bit unclear at the moment and we shouldn't rely on it being ready in time for our customer commit.

As such, for the MVP, store key generation will be mindfully written into Intrusion, in a way that makes it easy to drag-and-drop into the operator when that is ready to go. In the longer term, having store key generation done in the operator is definitely the right design.

### Short-Term MVP

**For the MVP but not long-term**, Intrusion will be responsible for generating, encrypting, and storing store keys. This is because Intrusion is currently entirely responsible for spinning up a CRDB cluster, and CMEK functionality is a part of that. The code will be written with a goal of making it easy to eventually separate out into the operator.

Steps:

1. Intrusion does region setup.
2. Intrusion sets up an encrypted store key.
    1. Intrusion generates a store key of size 32 bytes with key file size 64 bytes ([32 bytes for the key ID, 32 bytes for the key itself](https://www.cockroachlabs.com/docs/stable/encryption.html#generating-store-key-files))
    2. Intrusion fetches the necessary AWS credentials from the k8s secret that they are stored on and calls [AssumeRole](https://docs.aws.amazon.com/sdk-for-go/api/service/sts/#STS.AssumeRole) to acquire permissions to access AWS KMS.
    3. Intrusion encrypts the store key using the AWS KMS library, initializing a client and calling the [Encrypt](https://docs.aws.amazon.com/sdk-for-go/api/service/kms/#KMS.Encrypt) func with the CMEK key ID.
    4. Intrusion places the encrypted store key in a k8s secret on the region, using the [SecretInterface](https://pkg.go.dev/github.com/kubernetes/client-go@v11.0.0+incompatible/kubernetes/typed/core/v1#SecretInterface). The name of the secret will be something along the lines of "storeKey.v1". The secret should have "Opaque" [type](https://kubernetes.io/docs/concepts/configuration/secret/#secret-types) with entries in the data field of form ["encryptionType": "plain"/"AES", "encryptedKey": (encrypted key), "kmsPlatform": "AWS"/"GCP", "kmsExternalId": (customer's external ID)].
3. Intrusion creates the CRDB cluster. How the individual CRDB nodes decrypt the key is detailed in a later section.

Note that backup of the encrypted store key to disk will happen when the CRDB cluster is starting up, as part of each pod's init container. This is because the disks may not yet be created/accessible at the point that the store key is initially encrypted.

### Long-Term Operator Integration

For eventual operator integration, we assume that the Cloud Foundations team has provisioned a region and k8s nodes within the region. We also assume the existence of a running operator which is able to deploy a StatefulSet of CRDB nodes and initialize them.

In order to integrate CMEK into this, we need to add a few fields to the Spec of the operator object:

- CMEKEnabled bool
- CMEKInfo struct
    - necessary authentication info for AWS/GCP
    - key ID

With each call to Reconcile, the operator will check if CMEK is enabled for the CRDB cluster, and if an encrypted store key exists for the provided key ID. If not, the operator will generate, encrypt, and store a store key using the same steps as Intrusion detailed in the previous section.

### AWS Credentials Flow

This section goes into extreme detail on how one will be able to get credentials for a customer's AWS KMS, assuming that permissions are set up as detailed [above](#permission-flow).

In general, AWS credentials consist of AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and an optional AWS_SESSION_TOKEN. These are the steps required:

Assume the role that allows us to access KMS

1. Fetch AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY from k8s secret. Note that keys are associated with our own AWS account and not the customer's.

2. Create [session](https://docs.aws.amazon.com/sdk-for-go/api/aws/session/#NewSession) A, passing in [NewStaticCredentials](https://docs.aws.amazon.com/sdk-for-go/api/aws/credentials/#NewStaticCredentials) as documented [here](https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/configuring-sdk.html). Blank SessionToken seems fine

3. Create [STS client](https://docs.aws.amazon.com/sdk-for-go/api/service/sts/#New), passing in session A

4. Call STS.[AssumeRole](https://docs.aws.amazon.com/sdk-for-go/api/service/sts/#STS.AssumeRole), [passing in](https://docs.aws.amazon.com/sdk-for-go/api/service/sts/#AssumeRoleInput) DurationSeconds, ExternalId, RoleArn, RoleSessionName. Returns [this](https://docs.aws.amazon.com/sdk-for-go/api/service/sts/#AssumeRoleOutput). This gets us credentials for the customer's KMS.

Interact with KMS

5. Create a new session B, again passing in NewStaticCredentials, but this time with the credentials received from AssumeRole. SessionToken must be provided this time.

6. Create [KMS client](https://docs.aws.amazon.com/sdk-for-go/api/service/kms/#New), passing in session B

7. Interact with customer's KMS with Encrypt/Decrypt/etc., passing in key ARN, etc.

On our side, we need to persist:

- Our own AWS_ACCESS_KEY_ID

- Our own AWS_SECRET_ACCESS_KEY

- ExternalId, the unique ID generated by us for the customer's account. This will likely end up being the Cluster ID or the customer's org ID. If this is easily accessible through other means, we may not need to persist it separately.

## CRDB Node Initialization

When a CRDB node is being started, we need to decrypt the encrypted store key and pass it into the start command. Again, there is a short-term flow based on Intrusion, and long-term flow based on the Operator. In both cases, we plan to use the [init container](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) functionality provided by k8s.

### Init Container

The init container will run on the same pod as the CRDB container, and will perform the following steps:

1. Read the encrypted store key from the k8s secret, provided in the form of an [environment variable](https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables).

2. Decrypt the store key using AWS KMS, whose requisite credentials will be stored in a k8s secret and acquired using AssumeRole.

3. Write the store key to an ephemeral [emptyDir](https://kubernetes.io/docs/concepts/storage/volumes/#emptydir) volume with `emptyDir.medium` set to `Memory`, shared with the CRDB container.

4. Write the encrypted store key to the persistent CRDB volume as a backup.

After the init container runs, the CRDB container will start, getting the decrypted key from the ephemeral emptyDir volume as part of the `cockroach start` command.

Ideally, the decrypted store key should be removed from the ephemeral volume once CRDB starts successfully. It's not immediately clear how to do this - one would have to start some separate process that makes a single `rm` call.

## Revoke a CMEK-Enabled Cluster

One of the major points of this project is to give customers the ability to revoke all access to their Cockroach DB cluster. In order to do this, a customer will need to revoke our access to their KMS, and then inform us that CMEK has been revoked for the cluster using the appropriate endpoint.

When the `DELETE /clusters/{clusterId}/cmek` endpoint is called, it is sufficient to delete each of the pods running CRDB. The only place we are storing the decrypted key is on the ephemeral emptyDir volume on each pod, which is [deleted permanently](https://kubernetes.io/docs/concepts/storage/volumes/#emptydir) when the pod is removed.

By deleting the only volume where decrypted keys are stored, and removing our ability to decrypt the encrypted store keys, we disable a CMEK-enabled cluster.

## Cockroach Cloud API Endpoints

There is a [public API](https://www.cockroachlabs.com/docs/api/cloud/v1) under development (which will go into public preview before the CMEK MVP) that customers will be able to use to interact with their dedicated clusters. need to add several CMEK-related endpoints to this API for the CMEK MVP, as defined below ([original API reference](https://docs.google.com/document/d/1xEb0wBgEfgYEjafzARiGqR-WhTT2DKHq9er0lrKTSxY/edit#heading=h.rfd5ud7zkwc2)).

### Getting all the CMEK keys and their associated statuses

`GET /clusters/{cluster_id}/cmek`

#### Description

Returns the cluster's CMEK status and associated keys within each region.

#### Parameters

| Parameter | Type | Description | Example Values |
| --- | --- | --- | --- |
| `status_filter` [post-MVP] | `CMEKStatus`<br>string | If true, return information about all CMEKs regardless of status, including ones that are DISABLED. | `[true\|false]` |

#### Response codes
`200` on successful `GET`

#### Response

| Field | Type | Description | Example Values | 
| --- | --- | --- | --- |
| `cmek_info` | `CMEKInfo` | All the information related to CMEK keys and their status used for the given cluster. | <pre>{<br> status: ENABLING,<br> key_info: [<br>  {<br>   cmek_key_id : {cmek_key_id},<br>   status : ENABLED,<br>   region : us-east-1,<br>   key_data : {<br>    kms_type : AWS_KMS,<br>    kms_key_id : arn:kms:key/beef,<br>    kms_auth_principal: arn:CustRole,<br>   }<br>  },<br>  {<br>   cmek_key_id : {cmek_key_id},<br>   status : ENABLING,  <br>   region : ap-east-1,<br>   key_data : {<br>    kms_type: GCP_CLOUD_KMS,<br>    kms_key_id : /location/keyring/key,<br>    kms_auth_principal:NONE,<br>   }<br>  },<br>  {<br>   cmek_key_id : {cmek_key_id},<br>   status : ENABLED,  <br>   region : eu-central-1,<br>   key_data : {<br>    kms_type: AZURE_KEY_VAULT,<br>    kms_key_id : vault+key_name+version,<br>    kms_auth_principal: {svc_prncpl_id},<br>   }<br>  }<br> ]<br>}<br></pre> |

### Enabling new CMEK keys

`POST /clusters/{cluster_id}/cmek`

#### Description

Enabling CMEK with the provided per-region CMEKs ([see usability discussion on slack](https://cockroachlabs.slack.com/archives/C02Q8MLFFE0/p1641937074007300?thread_ts=1641923502.004100&cid=C02Q8MLFFE0)).

#### Parameters

| Parameter | Type | Description | Example Values |
| --- | --- | --- | --- |
| `cmek_keys` | `[]CMEKKey` | Cloud-vendor specific data identifying the customer-managed key and how (what principal) to use it. | <pre>[<br> {<br>  region : us-east-1,<br>  key_data : {<br>   kms_type: AWS_KMS,<br>   kms_key_id : arn:kms:key/beef,<br>   kms_auth_principal : arn:CustRole,<br>  }<br> },<br> {<br>  region : ap-east-1,<br>  key_data : {<br>   kms_type : GCP_CLOUD_KMS,<br>   kms_key_id : /location/keyring/key,<br>   kms_auth_principal : NONE,<br>  }<br> },<br> {<br>  region : eu-central-1,<br>  key_data : {<br>   kms_type : AZURE_KEY_VAULT,<br>   kms_key_id : vault+key_name+version,<br>   kms_auth_principal: {svc_prncpl_id},<br>  }<br> }<br>]</pre> |

#### Response codes

`201` on successful `GET`

`TODO` Error codes upon sanity checks (bad region, badly formatted key_data…).

#### Response

| Field | Type | Description | Example Values | 
| --- | --- | --- | --- |
| `cmek_info` | `CMEKInfo` | All the information related to CMEK keys and their status used for the given cluster. | <pre>{<br> cmek_status: ENABLING,<br> cmek_key_info: [<br>  {<br>   cmek_key_id : {cmek_key_id},<br>   status : ENABLING,<br>   region : us-east-1,<br>   key_data : { ... }<br>  }<br> ]<br>}<br></pre> |

### Revoking all CMEK keys

`PATCH /clusters/{clusterId}/cmek`

#### Description

Informs us that the list of keys has been revoked or re-enabled (un-revoked) for the cluster. Importantly, this does not actually revoke the existing keys; that responsibility falls onto the customer. The customer should revoke the keys on their end, and then call this endpoint so that we can disable the cluster on our side.

After this is called, customers should retrieve the status of the cluster using other endpoints to track the status of the cluster disablement.

#### Parameters 

| Parameter | Type | Description | Example Values |
| --- | --- | --- | --- |
| `action` | `CMEKKeyAction` | Whether to revoke or re-enable the keys. | `REVOKE\|RE-ENABLE` |

#### Response codes

`200` on successful `DELETE`

`TBD` Errors if we’re trying to re-enable keys that aren’t revoked, or revoke keys that aren’t active.

#### Response 

| Field | Type | Description | Example Values | 
| --- | --- | --- | --- |
| `cmek_info` | `CMEKInfo` | All the information related to CMEK keys that were affected by this update. | <pre>{<br> cmek_status: REVOKING,<br> cmek_key_info: [<br>  {<br>   cmek_key_id : {cmek_key_id},<br>   status : REVOKING,<br>   region : us-east-1,<br>   key_data : { ... }<br>  },<br>  {<br>   cmek_key_id : {cmek_key_id},<br>   status : REVOKING,<br>   region : us-east-1,<br>   key_data : { ... }<br>  }<br> ]<br>}<br></pre> |

### Disabling a CMEK [Not for MVP]

`DELETE /clusters/{clusterId}/cmek`

#### Description

WIP, not for MVP. Bulk disable.

### Adding or Rotating CMEK Keys [Not for MVP]

`PUT /clusters/{clusterId}/cmek`

#### Description

WIP, not for MVP. Bulk rotate or add, even to a new region. Very similar to enable, but PUT.

## Control Plane Data Model

We'll be introducing a new table, cmek_keys ([original](https://docs.google.com/document/d/1xEb0wBgEfgYEjafzARiGqR-WhTT2DKHq9er0lrKTSxY/edit#heading=h.62hq3lfvcwv0)):

- ID (cmek_key_id)
- cluster_region_id: `<intrusion_crdb_region_id>`
- key_type: `AWS_KMS|GCP_CLOUD_KMS`
- key_uri `arn:...` `prjct/...`
- key_auth_principal: `arn:...` `SA@project`
- status: `ENABLED|DISABLED|REVOKED |ENABLING|DISABLING|REVOKING|FAILED`
- created_at
- updated_at

In addition, we’ll be adding a `cmek_status` column in `intrusion_crdb_clusters`.

cmek_status:
- ENABLED
- DISABLED
- REVOKED
- ENABLING
- DISABLING
- REVOKING
- FAILED

### Hooking into the Intrusion Job System

[Here's](https://github.com/cockroachlabs/managed-service/blob/b9064db94b08021ce940b8101e7b7cbc27c23f69/intrusion/server/jobs.go#L82) a list of jobs:

- EnableCMEK { cluster_id, region, CMEK Key Data [WIP] }
    - Does lookup for cluster_region_id
    - Adds {intrusion_crdb_region_id,key_data,enabling}
    - Updates previous enabled to {intrusion_crdb_region_id,key_data,disabling} (if it exists)
    - Instructs operator to enable new CMEK (and disable previous one)
    - Changes state to enabled for new record and disabled for old record.
- RevokeCMEK
    - Does lookup for cluster_region_id
    - Updates previous enabled to {intrusion_crdb_region_id,key_data, revoking}
        - Error if previous doesn't exist (CMEK was never enabled)
    - Instructs operator to revoke.
    - Changes state from revoking to revoked.
- DisableCMEK (not in MVP)
    - Basically get back to "plaintext encryption" (meaning nothing is encrypted)

## Handling Key Rotation

Key rotation is not part of the MVP, but is probably the top-priority thing after the MVP. As such, the details are not entirely filled out yet.

At a high level, there are three levels of key rotation:

1.  Customer rotation of CMEK on KMS
2.  Rotation of per-region CRDB store key
3.  Rotation of underlying data key

The 3rd level of rotation is at a layer below anything we have to worry about, beyond possibly specifying a rotation period that isn't the default of 1 week. We will need to build support for the 1st and 2nd levels of key rotation.

### AWS

For a high-level overview, there is lots of context on rotating AWS KMS keys in their [documentation](https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html).

#### Automatic Key Rotation

In AWS KMS, it is possible to enable automatic key rotation. If a customer has this enabled, there is absolutely nothing we need to do on our side (at least for the MVP). If a key is automatically rotated on their side, older keys are stored in perpetuity and can be used to decrypt older data.

Past the MVP, we may want to regularly re-encrypt keys (even if using the same ARN), so that we can use freshly rotated keys instead of relying on old ones forever.

Note: One thing to be mindful of is that auto CMEK rotation in GCP generates a customer-facing version, which could be disabled. So we should document clearly that it shouldn't be disabled, else we would lose access to earlier backups (or maybe that doesn't matter). Anyways, it's of concern after figuring out backup encryption on GCP in general.

#### Manual Key Rotation

TODO

### GCP

- [This page](https://cloud.google.com/security/key-management-deep-dive/) is really useful

- When a key is rotated, a new version is created and made the primary version.
- New data will be encrypted with the new version.
- Older versions can be used to decrypt data.
- Data encrypted using older key versions will remain encrypted using the older version unless explicitly re-encrypted.
- When decrypting, there's a field in the [response](https://pkg.go.dev/google.golang.org/genproto/googleapis/cloud/kms/v1#DecryptResponse) that we can use to determine if we're on the most recent rotated version of the key

## Handling Backup and Restore

[Prior discussions](https://docs.google.com/document/d/1xEb0wBgEfgYEjafzARiGqR-WhTT2DKHq9er0lrKTSxY/edit#heading=h.7y6vtjwueddr) alluded to cloud vendors' abilities to attach a KMS to their buckets. But CockroachDB natively provides [a way to encrypt backups with KMS](https://www.cockroachlabs.com/docs/stable/take-and-restore-encrypted-backups.html?#use-aws-key-management-service). This, as with [Option 2](#option-2-store-principals-credentials-in-k8s-secrets-adwits-preference-selected) of the permissioning flow, decouples using KMS from AWS CC clusters, and, more generally, enables cross-KMS/vendor support (as long as CRDB supports the KMS; GCP Cloud KMS support is being [discussed](https://cockroachlabs.slack.com/archives/C2C5FKPPB/p1641496790004500); [spec sheet](https://docs.google.com/document/d/1cgvHCuIgZ7tgwJB9qbhk_nJZz5auaLlYhFYud_6GPCY/edit)).

The implementation boils down to a straightforward [edit in the backupper](https://github.com/cockroachlabs/managed-service/blob/1fcef2a44f3aa4d2074f0c818557474d1a251141/services/backupper/database.go#L80). The credentials for the command would need to be for the customer's role. This means we'd need to go through an [AssumeRole process](#aws-credentials-flow) to retrieve them as detailed above. GCP would look in a similar fashion - we'd need to provide the credentials for the enc/dec service account stored in the k8s secret.

### Note: What happens when the backupper moves to a different region upon a remove-region?

The backupper is effectively a k8s cron job that calls CRDB's "BACKUP INTO...". It runs in one region (a.k.a the auxiliary region), even for a multi-region cluster. The entire multi-region cluster's data is stored in one region's bucket. For CMEK, we'll be piggy-backing off of the customer's key used in the auxiliary region to encrypt backups for the entire cluster. If this region is deleted, the backuper will move to a different region, with a different customer key. Because keys are configured on a regional basis, this new region's key may not be able to decrypt old backups. We're in conversations with the bulk-io team to rekey old backups. In the interim, considering that contemporaneous delete-region and rotation operations are very unlikely, we can manually restore backups keyed with an old KMS.

# Future work

## Post-MVP for AWS & GCP

- Support CMEK and Store Key Rotation
- Support CMEK for newly added region to a multi-region cluster
- Support Disable CMEK scenario, to effectively convert to a non-CMEK cluster

## Azure Support

<https://docs.microsoft.com/en-us/azure/key-vault/general/overview> 

<https://github.com/Azure/kubernetes-kms>

# Relevant Links

- [Customer Managed Encryption Keys](https://docs.google.com/document/d/1VleJa-V6Fh-5xv9Rt7Qpy-LfXNqwH9uDu2pSWx0LNxk/edit#heading=h.d2xj40rkqtaw)
- [CMEK Technical Brief](https://docs.google.com/document/d/1xEb0wBgEfgYEjafzARiGqR-WhTT2DKHq9er0lrKTSxY/edit#heading=h.rfd5ud7zkwc2)
- [CMEK Customer-facing Workflow](https://docs.google.com/document/d/1bXtIT43-zEFZdMLHK-_FEvEIkuZ8s1y7VB97bG4np1c/edit#)
- [Encryption and Secrets High-level Future State](https://docs.google.com/document/d/1Q9zxfsEPbYwib-lY8D19mAnG7FBq1jzjopnZYpQ2Kio/edit#heading=h.7pn6nzmzevv)
- [Current High-Level CC Architecture](https://cockroachlabs.atlassian.net/wiki/spaces/MC/pages/935854355/Reference+Architecture)
- [CMEK estimates](https://docs.google.com/spreadsheets/u/0/d/1LxAjNDdzQvjygbITUidf7EVYFHwMuw45W7gHfNcgxBw/edit)
- [Muto's POC](https://github.com/pseudomuto/cmek-poc/tree/ear)
- [Tracking card for RFC: CC-5646](https://cockroachlabs.atlassian.net/browse/CC-5646)
- [Release plan for CMEK on CC Dedicated](https://docs.google.com/document/d/17Exh_0yoIUwdRtunAq3wAFJZX9spKY2J6tT62f46oVA/edit#heading=h.lzkx14ma7yb)
- [Why do we support a customer KMS per region vs. one for the whole cluster?](https://cockroachlabs.slack.com/archives/C02Q8MLFFE0/p1642040960034200?thread_ts=1642034669.017000&cid=C02Q8MLFFE0)
- [Dump of all the comments for this RFC as of 1/26, for ease of search](https://docs.google.com/document/d/1BF8m8s-HrDjb7mutPaNyoEqG1c9IGfIQX5V_mtBsjNI/edit)
- [GCP Cloud KMS for backups in CRDB Spec](https://docs.google.com/document/d/1cgvHCuIgZ7tgwJB9qbhk_nJZz5auaLlYhFYud_6GPCY/edit)

# Appendix

## Alternative Designs Considered

### Long-Term Store Key Generation

We decided that in the long term, the operator will be responsible for generating store keys.

**We also considered** having Intrusion generate store keys in the long-term architecture. Intrusion needs permissions to access customer KMS and encrypted store keys are persisted in the control plane DB.

Note: it was decided in [this sync](https://cockroachlabs.atlassian.net/wiki/spaces/PM/pages/2380759074) meeting that it's preferable if orchestration creates keys and stores them on the disk next to CRDB's data. [This](https://docs.google.com/drawings/d/1lM0Brg9quoVG_Ht9qrSKXTnezU_cuPAwWqvxmsgtoQo/edit) is the sequence diagram for reference.

### Short-Term Store Key Generation

We decided that in the short term, while waiting for the operator to be written and deployed, Intrusion will be responsible for generating store keys.

Reasons for doing this in Intrusion:

- The engineering work is the least heavy: Intrusion is essentially functioning as a CRDB cluster operator right now, and all the scaffolding to interact with k8s clusters from Intrusion is already there.
- Tech debt would not be particularly high either, if code is written keeping the eventual migration to the operator in mind.

Reasons against doing this in Intrusion:

- Having Intrusion fetch KMS credentials from a k8s feels unclean, as the credentials are "leaving" the k8s cluster. However, this is a temporary solution while the operator is being created.

**We also considered** writing a separate encryption application, kicked off by Intrusion, that would run within K8s and generate the store keys.

In this option, when initializing CRDB in a region, Intrusion will first initialize and run a separate encryption job running within the same k8s cluster as the eventual CRDB nodes. This encryption job will be responsible for generating, encrypting, and storing a store key.

Steps:

1. Intrusion creates a region.
2. Intrusion kicks off the encryption job within k8s.
3. The encryption job generates, encrypts, and stores a regional store key, as detailed in option 1.
4. Intrusion waits for the job to complete, and creates the CRDB cluster.

Reasons for this option:

- The architecture is closer to the eventual desired architecture, in that CMEK logic lives entirely within k8s. Intrusion would not have to fetch credentials out of k8s.

Reasons against this option:

- There is extra engineering work involved in getting encryption logic packaged up in a container and running it as a job on k8s, compared to putting the logic directly in Intrusion.
- Having this in place might step on the operator's toes somehow while the operator is getting deployed.
- This option ties cluster creation with CMEK initialization unless it is written in a more sophisticated way. We'd like to be able to enable CMEK after cluster initialization.

### The Alternative Approach to Permissioning: The customer owns the AWS user 

In the 5-step permission flow described above, we conjectured that CC must own a principal within the CC cluster account. Though it's true that the cluster must authenticate as some principal, as long as the CRDB pods know the permissioned user's credentials ([access key ID  ](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html)and[  secret access key](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html)), it's not necessary that this principal live within the CC's AWS organization.

This alternative revolves around giving the customer a CC API to provide credentials for a user the customer owns. This user could live in the customer's AWS KMS account and be permitted to access certain KMS keys and rotate its credentials. Once these credentials propagate to the AWS CC cluster, the flow would follow identically to that of [Option 2](#option-2-store-principals-credentials-in-k8s-secrets-adwits-preference-selected) above.

#### Pros

- This would decouple Amazon KMS from AWS CC clusters entirely; we'd own literally no principal. It'd be even more trivial than in [Option 2](#option-2-store-principals-credentials-in-k8s-secrets-adwits-preference-selected) to onboard AWS KMS with GCP CC clusters, or any other cloud vendor for that matter.
- This pattern extends to other vendors' KMS's, e.g. GCP Cloud KMS with service account tokens. 
- This option follows somewhat closely to CockroachDB's [natively-supported KMS backup integration](https://www.cockroachlabs.com/docs/stable/take-and-restore-encrypted-backups.html#aws-kms-uri-format), in which we pass AWS KMS user/role credentials. 

#### Cons

- This is slightly more work for the user, who has to create a user, vs edit an existing key policy.
- We're passing long-lived credentials over the wire, and we have to be careful to not log/persist these anywhere (at least before they're rotated). Security should probably advise as to whether or not this is actually an issue.
