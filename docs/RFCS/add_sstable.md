- Feature Name: AddSSTable Command
- Status: draft
- Start Date: 2017-05-12
- Authors: Daniel Harrison
- RFC PR: (PR # after acceptance of initial draft)
- Cockroach Issue: (one or more # from the issue tracker)

# Summary

A new kv command to use RocksDB's [IngestExternalFile] (previously called
`AddFile`) which links a prebuilt SSTable directly into the underlying
log-structured merge-tree.

# Motivation

`RESTORE` of a backup and bulk `LOAD` of external data both need to put data in
RocksDB as fast as possible.

Previously, this worked via the `WriteBatch` command, which commits the
serialized form of a RocksDB batch. This unfortunately involves a high write
amplification (because of compactions) and keeps a duplicate of the data in the
raft log. Both of these have contributed to stability issues.

Additionally, `disk max throughput / write amplification` is currently the
limiting constraint on bulk import speed.

TODO(dan): It's possible that snapshots could reuse all or parts of this work.
Update this RFC with details.

# Detailed design

A new kv command `AddSSTable` takes an `SSTable` generated by `SstFileWriter` as
a `[]byte` payload. (Please bikeshed the name. Some alternatives: AddTable,
IngestExternal, AddFile) The proposer validates both that the encoded keys are
within the declared span and that the key-value checksums are correct. MVCC
stats are calculated. The proposer then content addresses the payload and puts
only this checksum into raft.

Each replica, when applying this raft entry, retrieves the payload (details
below), writes it to disk in the store directory, verifies the checksum, and
inserts the file into RockdDB via `IngestExternalFile`.

## [WIP] Payload Trasnfer

Each replica needs the payload to apply the raft command, but by design the
payload is not passed through the normal raft mechanism. The most delicate part
of AddSSTable is making sure the payload is present (or retrievable) for this
step. If it's ever not, the replica will not be able to make progress. If this
happens to a majority of replicas, the range will not be able to make progress.

NB: _Reviewers, everything in the design below this note is a strawman. This RFC
is being sent out to get early high-level feedback before significant time is
spent polishing the details._

Payloads are transferred through a new `Payload` grpc service that's both push
and pull. The proposer, before adding the raft entry, pushes the
content-addressed payload to each of the followers, which store it in-memory.
This optimizes the common case of unchanged raft membership between proposal and
application. (TODO(dan): Or should it go on disk? Ideally it would not because
that adds to the write amplification, but the range will get stuck if we lose
all copies of the payload and it seems like we should be resilient to the
processes being restarted).

If membership does change, a node will not have the payload in its cache. To
apply the raft entry, it will first have to fetch the payload via the `Payload`
service's pull. The mapping of which payloads are available from which nodes is
distributed via gossip. (TODO(dan): This is similar to distributed file sharing,
which means there's certainly literature that will make this more principled.
Does anyone have pointers?)

TODO(dan): What are the effects if a raft follower has to fetch a payload and
this takes a while?

TODO(dan): The proposer distributes the payload to each follower before sending
the raft entry, in effect replicating it the same number of times as the range.
If all copies are lost before a majority can apply the entry, the range will be
stuck. Should we somehow upreplicate the payload if one of the nodes is lost
before a majority have applied it?

TODO(dan): When can a node remove entries from its payload cache?

## Benchmark

The TPCH-1 data (1.7GB) was restored from local disk into a one node cluster.
CockroachDB v1.0 restored it in 3m52s and wrote 33GB to disk. An AddSSTable
prototype (with an in-memory payload cache) restored it in 46s and wrote 4.3GB
to disk. The times aren't perfectly comparable because the amount of
parallelization was different, but write amplification was reduced dramatically.
The prototype had a restore write amplification of 2x, but this could be reduced
to 1x by rewriting the sstable in memory, though it would go back up to 2x if
the payload cache is put on disk.

(Multinode benchmarks coming soon. A subtle bug is preventing r=3 from working
in the prototype.)

# Drawbacks

HTTP downstream of raft is scary. Fortunately, the important part is that the
content-addressing remain logically consistent. Rolling upgrades should be safe
as long as that invariant isn't changed.

There are certainly unknown unknowns here that will lead to a range getting
stuck. This should be introduced early in a release cycle and tested heavily
under chaos to catch as many of them as possible.

# Alternatives

## Construct the SSTable on each Replica

The construction of the sstable involves volitile business logic (rekeying
requires decoding the keys for interleaved tables) as well as clients for using
various cloud storages (S3, GCS, azure blob storage). It is unrealistic for this
to ever stabilize enough to be downstream of raft.

# Unresolved questions

Many. See above.

[IngestExternalFile]: https://github.com/facebook/rocksdb/wiki/Creating-and-Ingesting-SST-files
