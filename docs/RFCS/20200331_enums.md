- Feature Name: Enum Data Types in CockroachDB
- Status: draft
- Start Date: 2020-03-31
- Authors: Rohan Yadav, Lucy Zhang, Andrew Werner, Jordan Lewis
- RFC PR: #TODO (rohany): Fill this out.
- Cockroach Issue: #24873

# Summary

This RFC proposes adding enum types to CockroachDB. 

# Background

Enum types are a class of user defined types where the values in 
the type are constrained to a fixed set
of user specified values. The system then ensures type safety over operations
on this type. This includes ensuring that only values that are members of the 
enum can be inserted into a column of the enum type, and that enums can only
be compared to other values of the same enum type. For example, consider an
application that needs to store events and the days of the week that they happen.
This application could use an enum to represent the days of the week.

```sql
CREATE TYPE day ENUM AS ('monday', 'tuesday', 'wednesday'...);
CREATE TABLE events (id INT, dayofweek day);
INSERT INTO events VALUES (1, 'monday');
```


# Overview

To implement enum types in CockroachDB, we have to touch many layers
of the system. In particular, we need to introduce a way of storing 
metadata about enums durably in the database. We then need a way to 
cache this metadata so that lookups on this metadata is fast, as well 
as a way to invalidate this cache when enum metadata changes. When 
enum metadata changes, we need to ensure that these changes do not
result in some nodes in the cluster entering a situation where 
they are unable to process enum values they find. Lastly, we need
to define a physical layout for enums and integrate enums within
the type system and SQL execution stack.

# Detailed Explanation

## Metadata Storage

Enums themselves are a special case of user-defined types. In order
to lay the groundwork for future work in this area, we propose storing
metadata about an enum in a new descriptor called a `UserDefinedTypeDescriptor`.
This descriptor will be added to the descriptor union
along side Table and Database descriptors and will reuse the same leasing
and versioning system. The descriptor will store metadata about the type,
including the parent database and schema IDs, a unique ID for the type, and
the name of the type. The descriptor will also include specific information 
for the kind of type being stored in the descriptor (as of now there
would only be enums). For enums, this information would include the mapping
of the enum's values to their physical representations.
 
Another option is to build off of
`TableDescriptor` and add the necessary fields for describing user-defined
types there. This has the benefit of allowing us to reuse all existing 
infrastructure relating to versioning/leasing without any code change. 
The con of this approach is that it clutters the `TableDescriptor` more 
with specialized information. If we can provide a proper wrapper interface
around `TableDescriptor` that allows access to fields only of the descriptor's
type then this is not bad.

There is ongoing work towards unifying the desciptor interface/building blocks
so that the common versioning and leasing components are easily shared
across different descriptor types. The exact proto layout of the type
metadata will depend on the outcome of this work, so a choice
between the above options may not be necessary.

## Caching and Name Resolution

The SQL Schema team is working on building a schema leasing system.
Using this leasing system, we can cache enum descriptors in memory
and invalidate entries when the lease is invalidated. 

Enums are scoped within a database and a schema. In Postgres, enums
cannot be accessed from other databases -- they can only be accessed from
different schemas in in the same database. However, there is no core reason
that CockroachDB cannot support this. In fact, we might need to support 
references of types across databases to be in line with other cross 
database references that we currently support.
Types can be optionally annotated with a schema, so name resolution
for enum types will need to share caching of schema information 
with the work being done on user defined schemas.

To proceed on this work without being blocked by the work on 
schema leasing and user defined schemas, we propose a prototype
implementation that either uses a simple, incoherent cache for 
enum type resolution, or just don't cache data at all. After
the schema team has made progress on these features, they can
be integrated into the system for enums.

## Changing Enum Definitions

There are a few ways that enums can change over time.
* The name can change.
* The schema the enum is in can change.
* A new enum member can be added to the set of values.
* A member in the enum can be renamed.
* The enum can be dropped.

In order to rename an enum or a value in an enum can be done with a write
to the enum descriptor and then waiting for all nodes to agree on the new value.

The case of adding a new enum element is more difficult. The key difficulty comes 
from ensuring that a node does not attempt to translate a physical layout that it
does not know about yet into a user facing representation of the enum. If we naively
just add the new enum value to the enum metadata, it is possible that another node 
reads a newly written enum from disk and is unsure how to decode it. Consider the
following sequence of events:
* Node 1 receives a new enum element `foo` to its enum descriptor and blocks on
  `WaitForOneVersion`
* Node 2 receives the new enum descriptor update and writes a value with `foo`
* Node 3 tries to read the value of `foo` before receiving the update to 
  its enum descriptor.

In order to avoid these situations, we propose an extension of the strategy 
used for performing online schema changes. As a reminder, when we add a new
schema object to a table, it moves through a series of states before becoming
usable. As the object moves through these states, the types of operations 
that are allowed upon the object change. Between each state, we require that
all nodes in the cluster agree on the new version of the schema object.
For more details, refer to the 
[online schema changes RFC](https://github.com/cockroachdb/cockroach/blob/master/docs/RFCS/20151014_online_schema_change.md).
We propose a similar state 
progression to adding new elements to an enum type. 
1. When a new value is added
to an enum, it is instead placed into a "read only" state. 
2. After all nodes agree on the "read only" state, the new enum value 
is promoted into the set of writeable values in the enum. 

This process ensures that all nodes know
about all potential enum values before they have a chance to be written.
This approach has the drawback of not being able to add an enum value and
then insert that value in the same transaction. This drawback is similar
to our existing limitation of not being able to add a column and insert
into it in the same transaction. Depending on how we represent the metadata
for an enum, we can either reuse the existing schema change infrastructure 
or write a simpler variant for managing changes on enums.

If an enum is dropped without `CASCADE`, the operation will not succeed
if there are any tables that use the enum. If an enum is dropped with
`CASCADE`, all dependent columns are dropped as well. If the database 
that an enum is created within is dropped, then the enum
is dropped as well. In order to maintain this information, the
descriptors that represent an enum need to hold back-references to
the tables that use them. We expect the descriptor leasing system being developed
to manage invalidation of cached enums when enums are destroyed in these cases.

## Physical Layout 

At first, it may seem that a valid implementation of enum values is
to map each to an integer, and then store these integers on disk.
This implementation seems like it would supply all the ordering 
guarantees needed of enums. However, Postgres allows for adding
new enums and specifying the order of the newly created enum
with respect to an existing value of the enum. This looks like:
```sql
CREATE TYPE t ENUM AS ('v1', 'v2');
ALTER TYPE t ADD VALUE 'v1.5' AFTER 'v1'
```
This means add the value `v1.5` to the enum `t` and order it
after the value `v1`. Using just integers as the backing value
for enums would not allow us to handle this sort of case.
Postgres implements this feature on enums by storing a sorting
order for enums as a float. When a new value is added like this,
Postgres takes the sort orders of the enums that the new enum is
being inserted in between, and creates a float that bisects the
range between the two orders. Concretely, if `v1` had a sort order
of `1.5` and `v2` had a sort order of `2.0`, then `v1.5` would be 
inserted with a sort order of `1.75`. However, once the floating
point precision limit has been reached, Postgres rewrites all
sort orders to integral values. Postgres can do this because it
doesn't require a stable disk encoding for enums. In our case,
we need to have a stable encoding to store data on disk if an enum
is used in an index, and cannot afford to rewrite all tables using an
enum if the enum sort order has changed.

We propose a different strategy that is related to this idea of
bisecting ranges, but doesn't suffer from problems due to floating
point arithmetic precision. The general idea is to use byte arrays
to hold the sort order of our enums, and reserve some bytes in the
arrays to create the ordering that we need. In particular we reserve
the minimum byte (`0`) and have a maximum allowed byte. In practice
this will be `255`. An example of the encoding scheme is below.

Assume we started with 3 elements (`a`, `b`, `c`), and let the maximum byte value be 3.
The sort order byte arrays for each element would be:
```
a 1/
b 2/
c 3/
```
To add an element after `b` we can create a new key that sits in the middle of the range
between `b` and `c`.
```
a 1/
b 2/
d 2/2/
c 3/
```
Now lets add more values before `d`. The first one is easy:
```
a 1/
b 2/
e 2/1/
d 2/2/
c 3/
```
The tricky case is adding a value before `e`. Because we reserved the minimum byte, we can
append it and then bisect the range again.
```
a 1/
b 2/
f 2/0/2
e 2/1/
d 2/2/
c 3/
```
This strategy can be extended indefinitely as long as this pattern is followed to reserve
the minimum byte. A prototype of the exact algorithm is included as part of the RFC PR.

This sort order byte array will be the physical layout and identifier of the enum. We expect
that for small enums only a byte or two will be used to hold all the values, and that our
compression strategies at the storage layer will compress this data well.

An additional strategy is to represent enum values as a tuple of an integer and 
this byte array.
The ordering of the enums will be first on the integer, then on the byte array. An advantage
of this approach is that adding a new enum to the beginning or end of the set requires only
an increment/decrement of the integer rather than creating a new and possibly longer byte array.
Adding to the beginning or end of the existing enum value set is the most
common operation, and is the "worst case" for the pure byte array algorithm.
A drawback of this strategy is that more space is used in the case where there are not many enum
values. 

## Parsing Changes

Currently, the CockroachDB grammar is not equipped to handle type names
that are qualified due to changes made in the past that separated parsing of
object and type identifiers. Some of these changes will have to be 
reverted/adapted in order to allow for types to have qualifications again.
I am currently investigating the extend of the changes that need to be made in
this area.


## Type System Changes

To update the type system to handle enums, we will introduce a new enum type
which holds onto the ID of the enum. Using this, only enums of the same type
can be compared etc. A new `Datum` `DEnum` will be introduced that holds 
onto the ID of the enum that it represents as well as the physical layout 
discussed above. Operations on two `DEnum` datums will succeed only if the
two `DEnum`s have the same enum ID.

When a user-defined type is created in Postgres, Postgres will automatically
create an alias for an array of the new type. For example, if a user creates
a type `days`, the system would also create the type `_days` as an alias for
`days[]`. This type tracks changes made to the referenced type as it 
moves through schemas and is dropped. It is unclear if supporting this sort
of type aliasing will be a significant lift from the enum work.

Each new enum type will need a unique OID. In order to guarantee that all
user defined types have a unique OID we will maintain a new k/v level counter
that is incremented transactionally as new types are created. This counter will
start at a value larger than all existing Postgres type OIDs to avoid 
collisions with existing types.

## Optimizer Changes

The optimizer will need to be taught about the check constraint implied by
a column being of an enum type. Additionally, it will need to be taught how
to convert enum values from their input string representation into their
`Datum` physical representation. Having access to this information involves
the leasing system discussed earlier.

# Alternatives

One alternative approach to this physical layout was to store just an
enum ID on disk, and store ordering and representation information in
a separate lookup table. When operations like on enums would involve
joining or rendering the enums, a join would be produced against this
reference table. This allows for easy changing of enum data, but
results in a variety of complexity during planning.

# Unresolved questions

It is unclear what interactions will arise between this work and the
planned/ongoing work with user defined schemas.
